14:58:02 [INFO] Is MPS available: true
14:58:02 [INFO] Using the following hyperparameters:
14:58:02 [INFO] Hidden layer dimensions: 3000
14:58:02 [INFO] Number of layers: 4
14:58:02 [INFO] Number of trajectories: 400
14:58:02 [INFO] Trajectory depth: 2
14:58:02 [INFO] Number of epochs: 125
14:58:02 [INFO] Learning rate: 0.0001
14:58:02 [INFO] Starting policy training...
14:58:02 [INFO] Training epoch 0
14:58:04 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:58:04 [INFO] Size of sampled logits tensor [400, 2, 1]
14:58:04 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:58:04 [INFO] Unweighted rewards: [400, 2]
14:58:04 [INFO] Weighted rewards: [400]
14:58:04 [INFO] Loss tensor: [1] [-115476.828125]
14:58:04 [INFO] Mean epoch policy entropy for the trajectories is: 2.5569708
14:58:08 [INFO] Training epoch 1
14:58:10 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:58:10 [INFO] Size of sampled logits tensor [400, 2, 1]
14:58:10 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:58:10 [INFO] Unweighted rewards: [400, 2]
14:58:10 [INFO] Weighted rewards: [400]
14:58:10 [INFO] Loss tensor: [1] [-80395.71875]
14:58:10 [INFO] Mean epoch policy entropy for the trajectories is: 2.540103
14:58:14 [INFO] Training epoch 2
14:58:15 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:58:15 [INFO] Size of sampled logits tensor [400, 2, 1]
14:58:15 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:58:15 [INFO] Unweighted rewards: [400, 2]
14:58:15 [INFO] Weighted rewards: [400]
14:58:15 [INFO] Loss tensor: [1] [-138781.390625]
14:58:16 [INFO] Mean epoch policy entropy for the trajectories is: 2.5187697
14:58:20 [INFO] Training epoch 3
14:58:21 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:58:21 [INFO] Size of sampled logits tensor [400, 2, 1]
14:58:21 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:58:21 [INFO] Unweighted rewards: [400, 2]
14:58:21 [INFO] Weighted rewards: [400]
14:58:21 [INFO] Loss tensor: [1] [-151002.28125]
14:58:22 [INFO] Mean epoch policy entropy for the trajectories is: 2.4891412
14:58:26 [INFO] Training epoch 4
14:58:27 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:58:27 [INFO] Size of sampled logits tensor [400, 2, 1]
14:58:27 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:58:27 [INFO] Unweighted rewards: [400, 2]
14:58:27 [INFO] Weighted rewards: [400]
14:58:27 [INFO] Loss tensor: [1] [-139203.390625]
14:58:27 [INFO] Mean epoch policy entropy for the trajectories is: 2.44086
14:58:32 [INFO] Training epoch 5
14:58:33 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:58:33 [INFO] Size of sampled logits tensor [400, 2, 1]
14:58:33 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:58:33 [INFO] Unweighted rewards: [400, 2]
14:58:33 [INFO] Weighted rewards: [400]
14:58:33 [INFO] Loss tensor: [1] [-84058.9765625]
14:58:33 [INFO] Mean epoch policy entropy for the trajectories is: 2.3346364
14:58:38 [INFO] Training epoch 6
14:58:39 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:58:39 [INFO] Size of sampled logits tensor [400, 2, 1]
14:58:39 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:58:39 [INFO] Unweighted rewards: [400, 2]
14:58:39 [INFO] Weighted rewards: [400]
14:58:39 [INFO] Loss tensor: [1] [36450.37109375]
14:58:39 [INFO] Mean epoch policy entropy for the trajectories is: 2.1956544
14:58:44 [INFO] Training epoch 7
14:58:45 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:58:45 [INFO] Size of sampled logits tensor [400, 2, 1]
14:58:45 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:58:45 [INFO] Unweighted rewards: [400, 2]
14:58:45 [INFO] Weighted rewards: [400]
14:58:45 [INFO] Loss tensor: [1] [45701.34765625]
14:58:45 [INFO] Mean epoch policy entropy for the trajectories is: 2.0455165
14:58:50 [INFO] Training epoch 8
14:58:51 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:58:51 [INFO] Size of sampled logits tensor [400, 2, 1]
14:58:51 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:58:51 [INFO] Unweighted rewards: [400, 2]
14:58:51 [INFO] Weighted rewards: [400]
14:58:51 [INFO] Loss tensor: [1] [37821.5859375]
14:58:51 [INFO] Mean epoch policy entropy for the trajectories is: 1.8866376
14:58:55 [INFO] Training epoch 9
14:58:57 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:58:57 [INFO] Size of sampled logits tensor [400, 2, 1]
14:58:57 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:58:57 [INFO] Unweighted rewards: [400, 2]
14:58:57 [INFO] Weighted rewards: [400]
14:58:57 [INFO] Loss tensor: [1] [127054.3359375]
14:58:57 [INFO] Mean epoch policy entropy for the trajectories is: 1.7483336
14:59:01 [INFO] Training epoch 10
14:59:03 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:59:03 [INFO] Size of sampled logits tensor [400, 2, 1]
14:59:03 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:59:03 [INFO] Unweighted rewards: [400, 2]
14:59:03 [INFO] Weighted rewards: [400]
14:59:03 [INFO] Loss tensor: [1] [143653.28125]
14:59:03 [INFO] Mean epoch policy entropy for the trajectories is: 1.6793202
14:59:07 [INFO] Training epoch 11
14:59:09 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:59:09 [INFO] Size of sampled logits tensor [400, 2, 1]
14:59:09 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:59:09 [INFO] Unweighted rewards: [400, 2]
14:59:09 [INFO] Weighted rewards: [400]
14:59:09 [INFO] Loss tensor: [1] [94286.578125]
14:59:09 [INFO] Mean epoch policy entropy for the trajectories is: 1.6873162
14:59:13 [INFO] Training epoch 12
14:59:15 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:59:15 [INFO] Size of sampled logits tensor [400, 2, 1]
14:59:15 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:59:15 [INFO] Unweighted rewards: [400, 2]
14:59:15 [INFO] Weighted rewards: [400]
14:59:15 [INFO] Loss tensor: [1] [93573.0]
14:59:15 [INFO] Mean epoch policy entropy for the trajectories is: 1.5459094
14:59:19 [INFO] Training epoch 13
14:59:21 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:59:21 [INFO] Size of sampled logits tensor [400, 2, 1]
14:59:21 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:59:21 [INFO] Unweighted rewards: [400, 2]
14:59:21 [INFO] Weighted rewards: [400]
14:59:21 [INFO] Loss tensor: [1] [133187.359375]
14:59:21 [INFO] Mean epoch policy entropy for the trajectories is: 1.469878
14:59:25 [INFO] Training epoch 14
14:59:27 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:59:27 [INFO] Size of sampled logits tensor [400, 2, 1]
14:59:27 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:59:27 [INFO] Unweighted rewards: [400, 2]
14:59:27 [INFO] Weighted rewards: [400]
14:59:27 [INFO] Loss tensor: [1] [73466.8203125]
14:59:27 [INFO] Mean epoch policy entropy for the trajectories is: 1.3421785
14:59:31 [INFO] Training epoch 15
14:59:33 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:59:33 [INFO] Size of sampled logits tensor [400, 2, 1]
14:59:33 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:59:33 [INFO] Unweighted rewards: [400, 2]
14:59:33 [INFO] Weighted rewards: [400]
14:59:33 [INFO] Loss tensor: [1] [90647.5390625]
14:59:33 [INFO] Mean epoch policy entropy for the trajectories is: 1.2570282
14:59:37 [INFO] Training epoch 16
14:59:39 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:59:39 [INFO] Size of sampled logits tensor [400, 2, 1]
14:59:39 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:59:39 [INFO] Unweighted rewards: [400, 2]
14:59:39 [INFO] Weighted rewards: [400]
14:59:39 [INFO] Loss tensor: [1] [165808.9375]
14:59:39 [INFO] Mean epoch policy entropy for the trajectories is: 1.1066635
14:59:43 [INFO] Training epoch 17
14:59:45 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:59:45 [INFO] Size of sampled logits tensor [400, 2, 1]
14:59:45 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:59:45 [INFO] Unweighted rewards: [400, 2]
14:59:45 [INFO] Weighted rewards: [400]
14:59:45 [INFO] Loss tensor: [1] [116089.0]
14:59:45 [INFO] Mean epoch policy entropy for the trajectories is: 1.0589432
14:59:50 [INFO] Training epoch 18
14:59:51 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:59:51 [INFO] Size of sampled logits tensor [400, 2, 1]
14:59:51 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:59:51 [INFO] Unweighted rewards: [400, 2]
14:59:51 [INFO] Weighted rewards: [400]
14:59:51 [INFO] Loss tensor: [1] [135790.21875]
14:59:52 [INFO] Mean epoch policy entropy for the trajectories is: 0.9314087
14:59:56 [INFO] Training epoch 19
14:59:58 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:59:58 [INFO] Size of sampled logits tensor [400, 2, 1]
14:59:58 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:59:58 [INFO] Unweighted rewards: [400, 2]
14:59:58 [INFO] Weighted rewards: [400]
14:59:58 [INFO] Loss tensor: [1] [167791.515625]
14:59:58 [INFO] Mean epoch policy entropy for the trajectories is: 0.9121581
15:00:03 [INFO] Training epoch 20
15:00:04 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:00:04 [INFO] Size of sampled logits tensor [400, 2, 1]
15:00:04 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:00:04 [INFO] Unweighted rewards: [400, 2]
15:00:04 [INFO] Weighted rewards: [400]
15:00:04 [INFO] Loss tensor: [1] [126921.0078125]
15:00:04 [INFO] Mean epoch policy entropy for the trajectories is: 0.80697954
15:00:09 [INFO] Training epoch 21
15:00:10 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:00:10 [INFO] Size of sampled logits tensor [400, 2, 1]
15:00:10 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:00:10 [INFO] Unweighted rewards: [400, 2]
15:00:10 [INFO] Weighted rewards: [400]
15:00:10 [INFO] Loss tensor: [1] [101091.5390625]
15:00:10 [INFO] Mean epoch policy entropy for the trajectories is: 0.78897315
15:00:15 [INFO] Training epoch 22
15:00:17 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:00:17 [INFO] Size of sampled logits tensor [400, 2, 1]
15:00:17 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:00:17 [INFO] Unweighted rewards: [400, 2]
15:00:17 [INFO] Weighted rewards: [400]
15:00:17 [INFO] Loss tensor: [1] [88144.0]
15:00:17 [INFO] Mean epoch policy entropy for the trajectories is: 0.6721546
15:00:21 [INFO] Training epoch 23
15:00:23 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:00:23 [INFO] Size of sampled logits tensor [400, 2, 1]
15:00:23 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:00:23 [INFO] Unweighted rewards: [400, 2]
15:00:23 [INFO] Weighted rewards: [400]
15:00:23 [INFO] Loss tensor: [1] [59617.984375]
15:00:23 [INFO] Mean epoch policy entropy for the trajectories is: 0.68268013
15:00:27 [INFO] Training epoch 24
15:00:28 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:00:28 [INFO] Size of sampled logits tensor [400, 2, 1]
15:00:28 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:00:28 [INFO] Unweighted rewards: [400, 2]
15:00:28 [INFO] Weighted rewards: [400]
15:00:28 [INFO] Loss tensor: [1] [56276.40234375]
15:00:29 [INFO] Mean epoch policy entropy for the trajectories is: 0.641256
15:00:33 [INFO] Training epoch 25
15:00:34 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:00:34 [INFO] Size of sampled logits tensor [400, 2, 1]
15:00:34 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:00:34 [INFO] Unweighted rewards: [400, 2]
15:00:34 [INFO] Weighted rewards: [400]
15:00:34 [INFO] Loss tensor: [1] [45289.05859375]
15:00:34 [INFO] Mean epoch policy entropy for the trajectories is: 0.56771
15:00:39 [INFO] Training epoch 26
15:00:40 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:00:40 [INFO] Size of sampled logits tensor [400, 2, 1]
15:00:40 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:00:40 [INFO] Unweighted rewards: [400, 2]
15:00:40 [INFO] Weighted rewards: [400]
15:00:40 [INFO] Loss tensor: [1] [55874.91015625]
15:00:40 [INFO] Mean epoch policy entropy for the trajectories is: 0.52999955
15:00:45 [INFO] Training epoch 27
15:00:46 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:00:46 [INFO] Size of sampled logits tensor [400, 2, 1]
15:00:46 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:00:46 [INFO] Unweighted rewards: [400, 2]
15:00:46 [INFO] Weighted rewards: [400]
15:00:46 [INFO] Loss tensor: [1] [56365.3203125]
15:00:46 [INFO] Mean epoch policy entropy for the trajectories is: 0.54105806
15:00:50 [INFO] Training epoch 28
15:00:52 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:00:52 [INFO] Size of sampled logits tensor [400, 2, 1]
15:00:52 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:00:52 [INFO] Unweighted rewards: [400, 2]
15:00:52 [INFO] Weighted rewards: [400]
15:00:52 [INFO] Loss tensor: [1] [37662.93359375]
15:00:52 [INFO] Mean epoch policy entropy for the trajectories is: 0.49858233
15:00:56 [INFO] Training epoch 29
15:00:58 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:00:58 [INFO] Size of sampled logits tensor [400, 2, 1]
15:00:58 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:00:58 [INFO] Unweighted rewards: [400, 2]
15:00:58 [INFO] Weighted rewards: [400]
15:00:58 [INFO] Loss tensor: [1] [47029.6171875]
15:00:58 [INFO] Mean epoch policy entropy for the trajectories is: 0.4380579
15:01:02 [INFO] Training epoch 30
15:01:04 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:01:04 [INFO] Size of sampled logits tensor [400, 2, 1]
15:01:04 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:01:04 [INFO] Unweighted rewards: [400, 2]
15:01:04 [INFO] Weighted rewards: [400]
15:01:04 [INFO] Loss tensor: [1] [78487.734375]
15:01:04 [INFO] Mean epoch policy entropy for the trajectories is: 0.45968765
15:01:08 [INFO] Training epoch 31
15:01:10 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:01:10 [INFO] Size of sampled logits tensor [400, 2, 1]
15:01:10 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:01:10 [INFO] Unweighted rewards: [400, 2]
15:01:10 [INFO] Weighted rewards: [400]
15:01:10 [INFO] Loss tensor: [1] [51838.8671875]
15:01:10 [INFO] Mean epoch policy entropy for the trajectories is: 0.42567664
15:01:14 [INFO] Training epoch 32
15:01:16 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:01:16 [INFO] Size of sampled logits tensor [400, 2, 1]
15:01:16 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:01:16 [INFO] Unweighted rewards: [400, 2]
15:01:16 [INFO] Weighted rewards: [400]
15:01:16 [INFO] Loss tensor: [1] [33628.12109375]
15:01:16 [INFO] Mean epoch policy entropy for the trajectories is: 0.47702163
15:01:20 [INFO] Training epoch 33
15:01:22 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:01:22 [INFO] Size of sampled logits tensor [400, 2, 1]
15:01:22 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:01:22 [INFO] Unweighted rewards: [400, 2]
15:01:22 [INFO] Weighted rewards: [400]
15:01:22 [INFO] Loss tensor: [1] [62772.96875]
15:01:22 [INFO] Mean epoch policy entropy for the trajectories is: 0.4382137
15:01:26 [INFO] Training epoch 34
15:01:28 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:01:28 [INFO] Size of sampled logits tensor [400, 2, 1]
15:01:28 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:01:28 [INFO] Unweighted rewards: [400, 2]
15:01:28 [INFO] Weighted rewards: [400]
15:01:28 [INFO] Loss tensor: [1] [49828.1171875]
15:01:28 [INFO] Mean epoch policy entropy for the trajectories is: 0.45089847
15:01:32 [INFO] Training epoch 35
15:01:34 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:01:34 [INFO] Size of sampled logits tensor [400, 2, 1]
15:01:34 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:01:34 [INFO] Unweighted rewards: [400, 2]
15:01:34 [INFO] Weighted rewards: [400]
15:01:34 [INFO] Loss tensor: [1] [50036.6875]
15:01:34 [INFO] Mean epoch policy entropy for the trajectories is: 0.4378185
15:01:38 [INFO] Training epoch 36
15:01:40 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:01:40 [INFO] Size of sampled logits tensor [400, 2, 1]
15:01:40 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:01:40 [INFO] Unweighted rewards: [400, 2]
15:01:40 [INFO] Weighted rewards: [400]
15:01:40 [INFO] Loss tensor: [1] [59132.3828125]
15:01:40 [INFO] Mean epoch policy entropy for the trajectories is: 0.4632733
15:01:44 [INFO] Training epoch 37
15:01:45 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:01:45 [INFO] Size of sampled logits tensor [400, 2, 1]
15:01:45 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:01:45 [INFO] Unweighted rewards: [400, 2]
15:01:45 [INFO] Weighted rewards: [400]
15:01:45 [INFO] Loss tensor: [1] [53624.95703125]
15:01:46 [INFO] Mean epoch policy entropy for the trajectories is: 0.415735
15:01:50 [INFO] Training epoch 38
15:01:51 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:01:51 [INFO] Size of sampled logits tensor [400, 2, 1]
15:01:51 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:01:51 [INFO] Unweighted rewards: [400, 2]
15:01:51 [INFO] Weighted rewards: [400]
15:01:51 [INFO] Loss tensor: [1] [27671.984375]
15:01:51 [INFO] Mean epoch policy entropy for the trajectories is: 0.5073744
15:01:56 [INFO] Training epoch 39
15:01:57 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:01:57 [INFO] Size of sampled logits tensor [400, 2, 1]
15:01:57 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:01:57 [INFO] Unweighted rewards: [400, 2]
15:01:57 [INFO] Weighted rewards: [400]
15:01:57 [INFO] Loss tensor: [1] [28500.3359375]
15:01:57 [INFO] Mean epoch policy entropy for the trajectories is: 0.45554662
15:02:01 [INFO] Training epoch 40
15:02:03 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:02:03 [INFO] Size of sampled logits tensor [400, 2, 1]
15:02:03 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:02:03 [INFO] Unweighted rewards: [400, 2]
15:02:03 [INFO] Weighted rewards: [400]
15:02:03 [INFO] Loss tensor: [1] [22521.154296875]
15:02:03 [INFO] Mean epoch policy entropy for the trajectories is: 0.43490878
15:02:07 [INFO] Training epoch 41
15:02:09 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:02:09 [INFO] Size of sampled logits tensor [400, 2, 1]
15:02:09 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:02:09 [INFO] Unweighted rewards: [400, 2]
15:02:09 [INFO] Weighted rewards: [400]
15:02:09 [INFO] Loss tensor: [1] [34070.30859375]
15:02:09 [INFO] Mean epoch policy entropy for the trajectories is: 0.48291004
15:02:13 [INFO] Training epoch 42
15:02:15 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:02:15 [INFO] Size of sampled logits tensor [400, 2, 1]
15:02:15 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:02:15 [INFO] Unweighted rewards: [400, 2]
15:02:15 [INFO] Weighted rewards: [400]
15:02:15 [INFO] Loss tensor: [1] [12110.986328125]
15:02:15 [INFO] Mean epoch policy entropy for the trajectories is: 0.514324
15:02:19 [INFO] Training epoch 43
15:02:21 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:02:21 [INFO] Size of sampled logits tensor [400, 2, 1]
15:02:21 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:02:21 [INFO] Unweighted rewards: [400, 2]
15:02:21 [INFO] Weighted rewards: [400]
15:02:21 [INFO] Loss tensor: [1] [47382.64453125]
15:02:21 [INFO] Mean epoch policy entropy for the trajectories is: 0.4827355
15:02:25 [INFO] Training epoch 44
15:02:27 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:02:27 [INFO] Size of sampled logits tensor [400, 2, 1]
15:02:27 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:02:27 [INFO] Unweighted rewards: [400, 2]
15:02:27 [INFO] Weighted rewards: [400]
15:02:27 [INFO] Loss tensor: [1] [30102.375]
15:02:27 [INFO] Mean epoch policy entropy for the trajectories is: 0.4615891
15:02:31 [INFO] Training epoch 45
15:02:33 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:02:33 [INFO] Size of sampled logits tensor [400, 2, 1]
15:02:33 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:02:33 [INFO] Unweighted rewards: [400, 2]
15:02:33 [INFO] Weighted rewards: [400]
15:02:33 [INFO] Loss tensor: [1] [53992.18359375]
15:02:33 [INFO] Mean epoch policy entropy for the trajectories is: 0.45034522
15:02:37 [INFO] Training epoch 46
15:02:39 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:02:39 [INFO] Size of sampled logits tensor [400, 2, 1]
15:02:39 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:02:39 [INFO] Unweighted rewards: [400, 2]
15:02:39 [INFO] Weighted rewards: [400]
15:02:39 [INFO] Loss tensor: [1] [71364.0078125]
15:02:39 [INFO] Mean epoch policy entropy for the trajectories is: 0.56197065
15:02:43 [INFO] Training epoch 47
15:02:44 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:02:44 [INFO] Size of sampled logits tensor [400, 2, 1]
15:02:44 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:02:44 [INFO] Unweighted rewards: [400, 2]
15:02:44 [INFO] Weighted rewards: [400]
15:02:44 [INFO] Loss tensor: [1] [26635.384765625]
15:02:44 [INFO] Mean epoch policy entropy for the trajectories is: 0.47315636
15:02:49 [INFO] Training epoch 48
15:02:50 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:02:50 [INFO] Size of sampled logits tensor [400, 2, 1]
15:02:50 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:02:50 [INFO] Unweighted rewards: [400, 2]
15:02:50 [INFO] Weighted rewards: [400]
15:02:50 [INFO] Loss tensor: [1] [40534.02734375]
15:02:50 [INFO] Mean epoch policy entropy for the trajectories is: 0.5191812
15:02:55 [INFO] Training epoch 49
15:02:56 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:02:56 [INFO] Size of sampled logits tensor [400, 2, 1]
15:02:56 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:02:56 [INFO] Unweighted rewards: [400, 2]
15:02:56 [INFO] Weighted rewards: [400]
15:02:56 [INFO] Loss tensor: [1] [51047.625]
15:02:56 [INFO] Mean epoch policy entropy for the trajectories is: 0.5324023
15:03:00 [INFO] Training epoch 50
15:03:02 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:03:02 [INFO] Size of sampled logits tensor [400, 2, 1]
15:03:02 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:03:02 [INFO] Unweighted rewards: [400, 2]
15:03:02 [INFO] Weighted rewards: [400]
15:03:02 [INFO] Loss tensor: [1] [13907.71484375]
15:03:02 [INFO] Mean epoch policy entropy for the trajectories is: 0.6369669
15:03:06 [INFO] Training epoch 51
15:03:08 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:03:08 [INFO] Size of sampled logits tensor [400, 2, 1]
15:03:08 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:03:08 [INFO] Unweighted rewards: [400, 2]
15:03:08 [INFO] Weighted rewards: [400]
15:03:08 [INFO] Loss tensor: [1] [75706.8671875]
15:03:08 [INFO] Mean epoch policy entropy for the trajectories is: 0.6065224
15:03:12 [INFO] Training epoch 52
15:03:14 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:03:14 [INFO] Size of sampled logits tensor [400, 2, 1]
15:03:14 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:03:14 [INFO] Unweighted rewards: [400, 2]
15:03:14 [INFO] Weighted rewards: [400]
15:03:14 [INFO] Loss tensor: [1] [47778.75390625]
15:03:14 [INFO] Mean epoch policy entropy for the trajectories is: 0.68090874
15:03:18 [INFO] Training epoch 53
15:03:20 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:03:20 [INFO] Size of sampled logits tensor [400, 2, 1]
15:03:20 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:03:20 [INFO] Unweighted rewards: [400, 2]
15:03:20 [INFO] Weighted rewards: [400]
15:03:20 [INFO] Loss tensor: [1] [64797.75]
15:03:20 [INFO] Mean epoch policy entropy for the trajectories is: 0.6628018
15:03:24 [INFO] Training epoch 54
15:03:26 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:03:26 [INFO] Size of sampled logits tensor [400, 2, 1]
15:03:26 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:03:26 [INFO] Unweighted rewards: [400, 2]
15:03:26 [INFO] Weighted rewards: [400]
15:03:26 [INFO] Loss tensor: [1] [63726.59765625]
15:03:26 [INFO] Mean epoch policy entropy for the trajectories is: 0.6822248
15:03:30 [INFO] Training epoch 55
15:03:31 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:03:31 [INFO] Size of sampled logits tensor [400, 2, 1]
15:03:31 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:03:31 [INFO] Unweighted rewards: [400, 2]
15:03:31 [INFO] Weighted rewards: [400]
15:03:31 [INFO] Loss tensor: [1] [38420.875]
15:03:32 [INFO] Mean epoch policy entropy for the trajectories is: 0.79301053
15:03:36 [INFO] Training epoch 56
15:03:37 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:03:37 [INFO] Size of sampled logits tensor [400, 2, 1]
15:03:37 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:03:37 [INFO] Unweighted rewards: [400, 2]
15:03:37 [INFO] Weighted rewards: [400]
15:03:37 [INFO] Loss tensor: [1] [25215.388671875]
15:03:37 [INFO] Mean epoch policy entropy for the trajectories is: 0.711173
15:03:42 [INFO] Training epoch 57
15:03:43 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:03:43 [INFO] Size of sampled logits tensor [400, 2, 1]
15:03:43 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:03:43 [INFO] Unweighted rewards: [400, 2]
15:03:43 [INFO] Weighted rewards: [400]
15:03:43 [INFO] Loss tensor: [1] [58644.96875]
15:03:43 [INFO] Mean epoch policy entropy for the trajectories is: 0.774504
15:03:48 [INFO] Training epoch 58
15:03:49 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:03:49 [INFO] Size of sampled logits tensor [400, 2, 1]
15:03:49 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:03:49 [INFO] Unweighted rewards: [400, 2]
15:03:49 [INFO] Weighted rewards: [400]
15:03:49 [INFO] Loss tensor: [1] [45149.66796875]
15:03:49 [INFO] Mean epoch policy entropy for the trajectories is: 0.7114155
15:03:54 [INFO] Training epoch 59
15:03:55 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:03:55 [INFO] Size of sampled logits tensor [400, 2, 1]
15:03:55 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:03:55 [INFO] Unweighted rewards: [400, 2]
15:03:55 [INFO] Weighted rewards: [400]
15:03:55 [INFO] Loss tensor: [1] [38175.953125]
15:03:55 [INFO] Mean epoch policy entropy for the trajectories is: 0.7666975
15:03:59 [INFO] Training epoch 60
15:04:01 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:04:01 [INFO] Size of sampled logits tensor [400, 2, 1]
15:04:01 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:04:01 [INFO] Unweighted rewards: [400, 2]
15:04:01 [INFO] Weighted rewards: [400]
15:04:01 [INFO] Loss tensor: [1] [72822.078125]
15:04:01 [INFO] Mean epoch policy entropy for the trajectories is: 0.73091894
15:04:05 [INFO] Training epoch 61
15:04:07 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:04:07 [INFO] Size of sampled logits tensor [400, 2, 1]
15:04:07 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:04:07 [INFO] Unweighted rewards: [400, 2]
15:04:07 [INFO] Weighted rewards: [400]
15:04:07 [INFO] Loss tensor: [1] [10927.7783203125]
15:04:07 [INFO] Mean epoch policy entropy for the trajectories is: 0.7689257
15:04:11 [INFO] Training epoch 62
15:04:13 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:04:13 [INFO] Size of sampled logits tensor [400, 2, 1]
15:04:13 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:04:13 [INFO] Unweighted rewards: [400, 2]
15:04:13 [INFO] Weighted rewards: [400]
15:04:13 [INFO] Loss tensor: [1] [45501.203125]
15:04:13 [INFO] Mean epoch policy entropy for the trajectories is: 0.7265919
15:04:17 [INFO] Training epoch 63
15:04:19 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:04:19 [INFO] Size of sampled logits tensor [400, 2, 1]
15:04:19 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:04:19 [INFO] Unweighted rewards: [400, 2]
15:04:19 [INFO] Weighted rewards: [400]
15:04:19 [INFO] Loss tensor: [1] [30070.82421875]
15:04:19 [INFO] Mean epoch policy entropy for the trajectories is: 0.7553465
15:04:23 [INFO] Training epoch 64
15:04:25 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:04:25 [INFO] Size of sampled logits tensor [400, 2, 1]
15:04:25 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:04:25 [INFO] Unweighted rewards: [400, 2]
15:04:25 [INFO] Weighted rewards: [400]
15:04:25 [INFO] Loss tensor: [1] [58882.390625]
15:04:25 [INFO] Mean epoch policy entropy for the trajectories is: 0.7474766
15:04:29 [INFO] Training epoch 65
15:04:30 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:04:30 [INFO] Size of sampled logits tensor [400, 2, 1]
15:04:30 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:04:30 [INFO] Unweighted rewards: [400, 2]
15:04:30 [INFO] Weighted rewards: [400]
15:04:30 [INFO] Loss tensor: [1] [14688.83203125]
15:04:30 [INFO] Mean epoch policy entropy for the trajectories is: 0.77187836
15:04:35 [INFO] Training epoch 66
15:04:36 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:04:36 [INFO] Size of sampled logits tensor [400, 2, 1]
15:04:36 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:04:36 [INFO] Unweighted rewards: [400, 2]
15:04:36 [INFO] Weighted rewards: [400]
15:04:36 [INFO] Loss tensor: [1] [60262.38671875]
15:04:36 [INFO] Mean epoch policy entropy for the trajectories is: 0.7563997
15:04:41 [INFO] Training epoch 67
15:04:42 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:04:42 [INFO] Size of sampled logits tensor [400, 2, 1]
15:04:42 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:04:42 [INFO] Unweighted rewards: [400, 2]
15:04:42 [INFO] Weighted rewards: [400]
15:04:42 [INFO] Loss tensor: [1] [40677.85546875]
15:04:42 [INFO] Mean epoch policy entropy for the trajectories is: 0.6922829
15:04:46 [INFO] Training epoch 68
15:04:48 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:04:48 [INFO] Size of sampled logits tensor [400, 2, 1]
15:04:48 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:04:48 [INFO] Unweighted rewards: [400, 2]
15:04:48 [INFO] Weighted rewards: [400]
15:04:48 [INFO] Loss tensor: [1] [51541.859375]
15:04:48 [INFO] Mean epoch policy entropy for the trajectories is: 0.70031106
15:04:52 [INFO] Training epoch 69
15:04:54 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:04:54 [INFO] Size of sampled logits tensor [400, 2, 1]
15:04:54 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:04:54 [INFO] Unweighted rewards: [400, 2]
15:04:54 [INFO] Weighted rewards: [400]
15:04:54 [INFO] Loss tensor: [1] [51831.59765625]
15:04:54 [INFO] Mean epoch policy entropy for the trajectories is: 0.7355167
15:04:58 [INFO] Training epoch 70
15:05:00 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:05:00 [INFO] Size of sampled logits tensor [400, 2, 1]
15:05:00 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:05:00 [INFO] Unweighted rewards: [400, 2]
15:05:00 [INFO] Weighted rewards: [400]
15:05:00 [INFO] Loss tensor: [1] [44890.890625]
15:05:00 [INFO] Mean epoch policy entropy for the trajectories is: 0.7702919
15:05:04 [INFO] Training epoch 71
15:05:06 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:05:06 [INFO] Size of sampled logits tensor [400, 2, 1]
15:05:06 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:05:06 [INFO] Unweighted rewards: [400, 2]
15:05:06 [INFO] Weighted rewards: [400]
15:05:06 [INFO] Loss tensor: [1] [99815.8203125]
15:05:06 [INFO] Mean epoch policy entropy for the trajectories is: 0.7337957
15:05:10 [INFO] Training epoch 72
15:05:12 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:05:12 [INFO] Size of sampled logits tensor [400, 2, 1]
15:05:12 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:05:12 [INFO] Unweighted rewards: [400, 2]
15:05:12 [INFO] Weighted rewards: [400]
15:05:12 [INFO] Loss tensor: [1] [58899.375]
15:05:12 [INFO] Mean epoch policy entropy for the trajectories is: 0.708855
15:05:16 [INFO] Training epoch 73
15:05:18 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:05:18 [INFO] Size of sampled logits tensor [400, 2, 1]
15:05:18 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:05:18 [INFO] Unweighted rewards: [400, 2]
15:05:18 [INFO] Weighted rewards: [400]
15:05:18 [INFO] Loss tensor: [1] [57244.71875]
15:05:18 [INFO] Mean epoch policy entropy for the trajectories is: 0.78959554
15:05:22 [INFO] Training epoch 74
15:05:23 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:05:24 [INFO] Size of sampled logits tensor [400, 2, 1]
15:05:24 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:05:24 [INFO] Unweighted rewards: [400, 2]
15:05:24 [INFO] Weighted rewards: [400]
15:05:24 [INFO] Loss tensor: [1] [85817.4765625]
15:05:24 [INFO] Mean epoch policy entropy for the trajectories is: 0.71222067
15:05:28 [INFO] Training epoch 75
15:05:29 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:05:29 [INFO] Size of sampled logits tensor [400, 2, 1]
15:05:29 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:05:29 [INFO] Unweighted rewards: [400, 2]
15:05:29 [INFO] Weighted rewards: [400]
15:05:29 [INFO] Loss tensor: [1] [72656.1953125]
15:05:30 [INFO] Mean epoch policy entropy for the trajectories is: 0.70351845
15:05:34 [INFO] Training epoch 76
15:05:36 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:05:36 [INFO] Size of sampled logits tensor [400, 2, 1]
15:05:36 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:05:36 [INFO] Unweighted rewards: [400, 2]
15:05:36 [INFO] Weighted rewards: [400]
15:05:36 [INFO] Loss tensor: [1] [76588.90625]
15:05:36 [INFO] Mean epoch policy entropy for the trajectories is: 0.71797854
15:05:40 [INFO] Training epoch 77
15:05:42 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:05:42 [INFO] Size of sampled logits tensor [400, 2, 1]
15:05:42 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:05:42 [INFO] Unweighted rewards: [400, 2]
15:05:42 [INFO] Weighted rewards: [400]
15:05:42 [INFO] Loss tensor: [1] [31832.857421875]
15:05:42 [INFO] Mean epoch policy entropy for the trajectories is: 0.7727702
15:05:46 [INFO] Training epoch 78
15:05:48 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:05:48 [INFO] Size of sampled logits tensor [400, 2, 1]
15:05:48 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:05:48 [INFO] Unweighted rewards: [400, 2]
15:05:48 [INFO] Weighted rewards: [400]
15:05:48 [INFO] Loss tensor: [1] [79529.2265625]
15:05:48 [INFO] Mean epoch policy entropy for the trajectories is: 0.70814246
15:05:52 [INFO] Training epoch 79
15:05:54 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:05:54 [INFO] Size of sampled logits tensor [400, 2, 1]
15:05:54 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:05:54 [INFO] Unweighted rewards: [400, 2]
15:05:54 [INFO] Weighted rewards: [400]
15:05:54 [INFO] Loss tensor: [1] [31621.154296875]
15:05:54 [INFO] Mean epoch policy entropy for the trajectories is: 0.8092978
15:05:59 [INFO] Training epoch 80
15:06:00 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:06:00 [INFO] Size of sampled logits tensor [400, 2, 1]
15:06:00 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:06:00 [INFO] Unweighted rewards: [400, 2]
15:06:00 [INFO] Weighted rewards: [400]
15:06:00 [INFO] Loss tensor: [1] [68676.6875]
15:06:00 [INFO] Mean epoch policy entropy for the trajectories is: 0.7672199
15:06:05 [INFO] Training epoch 81
15:06:06 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:06:06 [INFO] Size of sampled logits tensor [400, 2, 1]
15:06:06 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:06:06 [INFO] Unweighted rewards: [400, 2]
15:06:06 [INFO] Weighted rewards: [400]
15:06:06 [INFO] Loss tensor: [1] [67554.1015625]
15:06:07 [INFO] Mean epoch policy entropy for the trajectories is: 0.7305105
15:06:11 [INFO] Training epoch 82
15:06:13 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:06:13 [INFO] Size of sampled logits tensor [400, 2, 1]
15:06:13 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:06:13 [INFO] Unweighted rewards: [400, 2]
15:06:13 [INFO] Weighted rewards: [400]
15:06:13 [INFO] Loss tensor: [1] [93224.9375]
15:06:13 [INFO] Mean epoch policy entropy for the trajectories is: 0.7030795
15:06:17 [INFO] Training epoch 83
15:06:19 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:06:19 [INFO] Size of sampled logits tensor [400, 2, 1]
15:06:19 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:06:19 [INFO] Unweighted rewards: [400, 2]
15:06:19 [INFO] Weighted rewards: [400]
15:06:19 [INFO] Loss tensor: [1] [52454.0390625]
15:06:19 [INFO] Mean epoch policy entropy for the trajectories is: 0.8395285
15:06:23 [INFO] Training epoch 84
15:06:25 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:06:25 [INFO] Size of sampled logits tensor [400, 2, 1]
15:06:25 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:06:25 [INFO] Unweighted rewards: [400, 2]
15:06:25 [INFO] Weighted rewards: [400]
15:06:25 [INFO] Loss tensor: [1] [52159.01953125]
15:06:25 [INFO] Mean epoch policy entropy for the trajectories is: 0.75849414
15:06:30 [INFO] Training epoch 85
15:06:31 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:06:31 [INFO] Size of sampled logits tensor [400, 2, 1]
15:06:31 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:06:31 [INFO] Unweighted rewards: [400, 2]
15:06:31 [INFO] Weighted rewards: [400]
15:06:31 [INFO] Loss tensor: [1] [81509.3203125]
15:06:31 [INFO] Mean epoch policy entropy for the trajectories is: 0.7337345
15:06:36 [INFO] Training epoch 86
15:06:38 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:06:38 [INFO] Size of sampled logits tensor [400, 2, 1]
15:06:38 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:06:38 [INFO] Unweighted rewards: [400, 2]
15:06:38 [INFO] Weighted rewards: [400]
15:06:38 [INFO] Loss tensor: [1] [79381.5859375]
15:06:38 [INFO] Mean epoch policy entropy for the trajectories is: 0.6933779
15:06:42 [INFO] Training epoch 87
15:06:44 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:06:44 [INFO] Size of sampled logits tensor [400, 2, 1]
15:06:44 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:06:44 [INFO] Unweighted rewards: [400, 2]
15:06:44 [INFO] Weighted rewards: [400]
15:06:44 [INFO] Loss tensor: [1] [88164.5]
15:06:44 [INFO] Mean epoch policy entropy for the trajectories is: 0.66002184
15:06:48 [INFO] Training epoch 88
15:06:50 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:06:50 [INFO] Size of sampled logits tensor [400, 2, 1]
15:06:50 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:06:50 [INFO] Unweighted rewards: [400, 2]
15:06:50 [INFO] Weighted rewards: [400]
15:06:50 [INFO] Loss tensor: [1] [63756.4296875]
15:06:50 [INFO] Mean epoch policy entropy for the trajectories is: 0.7888723
15:06:54 [INFO] Training epoch 89
15:06:56 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:06:56 [INFO] Size of sampled logits tensor [400, 2, 1]
15:06:56 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:06:56 [INFO] Unweighted rewards: [400, 2]
15:06:56 [INFO] Weighted rewards: [400]
15:06:56 [INFO] Loss tensor: [1] [78384.5625]
15:06:56 [INFO] Mean epoch policy entropy for the trajectories is: 0.83460754
15:07:01 [INFO] Training epoch 90
15:07:02 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:07:02 [INFO] Size of sampled logits tensor [400, 2, 1]
15:07:02 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:07:02 [INFO] Unweighted rewards: [400, 2]
15:07:02 [INFO] Weighted rewards: [400]
15:07:02 [INFO] Loss tensor: [1] [114271.4296875]
15:07:02 [INFO] Mean epoch policy entropy for the trajectories is: 0.79943526
15:07:07 [INFO] Training epoch 91
15:07:09 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:07:09 [INFO] Size of sampled logits tensor [400, 2, 1]
15:07:09 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:07:09 [INFO] Unweighted rewards: [400, 2]
15:07:09 [INFO] Weighted rewards: [400]
15:07:09 [INFO] Loss tensor: [1] [82733.7265625]
15:07:09 [INFO] Mean epoch policy entropy for the trajectories is: 0.7928072
15:07:14 [INFO] Training epoch 92
15:07:17 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:07:17 [INFO] Size of sampled logits tensor [400, 2, 1]
15:07:17 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:07:17 [INFO] Unweighted rewards: [400, 2]
15:07:17 [INFO] Weighted rewards: [400]
15:07:17 [INFO] Loss tensor: [1] [55858.03515625]
15:07:17 [INFO] Mean epoch policy entropy for the trajectories is: 0.8431508
15:07:22 [INFO] Training epoch 93
15:07:26 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:07:26 [INFO] Size of sampled logits tensor [400, 2, 1]
15:07:26 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:07:26 [INFO] Unweighted rewards: [400, 2]
15:07:26 [INFO] Weighted rewards: [400]
15:07:26 [INFO] Loss tensor: [1] [69404.671875]
15:07:26 [INFO] Mean epoch policy entropy for the trajectories is: 0.7919597
15:07:33 [INFO] Training epoch 94
15:07:37 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:07:37 [INFO] Size of sampled logits tensor [400, 2, 1]
15:07:37 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:07:37 [INFO] Unweighted rewards: [400, 2]
15:07:37 [INFO] Weighted rewards: [400]
15:07:37 [INFO] Loss tensor: [1] [34766.75390625]
15:07:37 [INFO] Mean epoch policy entropy for the trajectories is: 0.81368184
15:07:46 [INFO] Training epoch 95
15:07:50 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:07:50 [INFO] Size of sampled logits tensor [400, 2, 1]
15:07:50 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:07:50 [INFO] Unweighted rewards: [400, 2]
15:07:50 [INFO] Weighted rewards: [400]
15:07:50 [INFO] Loss tensor: [1] [68833.796875]
15:07:50 [INFO] Mean epoch policy entropy for the trajectories is: 0.8184664
15:08:00 [INFO] Training epoch 96
15:08:04 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:08:04 [INFO] Size of sampled logits tensor [400, 2, 1]
15:08:04 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:08:04 [INFO] Unweighted rewards: [400, 2]
15:08:04 [INFO] Weighted rewards: [400]
15:08:04 [INFO] Loss tensor: [1] [48798.84765625]
15:08:04 [INFO] Mean epoch policy entropy for the trajectories is: 0.7335331
15:08:13 [INFO] Training epoch 97
15:08:17 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:08:17 [INFO] Size of sampled logits tensor [400, 2, 1]
15:08:17 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:08:17 [INFO] Unweighted rewards: [400, 2]
15:08:17 [INFO] Weighted rewards: [400]
15:08:17 [INFO] Loss tensor: [1] [159296.296875]
15:08:18 [INFO] Mean epoch policy entropy for the trajectories is: 0.8971361
15:08:26 [INFO] Training epoch 98
15:08:30 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:08:30 [INFO] Size of sampled logits tensor [400, 2, 1]
15:08:30 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:08:30 [INFO] Unweighted rewards: [400, 2]
15:08:30 [INFO] Weighted rewards: [400]
15:08:30 [INFO] Loss tensor: [1] [53490.34765625]
15:08:30 [INFO] Mean epoch policy entropy for the trajectories is: 0.83381385
15:08:38 [INFO] Training epoch 99
15:08:41 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:08:41 [INFO] Size of sampled logits tensor [400, 2, 1]
15:08:41 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:08:41 [INFO] Unweighted rewards: [400, 2]
15:08:41 [INFO] Weighted rewards: [400]
15:08:41 [INFO] Loss tensor: [1] [63115.16015625]
15:08:42 [INFO] Mean epoch policy entropy for the trajectories is: 0.915748
15:08:48 [INFO] Training epoch 100
15:08:52 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:08:52 [INFO] Size of sampled logits tensor [400, 2, 1]
15:08:52 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:08:52 [INFO] Unweighted rewards: [400, 2]
15:08:52 [INFO] Weighted rewards: [400]
15:08:52 [INFO] Loss tensor: [1] [52915.859375]
15:08:52 [INFO] Mean epoch policy entropy for the trajectories is: 0.8396184
15:08:59 [INFO] Training epoch 101
15:09:02 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:09:02 [INFO] Size of sampled logits tensor [400, 2, 1]
15:09:02 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:09:02 [INFO] Unweighted rewards: [400, 2]
15:09:02 [INFO] Weighted rewards: [400]
15:09:02 [INFO] Loss tensor: [1] [60522.2890625]
15:09:02 [INFO] Mean epoch policy entropy for the trajectories is: 0.8254648
15:09:08 [INFO] Training epoch 102
15:09:11 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:09:11 [INFO] Size of sampled logits tensor [400, 2, 1]
15:09:11 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:09:11 [INFO] Unweighted rewards: [400, 2]
15:09:11 [INFO] Weighted rewards: [400]
15:09:11 [INFO] Loss tensor: [1] [79466.609375]
15:09:12 [INFO] Mean epoch policy entropy for the trajectories is: 0.86711603
15:09:17 [INFO] Training epoch 103
15:09:20 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:09:20 [INFO] Size of sampled logits tensor [400, 2, 1]
15:09:20 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:09:20 [INFO] Unweighted rewards: [400, 2]
15:09:20 [INFO] Weighted rewards: [400]
15:09:20 [INFO] Loss tensor: [1] [54523.0390625]
15:09:21 [INFO] Mean epoch policy entropy for the trajectories is: 0.89399207
15:09:26 [INFO] Training epoch 104
15:09:29 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:09:29 [INFO] Size of sampled logits tensor [400, 2, 1]
15:09:29 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:09:29 [INFO] Unweighted rewards: [400, 2]
15:09:29 [INFO] Weighted rewards: [400]
15:09:29 [INFO] Loss tensor: [1] [33105.61328125]
15:09:30 [INFO] Mean epoch policy entropy for the trajectories is: 0.89430654
15:09:35 [INFO] Training epoch 105
15:09:38 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:09:38 [INFO] Size of sampled logits tensor [400, 2, 1]
15:09:38 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:09:38 [INFO] Unweighted rewards: [400, 2]
15:09:38 [INFO] Weighted rewards: [400]
15:09:38 [INFO] Loss tensor: [1] [95331.296875]
15:09:38 [INFO] Mean epoch policy entropy for the trajectories is: 0.90681684
15:09:44 [INFO] Training epoch 106
15:09:47 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:09:47 [INFO] Size of sampled logits tensor [400, 2, 1]
15:09:47 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:09:47 [INFO] Unweighted rewards: [400, 2]
15:09:47 [INFO] Weighted rewards: [400]
15:09:47 [INFO] Loss tensor: [1] [72369.6484375]
15:09:47 [INFO] Mean epoch policy entropy for the trajectories is: 0.7846363
15:09:52 [INFO] Training epoch 107
15:09:55 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:09:55 [INFO] Size of sampled logits tensor [400, 2, 1]
15:09:55 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:09:55 [INFO] Unweighted rewards: [400, 2]
15:09:55 [INFO] Weighted rewards: [400]
15:09:55 [INFO] Loss tensor: [1] [32516.138671875]
15:09:56 [INFO] Mean epoch policy entropy for the trajectories is: 0.81125426
15:10:01 [INFO] Training epoch 108
15:10:04 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:10:04 [INFO] Size of sampled logits tensor [400, 2, 1]
15:10:04 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:10:04 [INFO] Unweighted rewards: [400, 2]
15:10:04 [INFO] Weighted rewards: [400]
15:10:04 [INFO] Loss tensor: [1] [67353.6796875]
15:10:04 [INFO] Mean epoch policy entropy for the trajectories is: 0.88579005
15:10:09 [INFO] Training epoch 109
15:10:12 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:10:12 [INFO] Size of sampled logits tensor [400, 2, 1]
15:10:12 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:10:12 [INFO] Unweighted rewards: [400, 2]
15:10:12 [INFO] Weighted rewards: [400]
15:10:12 [INFO] Loss tensor: [1] [52323.05078125]
15:10:12 [INFO] Mean epoch policy entropy for the trajectories is: 0.8844545
15:10:17 [INFO] Training epoch 110
15:10:20 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:10:20 [INFO] Size of sampled logits tensor [400, 2, 1]
15:10:20 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:10:20 [INFO] Unweighted rewards: [400, 2]
15:10:20 [INFO] Weighted rewards: [400]
15:10:20 [INFO] Loss tensor: [1] [116422.2265625]
15:10:21 [INFO] Mean epoch policy entropy for the trajectories is: 0.8218348
15:10:26 [INFO] Training epoch 111
15:10:30 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:10:30 [INFO] Size of sampled logits tensor [400, 2, 1]
15:10:30 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:10:30 [INFO] Unweighted rewards: [400, 2]
15:10:30 [INFO] Weighted rewards: [400]
15:10:30 [INFO] Loss tensor: [1] [32105.91015625]
15:10:30 [INFO] Mean epoch policy entropy for the trajectories is: 0.8102494
15:10:34 [INFO] Training epoch 112
15:12:19 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:12:19 [INFO] Size of sampled logits tensor [400, 2, 1]
15:12:19 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:12:19 [INFO] Unweighted rewards: [400, 2]
15:12:19 [INFO] Weighted rewards: [400]
15:12:19 [INFO] Loss tensor: [1] [52621.98828125]
15:12:19 [INFO] Mean epoch policy entropy for the trajectories is: 0.85556257
15:12:24 [INFO] Training epoch 113
15:12:25 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:12:25 [INFO] Size of sampled logits tensor [400, 2, 1]
15:12:25 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:12:25 [INFO] Unweighted rewards: [400, 2]
15:12:25 [INFO] Weighted rewards: [400]
15:12:25 [INFO] Loss tensor: [1] [52188.16796875]
15:12:25 [INFO] Mean epoch policy entropy for the trajectories is: 0.893653
15:12:30 [INFO] Training epoch 114
15:12:31 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:12:31 [INFO] Size of sampled logits tensor [400, 2, 1]
15:12:31 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:12:31 [INFO] Unweighted rewards: [400, 2]
15:12:31 [INFO] Weighted rewards: [400]
15:12:31 [INFO] Loss tensor: [1] [72557.90625]
15:12:31 [INFO] Mean epoch policy entropy for the trajectories is: 0.8677877
15:12:36 [INFO] Training epoch 115
15:12:37 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:12:37 [INFO] Size of sampled logits tensor [400, 2, 1]
15:12:37 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:12:37 [INFO] Unweighted rewards: [400, 2]
15:12:37 [INFO] Weighted rewards: [400]
15:12:37 [INFO] Loss tensor: [1] [53058.296875]
15:12:37 [INFO] Mean epoch policy entropy for the trajectories is: 0.79797125
15:12:42 [INFO] Training epoch 116
15:12:43 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:12:43 [INFO] Size of sampled logits tensor [400, 2, 1]
15:12:43 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:12:43 [INFO] Unweighted rewards: [400, 2]
15:12:43 [INFO] Weighted rewards: [400]
15:12:43 [INFO] Loss tensor: [1] [66681.8671875]
15:12:43 [INFO] Mean epoch policy entropy for the trajectories is: 0.78911483
15:12:48 [INFO] Training epoch 117
15:12:49 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:12:49 [INFO] Size of sampled logits tensor [400, 2, 1]
15:12:49 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:12:49 [INFO] Unweighted rewards: [400, 2]
15:12:49 [INFO] Weighted rewards: [400]
15:12:49 [INFO] Loss tensor: [1] [92951.65625]
15:12:49 [INFO] Mean epoch policy entropy for the trajectories is: 0.83062255
15:12:54 [INFO] Training epoch 118
15:12:55 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:12:55 [INFO] Size of sampled logits tensor [400, 2, 1]
15:12:55 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:12:55 [INFO] Unweighted rewards: [400, 2]
15:12:55 [INFO] Weighted rewards: [400]
15:12:55 [INFO] Loss tensor: [1] [85833.171875]
15:12:55 [INFO] Mean epoch policy entropy for the trajectories is: 0.82134897
15:13:00 [INFO] Training epoch 119
15:13:01 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:13:01 [INFO] Size of sampled logits tensor [400, 2, 1]
15:13:01 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:13:01 [INFO] Unweighted rewards: [400, 2]
15:13:01 [INFO] Weighted rewards: [400]
15:13:01 [INFO] Loss tensor: [1] [88956.296875]
15:13:02 [INFO] Mean epoch policy entropy for the trajectories is: 0.83360374
15:13:06 [INFO] Training epoch 120
15:13:07 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:13:07 [INFO] Size of sampled logits tensor [400, 2, 1]
15:13:07 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:13:07 [INFO] Unweighted rewards: [400, 2]
15:13:07 [INFO] Weighted rewards: [400]
15:13:07 [INFO] Loss tensor: [1] [94342.6796875]
15:13:07 [INFO] Mean epoch policy entropy for the trajectories is: 0.8278632
15:13:12 [INFO] Training epoch 121
15:13:13 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:13:13 [INFO] Size of sampled logits tensor [400, 2, 1]
15:13:13 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:13:13 [INFO] Unweighted rewards: [400, 2]
15:13:13 [INFO] Weighted rewards: [400]
15:13:13 [INFO] Loss tensor: [1] [71849.515625]
15:13:13 [INFO] Mean epoch policy entropy for the trajectories is: 0.9275929
15:13:18 [INFO] Training epoch 122
15:13:19 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:13:19 [INFO] Size of sampled logits tensor [400, 2, 1]
15:13:19 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:13:19 [INFO] Unweighted rewards: [400, 2]
15:13:19 [INFO] Weighted rewards: [400]
15:13:19 [INFO] Loss tensor: [1] [72740.6640625]
15:13:19 [INFO] Mean epoch policy entropy for the trajectories is: 0.9036977
15:13:24 [INFO] Training epoch 123
15:13:25 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:13:25 [INFO] Size of sampled logits tensor [400, 2, 1]
15:13:25 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:13:25 [INFO] Unweighted rewards: [400, 2]
15:13:25 [INFO] Weighted rewards: [400]
15:13:25 [INFO] Loss tensor: [1] [71715.9453125]
15:13:25 [INFO] Mean epoch policy entropy for the trajectories is: 0.7658996
15:13:30 [INFO] Training epoch 124
15:13:31 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
15:13:31 [INFO] Size of sampled logits tensor [400, 2, 1]
15:13:31 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
15:13:31 [INFO] Unweighted rewards: [400, 2]
15:13:31 [INFO] Weighted rewards: [400]
15:13:31 [INFO] Loss tensor: [1] [72144.3515625]
15:13:31 [INFO] Mean epoch policy entropy for the trajectories is: 0.7837033
15:13:35 [INFO] The random scrambling applied these moves: [[RPlus, DPlus], [RPlus, LMinus], [UPlus, LPlus], [FMinus, LMinus], [LMinus, LPlus], [UMinus, DMinus], [LPlus, UMinus], [BMinus, LPlus], [BPlus, FMinus], [LMinus, RMinus]] for testing
15:13:35 [INFO] Running the test loop now: 
15:13:35 [INFO] Number of tests: 10
15:13:35 [INFO] Scrambled cube used for test 0 used this moves: [RPlus, DPlus]
15:13:36 [INFO] Got the cubemove from policy: DMinus for turn 0
15:13:36 [INFO] The above is complementary or not: true
15:13:36 [INFO] The cross entropy for turn 0 0.0067119002
15:13:36 [INFO] Got the cubemove from policy: RMinus for turn 1
15:13:36 [INFO] The above is complementary or not: true
15:13:36 [INFO] The cross entropy for turn 1 4.191942e-5
15:13:36 [INFO] Scrambled cube used for test 1 used this moves: [RPlus, LMinus]
15:13:36 [INFO] Got the cubemove from policy: LPlus for turn 0
15:13:36 [INFO] The above is complementary or not: true
15:13:36 [INFO] The cross entropy for turn 0 0.00012493346
15:13:36 [INFO] Got the cubemove from policy: RMinus for turn 1
15:13:36 [INFO] The above is complementary or not: true
15:13:36 [INFO] The cross entropy for turn 1 4.191942e-5
15:13:36 [INFO] Scrambled cube used for test 2 used this moves: [UPlus, LPlus]
15:13:36 [INFO] Got the cubemove from policy: RPlus for turn 0
15:13:36 [INFO] The above is complementary or not: false
15:13:36 [INFO] The cross entropy for turn 0 2.3260171
15:13:36 [INFO] Got the cubemove from policy: RMinus for turn 1
15:13:36 [INFO] The above is complementary or not: false
15:13:36 [INFO] The cross entropy for turn 1 0.03705398
15:13:36 [INFO] Scrambled cube used for test 3 used this moves: [FMinus, LMinus]
15:13:36 [INFO] Got the cubemove from policy: LPlus for turn 0
15:13:36 [INFO] The above is complementary or not: true
15:13:36 [INFO] The cross entropy for turn 0 0.0033612074
15:13:36 [INFO] Got the cubemove from policy: FPlus for turn 1
15:13:36 [INFO] The above is complementary or not: true
15:13:36 [INFO] The cross entropy for turn 1 5.1185518e-5
15:13:36 [INFO] Scrambled cube used for test 4 used this moves: [LMinus, LPlus]
15:13:36 [INFO] Got the cubemove from policy: RPlus for turn 0
15:13:36 [INFO] The above is complementary or not: false
15:13:36 [INFO] The cross entropy for turn 0 0.06770018
15:13:36 [INFO] Got the cubemove from policy: RMinus for turn 1
15:13:36 [INFO] The above is complementary or not: false
15:13:36 [INFO] The cross entropy for turn 1 4.191942e-5
15:13:36 [INFO] Scrambled cube used for test 5 used this moves: [UMinus, DMinus]
15:13:36 [INFO] Got the cubemove from policy: UPlus for turn 0
15:13:36 [INFO] The above is complementary or not: false
15:13:36 [INFO] The cross entropy for turn 0 0.49215457
15:13:36 [INFO] Got the cubemove from policy: NoOp for turn 1
15:13:36 [INFO] The above is complementary or not: false
15:13:36 [INFO] The cross entropy for turn 1 2.5649495
15:13:36 [INFO] Scrambled cube used for test 6 used this moves: [LPlus, UMinus]
15:13:36 [INFO] Got the cubemove from policy: UPlus for turn 0
15:13:36 [INFO] The above is complementary or not: true
15:13:36 [INFO] The cross entropy for turn 0 0.0637749
15:13:36 [INFO] Got the cubemove from policy: LPlus for turn 1
15:13:36 [INFO] The above is complementary or not: false
15:13:36 [INFO] The cross entropy for turn 1 2.5649495
15:13:36 [INFO] Scrambled cube used for test 7 used this moves: [BMinus, LPlus]
15:13:36 [INFO] Got the cubemove from policy: LPlus for turn 0
15:13:36 [INFO] The above is complementary or not: false
15:13:36 [INFO] The cross entropy for turn 0 2.5649495
15:13:36 [INFO] Got the cubemove from policy: LPlus for turn 1
15:13:36 [INFO] The above is complementary or not: false
15:13:36 [INFO] The cross entropy for turn 1 1.0009358
15:13:36 [INFO] Scrambled cube used for test 8 used this moves: [BPlus, FMinus]
15:13:36 [INFO] Got the cubemove from policy: FPlus for turn 0
15:13:36 [INFO] The above is complementary or not: true
15:13:36 [INFO] The cross entropy for turn 0 0.014758853
15:13:36 [INFO] Got the cubemove from policy: BMinus for turn 1
15:13:36 [INFO] The above is complementary or not: true
15:13:36 [INFO] The cross entropy for turn 1 2.5649495
15:13:36 [INFO] Scrambled cube used for test 9 used this moves: [LMinus, RMinus]
15:13:36 [INFO] Got the cubemove from policy: LPlus for turn 0
15:13:36 [INFO] The above is complementary or not: false
15:13:36 [INFO] The cross entropy for turn 0 0.00014363733
15:13:36 [INFO] Got the cubemove from policy: RPlus for turn 1
15:13:36 [INFO] The above is complementary or not: false
15:13:36 [INFO] The cross entropy for turn 1 4.2003932e-5
15:13:36 [INFO] Mean loss for tests 0.0
15:13:36 [INFO] Mean entropy for tests 0.71363777
15:13:36 [INFO] Mean correct moves for 0.9
