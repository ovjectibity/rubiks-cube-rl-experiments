14:15:17 [INFO] Is MPS available: true
14:15:17 [INFO] Using the following hyperparameters:
14:15:17 [INFO] Hidden layer dimensions: 2500
14:15:17 [INFO] Number of layers: 4
14:15:17 [INFO] Number of trajectories: 400
14:15:17 [INFO] Trajectory depth: 2
14:15:17 [INFO] Number of epochs: 150
14:15:17 [INFO] Learning rate: 0.0001
14:15:17 [INFO] Starting policy training...
14:15:17 [INFO] Training epoch 0
14:15:19 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:15:19 [INFO] Size of sampled logits tensor [400, 2, 1]
14:15:19 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:15:19 [INFO] Unweighted rewards: [400, 2]
14:15:19 [INFO] Weighted rewards: [400]
14:15:19 [INFO] Loss tensor: [1] [-201524.796875]
14:15:19 [INFO] Mean epoch policy entropy for the trajectories is: 2.5606802
14:15:22 [INFO] Training epoch 1
14:15:24 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:15:24 [INFO] Size of sampled logits tensor [400, 2, 1]
14:15:24 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:15:24 [INFO] Unweighted rewards: [400, 2]
14:15:24 [INFO] Weighted rewards: [400]
14:15:24 [INFO] Loss tensor: [1] [-177337.359375]
14:15:24 [INFO] Mean epoch policy entropy for the trajectories is: 2.547157
14:15:27 [INFO] Training epoch 2
14:15:28 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:15:28 [INFO] Size of sampled logits tensor [400, 2, 1]
14:15:28 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:15:28 [INFO] Unweighted rewards: [400, 2]
14:15:28 [INFO] Weighted rewards: [400]
14:15:28 [INFO] Loss tensor: [1] [-165604.265625]
14:15:28 [INFO] Mean epoch policy entropy for the trajectories is: 2.5321164
14:15:31 [INFO] Training epoch 3
14:15:33 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:15:33 [INFO] Size of sampled logits tensor [400, 2, 1]
14:15:33 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:15:33 [INFO] Unweighted rewards: [400, 2]
14:15:33 [INFO] Weighted rewards: [400]
14:15:33 [INFO] Loss tensor: [1] [-97728.3359375]
14:15:33 [INFO] Mean epoch policy entropy for the trajectories is: 2.4656706
14:15:36 [INFO] Training epoch 4
14:15:37 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:15:37 [INFO] Size of sampled logits tensor [400, 2, 1]
14:15:37 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:15:37 [INFO] Unweighted rewards: [400, 2]
14:15:37 [INFO] Weighted rewards: [400]
14:15:37 [INFO] Loss tensor: [1] [-30730.46484375]
14:15:38 [INFO] Mean epoch policy entropy for the trajectories is: 2.4495156
14:15:41 [INFO] Training epoch 5
14:15:42 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:15:42 [INFO] Size of sampled logits tensor [400, 2, 1]
14:15:42 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:15:42 [INFO] Unweighted rewards: [400, 2]
14:15:42 [INFO] Weighted rewards: [400]
14:15:42 [INFO] Loss tensor: [1] [-47840.53515625]
14:15:42 [INFO] Mean epoch policy entropy for the trajectories is: 2.4429328
14:15:45 [INFO] Training epoch 6
14:15:47 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:15:47 [INFO] Size of sampled logits tensor [400, 2, 1]
14:15:47 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:15:47 [INFO] Unweighted rewards: [400, 2]
14:15:47 [INFO] Weighted rewards: [400]
14:15:47 [INFO] Loss tensor: [1] [-37375.6875]
14:15:47 [INFO] Mean epoch policy entropy for the trajectories is: 2.4154787
14:15:50 [INFO] Training epoch 7
14:15:51 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:15:51 [INFO] Size of sampled logits tensor [400, 2, 1]
14:15:51 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:15:51 [INFO] Unweighted rewards: [400, 2]
14:15:51 [INFO] Weighted rewards: [400]
14:15:51 [INFO] Loss tensor: [1] [-35000.41796875]
14:15:51 [INFO] Mean epoch policy entropy for the trajectories is: 2.3433006
14:15:54 [INFO] Training epoch 8
14:15:56 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:15:56 [INFO] Size of sampled logits tensor [400, 2, 1]
14:15:56 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:15:56 [INFO] Unweighted rewards: [400, 2]
14:15:56 [INFO] Weighted rewards: [400]
14:15:56 [INFO] Loss tensor: [1] [-58426.390625]
14:15:56 [INFO] Mean epoch policy entropy for the trajectories is: 2.285882
14:15:59 [INFO] Training epoch 9
14:16:00 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:16:00 [INFO] Size of sampled logits tensor [400, 2, 1]
14:16:00 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:16:00 [INFO] Unweighted rewards: [400, 2]
14:16:00 [INFO] Weighted rewards: [400]
14:16:00 [INFO] Loss tensor: [1] [-97330.9375]
14:16:01 [INFO] Mean epoch policy entropy for the trajectories is: 2.1633725
14:16:04 [INFO] Training epoch 10
14:16:05 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:16:05 [INFO] Size of sampled logits tensor [400, 2, 1]
14:16:05 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:16:05 [INFO] Unweighted rewards: [400, 2]
14:16:05 [INFO] Weighted rewards: [400]
14:16:05 [INFO] Loss tensor: [1] [-43889.35546875]
14:16:05 [INFO] Mean epoch policy entropy for the trajectories is: 2.0046558
14:16:08 [INFO] Training epoch 11
14:16:10 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:16:10 [INFO] Size of sampled logits tensor [400, 2, 1]
14:16:10 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:16:10 [INFO] Unweighted rewards: [400, 2]
14:16:10 [INFO] Weighted rewards: [400]
14:16:10 [INFO] Loss tensor: [1] [-1954.9749755859375]
14:16:10 [INFO] Mean epoch policy entropy for the trajectories is: 1.9883298
14:16:13 [INFO] Training epoch 12
14:16:14 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:16:14 [INFO] Size of sampled logits tensor [400, 2, 1]
14:16:14 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:16:14 [INFO] Unweighted rewards: [400, 2]
14:16:14 [INFO] Weighted rewards: [400]
14:16:14 [INFO] Loss tensor: [1] [-53218.01953125]
14:16:14 [INFO] Mean epoch policy entropy for the trajectories is: 2.043293
14:16:17 [INFO] Training epoch 13
14:16:19 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:16:19 [INFO] Size of sampled logits tensor [400, 2, 1]
14:16:19 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:16:19 [INFO] Unweighted rewards: [400, 2]
14:16:19 [INFO] Weighted rewards: [400]
14:16:19 [INFO] Loss tensor: [1] [30411.017578125]
14:16:19 [INFO] Mean epoch policy entropy for the trajectories is: 2.050349
14:16:22 [INFO] Training epoch 14
14:16:23 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:16:23 [INFO] Size of sampled logits tensor [400, 2, 1]
14:16:23 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:16:23 [INFO] Unweighted rewards: [400, 2]
14:16:23 [INFO] Weighted rewards: [400]
14:16:23 [INFO] Loss tensor: [1] [2889.708740234375]
14:16:24 [INFO] Mean epoch policy entropy for the trajectories is: 2.1067796
14:16:27 [INFO] Training epoch 15
14:16:28 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:16:28 [INFO] Size of sampled logits tensor [400, 2, 1]
14:16:28 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:16:28 [INFO] Unweighted rewards: [400, 2]
14:16:28 [INFO] Weighted rewards: [400]
14:16:28 [INFO] Loss tensor: [1] [-45658.77734375]
14:16:28 [INFO] Mean epoch policy entropy for the trajectories is: 2.0533187
14:16:31 [INFO] Training epoch 16
14:16:33 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:16:33 [INFO] Size of sampled logits tensor [400, 2, 1]
14:16:33 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:16:33 [INFO] Unweighted rewards: [400, 2]
14:16:33 [INFO] Weighted rewards: [400]
14:16:33 [INFO] Loss tensor: [1] [-21226.822265625]
14:16:33 [INFO] Mean epoch policy entropy for the trajectories is: 1.9923048
14:16:36 [INFO] Training epoch 17
14:16:37 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:16:37 [INFO] Size of sampled logits tensor [400, 2, 1]
14:16:37 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:16:37 [INFO] Unweighted rewards: [400, 2]
14:16:37 [INFO] Weighted rewards: [400]
14:16:37 [INFO] Loss tensor: [1] [-135841.265625]
14:16:37 [INFO] Mean epoch policy entropy for the trajectories is: 1.9932011
14:16:40 [INFO] Training epoch 18
14:16:42 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:16:42 [INFO] Size of sampled logits tensor [400, 2, 1]
14:16:42 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:16:42 [INFO] Unweighted rewards: [400, 2]
14:16:42 [INFO] Weighted rewards: [400]
14:16:42 [INFO] Loss tensor: [1] [-37346.71484375]
14:16:42 [INFO] Mean epoch policy entropy for the trajectories is: 2.0044794
14:16:45 [INFO] Training epoch 19
14:16:46 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:16:47 [INFO] Size of sampled logits tensor [400, 2, 1]
14:16:47 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:16:47 [INFO] Unweighted rewards: [400, 2]
14:16:47 [INFO] Weighted rewards: [400]
14:16:47 [INFO] Loss tensor: [1] [-65270.22265625]
14:16:47 [INFO] Mean epoch policy entropy for the trajectories is: 2.0758667
14:16:50 [INFO] Training epoch 20
14:16:51 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:16:51 [INFO] Size of sampled logits tensor [400, 2, 1]
14:16:51 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:16:51 [INFO] Unweighted rewards: [400, 2]
14:16:51 [INFO] Weighted rewards: [400]
14:16:51 [INFO] Loss tensor: [1] [-145784.140625]
14:16:51 [INFO] Mean epoch policy entropy for the trajectories is: 2.06792
14:16:54 [INFO] Training epoch 21
14:16:56 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:16:56 [INFO] Size of sampled logits tensor [400, 2, 1]
14:16:56 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:16:56 [INFO] Unweighted rewards: [400, 2]
14:16:56 [INFO] Weighted rewards: [400]
14:16:56 [INFO] Loss tensor: [1] [-50127.21484375]
14:16:56 [INFO] Mean epoch policy entropy for the trajectories is: 2.0622299
14:16:59 [INFO] Training epoch 22
14:17:01 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:17:01 [INFO] Size of sampled logits tensor [400, 2, 1]
14:17:01 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:17:01 [INFO] Unweighted rewards: [400, 2]
14:17:01 [INFO] Weighted rewards: [400]
14:17:01 [INFO] Loss tensor: [1] [-127624.734375]
14:17:01 [INFO] Mean epoch policy entropy for the trajectories is: 2.1494782
14:17:04 [INFO] Training epoch 23
14:17:05 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:17:05 [INFO] Size of sampled logits tensor [400, 2, 1]
14:17:05 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:17:05 [INFO] Unweighted rewards: [400, 2]
14:17:05 [INFO] Weighted rewards: [400]
14:17:05 [INFO] Loss tensor: [1] [-60217.44921875]
14:17:05 [INFO] Mean epoch policy entropy for the trajectories is: 2.0776658
14:17:08 [INFO] Training epoch 24
14:17:10 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:17:10 [INFO] Size of sampled logits tensor [400, 2, 1]
14:17:10 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:17:10 [INFO] Unweighted rewards: [400, 2]
14:17:10 [INFO] Weighted rewards: [400]
14:17:10 [INFO] Loss tensor: [1] [-57773.73046875]
14:17:10 [INFO] Mean epoch policy entropy for the trajectories is: 2.044744
14:17:13 [INFO] Training epoch 25
14:17:15 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:17:15 [INFO] Size of sampled logits tensor [400, 2, 1]
14:17:15 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:17:15 [INFO] Unweighted rewards: [400, 2]
14:17:15 [INFO] Weighted rewards: [400]
14:17:15 [INFO] Loss tensor: [1] [-75300.4296875]
14:17:15 [INFO] Mean epoch policy entropy for the trajectories is: 2.0935438
14:17:18 [INFO] Training epoch 26
14:17:19 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:17:19 [INFO] Size of sampled logits tensor [400, 2, 1]
14:17:19 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:17:19 [INFO] Unweighted rewards: [400, 2]
14:17:19 [INFO] Weighted rewards: [400]
14:17:19 [INFO] Loss tensor: [1] [18876.904296875]
14:17:19 [INFO] Mean epoch policy entropy for the trajectories is: 1.947476
14:17:22 [INFO] Training epoch 27
14:17:24 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:17:24 [INFO] Size of sampled logits tensor [400, 2, 1]
14:17:24 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:17:24 [INFO] Unweighted rewards: [400, 2]
14:17:24 [INFO] Weighted rewards: [400]
14:17:24 [INFO] Loss tensor: [1] [-96283.8203125]
14:17:24 [INFO] Mean epoch policy entropy for the trajectories is: 1.9306483
14:17:27 [INFO] Training epoch 28
14:17:28 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:17:28 [INFO] Size of sampled logits tensor [400, 2, 1]
14:17:28 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:17:28 [INFO] Unweighted rewards: [400, 2]
14:17:28 [INFO] Weighted rewards: [400]
14:17:28 [INFO] Loss tensor: [1] [-75282.8125]
14:17:28 [INFO] Mean epoch policy entropy for the trajectories is: 1.9181752
14:17:31 [INFO] Training epoch 29
14:17:33 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:17:33 [INFO] Size of sampled logits tensor [400, 2, 1]
14:17:33 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:17:33 [INFO] Unweighted rewards: [400, 2]
14:17:33 [INFO] Weighted rewards: [400]
14:17:33 [INFO] Loss tensor: [1] [-71493.546875]
14:17:33 [INFO] Mean epoch policy entropy for the trajectories is: 1.799022
14:17:36 [INFO] Training epoch 30
14:17:37 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:17:37 [INFO] Size of sampled logits tensor [400, 2, 1]
14:17:37 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:17:37 [INFO] Unweighted rewards: [400, 2]
14:17:37 [INFO] Weighted rewards: [400]
14:17:37 [INFO] Loss tensor: [1] [-33419.75]
14:17:37 [INFO] Mean epoch policy entropy for the trajectories is: 1.7764068
14:17:40 [INFO] Training epoch 31
14:17:42 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:17:42 [INFO] Size of sampled logits tensor [400, 2, 1]
14:17:42 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:17:42 [INFO] Unweighted rewards: [400, 2]
14:17:42 [INFO] Weighted rewards: [400]
14:17:42 [INFO] Loss tensor: [1] [-90616.3515625]
14:17:42 [INFO] Mean epoch policy entropy for the trajectories is: 1.6848325
14:17:45 [INFO] Training epoch 32
14:17:46 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:17:46 [INFO] Size of sampled logits tensor [400, 2, 1]
14:17:46 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:17:46 [INFO] Unweighted rewards: [400, 2]
14:17:46 [INFO] Weighted rewards: [400]
14:17:46 [INFO] Loss tensor: [1] [-17283.220703125]
14:17:46 [INFO] Mean epoch policy entropy for the trajectories is: 1.7077605
14:17:49 [INFO] Training epoch 33
14:17:51 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:17:51 [INFO] Size of sampled logits tensor [400, 2, 1]
14:17:51 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:17:51 [INFO] Unweighted rewards: [400, 2]
14:17:51 [INFO] Weighted rewards: [400]
14:17:51 [INFO] Loss tensor: [1] [-49727.72265625]
14:17:51 [INFO] Mean epoch policy entropy for the trajectories is: 1.6509253
14:17:54 [INFO] Training epoch 34
14:17:55 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:17:55 [INFO] Size of sampled logits tensor [400, 2, 1]
14:17:55 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:17:55 [INFO] Unweighted rewards: [400, 2]
14:17:55 [INFO] Weighted rewards: [400]
14:17:55 [INFO] Loss tensor: [1] [3566.10986328125]
14:17:55 [INFO] Mean epoch policy entropy for the trajectories is: 1.6394098
14:17:58 [INFO] Training epoch 35
14:18:00 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:18:00 [INFO] Size of sampled logits tensor [400, 2, 1]
14:18:00 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:18:00 [INFO] Unweighted rewards: [400, 2]
14:18:00 [INFO] Weighted rewards: [400]
14:18:00 [INFO] Loss tensor: [1] [-19679.544921875]
14:18:00 [INFO] Mean epoch policy entropy for the trajectories is: 1.6448436
14:18:03 [INFO] Training epoch 36
14:18:04 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:18:04 [INFO] Size of sampled logits tensor [400, 2, 1]
14:18:04 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:18:04 [INFO] Unweighted rewards: [400, 2]
14:18:04 [INFO] Weighted rewards: [400]
14:18:04 [INFO] Loss tensor: [1] [-145890.5625]
14:18:04 [INFO] Mean epoch policy entropy for the trajectories is: 1.6779689
14:18:07 [INFO] Training epoch 37
14:18:09 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:18:09 [INFO] Size of sampled logits tensor [400, 2, 1]
14:18:09 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:18:09 [INFO] Unweighted rewards: [400, 2]
14:18:09 [INFO] Weighted rewards: [400]
14:18:09 [INFO] Loss tensor: [1] [-72710.7578125]
14:18:09 [INFO] Mean epoch policy entropy for the trajectories is: 1.6853322
14:18:12 [INFO] Training epoch 38
14:18:13 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:18:13 [INFO] Size of sampled logits tensor [400, 2, 1]
14:18:13 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:18:13 [INFO] Unweighted rewards: [400, 2]
14:18:13 [INFO] Weighted rewards: [400]
14:18:13 [INFO] Loss tensor: [1] [-37815.48828125]
14:18:13 [INFO] Mean epoch policy entropy for the trajectories is: 1.6472461
14:18:16 [INFO] Training epoch 39
14:18:18 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:18:18 [INFO] Size of sampled logits tensor [400, 2, 1]
14:18:18 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:18:18 [INFO] Unweighted rewards: [400, 2]
14:18:18 [INFO] Weighted rewards: [400]
14:18:18 [INFO] Loss tensor: [1] [34124.453125]
14:18:18 [INFO] Mean epoch policy entropy for the trajectories is: 1.6467955
14:18:21 [INFO] Training epoch 40
14:18:22 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:18:22 [INFO] Size of sampled logits tensor [400, 2, 1]
14:18:22 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:18:22 [INFO] Unweighted rewards: [400, 2]
14:18:22 [INFO] Weighted rewards: [400]
14:18:22 [INFO] Loss tensor: [1] [-53682.26953125]
14:18:22 [INFO] Mean epoch policy entropy for the trajectories is: 1.6839179
14:18:25 [INFO] Training epoch 41
14:18:27 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:18:27 [INFO] Size of sampled logits tensor [400, 2, 1]
14:18:27 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:18:27 [INFO] Unweighted rewards: [400, 2]
14:18:27 [INFO] Weighted rewards: [400]
14:18:27 [INFO] Loss tensor: [1] [-96517.6171875]
14:18:27 [INFO] Mean epoch policy entropy for the trajectories is: 1.7817057
14:18:30 [INFO] Training epoch 42
14:18:31 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:18:31 [INFO] Size of sampled logits tensor [400, 2, 1]
14:18:31 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:18:31 [INFO] Unweighted rewards: [400, 2]
14:18:31 [INFO] Weighted rewards: [400]
14:18:31 [INFO] Loss tensor: [1] [-167356.453125]
14:18:31 [INFO] Mean epoch policy entropy for the trajectories is: 1.7582681
14:18:34 [INFO] Training epoch 43
14:18:36 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:18:36 [INFO] Size of sampled logits tensor [400, 2, 1]
14:18:36 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:18:36 [INFO] Unweighted rewards: [400, 2]
14:18:36 [INFO] Weighted rewards: [400]
14:18:36 [INFO] Loss tensor: [1] [-167789.4375]
14:18:36 [INFO] Mean epoch policy entropy for the trajectories is: 1.6487224
14:18:39 [INFO] Training epoch 44
14:18:40 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:18:40 [INFO] Size of sampled logits tensor [400, 2, 1]
14:18:40 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:18:40 [INFO] Unweighted rewards: [400, 2]
14:18:40 [INFO] Weighted rewards: [400]
14:18:40 [INFO] Loss tensor: [1] [-98698.4453125]
14:18:40 [INFO] Mean epoch policy entropy for the trajectories is: 1.712091
14:18:43 [INFO] Training epoch 45
14:18:45 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:18:45 [INFO] Size of sampled logits tensor [400, 2, 1]
14:18:45 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:18:45 [INFO] Unweighted rewards: [400, 2]
14:18:45 [INFO] Weighted rewards: [400]
14:18:45 [INFO] Loss tensor: [1] [-36102.74609375]
14:18:45 [INFO] Mean epoch policy entropy for the trajectories is: 1.7435594
14:18:48 [INFO] Training epoch 46
14:18:49 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:18:49 [INFO] Size of sampled logits tensor [400, 2, 1]
14:18:49 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:18:49 [INFO] Unweighted rewards: [400, 2]
14:18:49 [INFO] Weighted rewards: [400]
14:18:49 [INFO] Loss tensor: [1] [-137536.484375]
14:18:49 [INFO] Mean epoch policy entropy for the trajectories is: 1.6959331
14:18:52 [INFO] Training epoch 47
14:18:54 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:18:54 [INFO] Size of sampled logits tensor [400, 2, 1]
14:18:54 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:18:54 [INFO] Unweighted rewards: [400, 2]
14:18:54 [INFO] Weighted rewards: [400]
14:18:54 [INFO] Loss tensor: [1] [-70486.3671875]
14:18:54 [INFO] Mean epoch policy entropy for the trajectories is: 1.7232937
14:18:57 [INFO] Training epoch 48
14:18:58 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:18:58 [INFO] Size of sampled logits tensor [400, 2, 1]
14:18:58 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:18:58 [INFO] Unweighted rewards: [400, 2]
14:18:58 [INFO] Weighted rewards: [400]
14:18:58 [INFO] Loss tensor: [1] [-94555.7578125]
14:18:58 [INFO] Mean epoch policy entropy for the trajectories is: 1.8005469
14:19:02 [INFO] Training epoch 49
14:19:03 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:19:03 [INFO] Size of sampled logits tensor [400, 2, 1]
14:19:03 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:19:03 [INFO] Unweighted rewards: [400, 2]
14:19:03 [INFO] Weighted rewards: [400]
14:19:03 [INFO] Loss tensor: [1] [-80468.328125]
14:19:03 [INFO] Mean epoch policy entropy for the trajectories is: 1.8530675
14:19:06 [INFO] Training epoch 50
14:19:07 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:19:07 [INFO] Size of sampled logits tensor [400, 2, 1]
14:19:07 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:19:07 [INFO] Unweighted rewards: [400, 2]
14:19:07 [INFO] Weighted rewards: [400]
14:19:07 [INFO] Loss tensor: [1] [-99746.5]
14:19:08 [INFO] Mean epoch policy entropy for the trajectories is: 1.7760724
14:19:11 [INFO] Training epoch 51
14:19:12 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:19:12 [INFO] Size of sampled logits tensor [400, 2, 1]
14:19:12 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:19:12 [INFO] Unweighted rewards: [400, 2]
14:19:12 [INFO] Weighted rewards: [400]
14:19:12 [INFO] Loss tensor: [1] [-33449.09765625]
14:19:12 [INFO] Mean epoch policy entropy for the trajectories is: 1.7970477
14:19:15 [INFO] Training epoch 52
14:19:16 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:19:16 [INFO] Size of sampled logits tensor [400, 2, 1]
14:19:16 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:19:16 [INFO] Unweighted rewards: [400, 2]
14:19:16 [INFO] Weighted rewards: [400]
14:19:16 [INFO] Loss tensor: [1] [-155655.859375]
14:19:17 [INFO] Mean epoch policy entropy for the trajectories is: 1.6838075
14:19:20 [INFO] Training epoch 53
14:19:21 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:19:21 [INFO] Size of sampled logits tensor [400, 2, 1]
14:19:21 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:19:21 [INFO] Unweighted rewards: [400, 2]
14:19:21 [INFO] Weighted rewards: [400]
14:19:21 [INFO] Loss tensor: [1] [-53224.93359375]
14:19:21 [INFO] Mean epoch policy entropy for the trajectories is: 1.7457936
14:19:24 [INFO] Training epoch 54
14:19:26 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:19:26 [INFO] Size of sampled logits tensor [400, 2, 1]
14:19:26 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:19:26 [INFO] Unweighted rewards: [400, 2]
14:19:26 [INFO] Weighted rewards: [400]
14:19:26 [INFO] Loss tensor: [1] [-87536.15625]
14:19:26 [INFO] Mean epoch policy entropy for the trajectories is: 1.7011778
14:19:29 [INFO] Training epoch 55
14:19:30 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:19:30 [INFO] Size of sampled logits tensor [400, 2, 1]
14:19:30 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:19:30 [INFO] Unweighted rewards: [400, 2]
14:19:30 [INFO] Weighted rewards: [400]
14:19:30 [INFO] Loss tensor: [1] [-6044.4697265625]
14:19:30 [INFO] Mean epoch policy entropy for the trajectories is: 1.7969792
14:19:34 [INFO] Training epoch 56
14:19:35 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:19:35 [INFO] Size of sampled logits tensor [400, 2, 1]
14:19:35 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:19:35 [INFO] Unweighted rewards: [400, 2]
14:19:35 [INFO] Weighted rewards: [400]
14:19:35 [INFO] Loss tensor: [1] [-133925.984375]
14:19:35 [INFO] Mean epoch policy entropy for the trajectories is: 1.7927746
14:19:38 [INFO] Training epoch 57
14:19:40 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:19:40 [INFO] Size of sampled logits tensor [400, 2, 1]
14:19:40 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:19:40 [INFO] Unweighted rewards: [400, 2]
14:19:40 [INFO] Weighted rewards: [400]
14:19:40 [INFO] Loss tensor: [1] [-86460.359375]
14:19:40 [INFO] Mean epoch policy entropy for the trajectories is: 1.8609815
14:19:43 [INFO] Training epoch 58
14:19:45 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:19:45 [INFO] Size of sampled logits tensor [400, 2, 1]
14:19:45 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:19:45 [INFO] Unweighted rewards: [400, 2]
14:19:45 [INFO] Weighted rewards: [400]
14:19:45 [INFO] Loss tensor: [1] [-57514.89453125]
14:19:45 [INFO] Mean epoch policy entropy for the trajectories is: 1.8551729
14:19:48 [INFO] Training epoch 59
14:19:49 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:19:49 [INFO] Size of sampled logits tensor [400, 2, 1]
14:19:49 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:19:49 [INFO] Unweighted rewards: [400, 2]
14:19:49 [INFO] Weighted rewards: [400]
14:19:49 [INFO] Loss tensor: [1] [-37670.51953125]
14:19:49 [INFO] Mean epoch policy entropy for the trajectories is: 1.7799295
14:19:53 [INFO] Training epoch 60
14:19:54 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:19:54 [INFO] Size of sampled logits tensor [400, 2, 1]
14:19:54 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:19:54 [INFO] Unweighted rewards: [400, 2]
14:19:54 [INFO] Weighted rewards: [400]
14:19:54 [INFO] Loss tensor: [1] [-78094.265625]
14:19:54 [INFO] Mean epoch policy entropy for the trajectories is: 1.7485427
14:19:57 [INFO] Training epoch 61
14:19:59 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:19:59 [INFO] Size of sampled logits tensor [400, 2, 1]
14:19:59 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:19:59 [INFO] Unweighted rewards: [400, 2]
14:19:59 [INFO] Weighted rewards: [400]
14:19:59 [INFO] Loss tensor: [1] [-43778.0703125]
14:19:59 [INFO] Mean epoch policy entropy for the trajectories is: 1.8438717
14:20:02 [INFO] Training epoch 62
14:20:03 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:20:03 [INFO] Size of sampled logits tensor [400, 2, 1]
14:20:03 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:20:03 [INFO] Unweighted rewards: [400, 2]
14:20:03 [INFO] Weighted rewards: [400]
14:20:03 [INFO] Loss tensor: [1] [-29030.126953125]
14:20:03 [INFO] Mean epoch policy entropy for the trajectories is: 1.8214631
14:20:06 [INFO] Training epoch 63
14:20:08 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:20:08 [INFO] Size of sampled logits tensor [400, 2, 1]
14:20:08 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:20:08 [INFO] Unweighted rewards: [400, 2]
14:20:08 [INFO] Weighted rewards: [400]
14:20:08 [INFO] Loss tensor: [1] [68427.9765625]
14:20:08 [INFO] Mean epoch policy entropy for the trajectories is: 1.838053
14:20:11 [INFO] Training epoch 64
14:20:12 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:20:12 [INFO] Size of sampled logits tensor [400, 2, 1]
14:20:12 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:20:12 [INFO] Unweighted rewards: [400, 2]
14:20:12 [INFO] Weighted rewards: [400]
14:20:12 [INFO] Loss tensor: [1] [-58731.6796875]
14:20:12 [INFO] Mean epoch policy entropy for the trajectories is: 1.8021027
14:20:15 [INFO] Training epoch 65
14:20:17 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:20:17 [INFO] Size of sampled logits tensor [400, 2, 1]
14:20:17 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:20:17 [INFO] Unweighted rewards: [400, 2]
14:20:17 [INFO] Weighted rewards: [400]
14:20:17 [INFO] Loss tensor: [1] [-74159.5234375]
14:20:17 [INFO] Mean epoch policy entropy for the trajectories is: 1.8259703
14:20:20 [INFO] Training epoch 66
14:20:21 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:20:21 [INFO] Size of sampled logits tensor [400, 2, 1]
14:20:21 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:20:21 [INFO] Unweighted rewards: [400, 2]
14:20:21 [INFO] Weighted rewards: [400]
14:20:21 [INFO] Loss tensor: [1] [-46703.50390625]
14:20:21 [INFO] Mean epoch policy entropy for the trajectories is: 1.7837319
14:20:24 [INFO] Training epoch 67
14:20:26 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:20:26 [INFO] Size of sampled logits tensor [400, 2, 1]
14:20:26 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:20:26 [INFO] Unweighted rewards: [400, 2]
14:20:26 [INFO] Weighted rewards: [400]
14:20:26 [INFO] Loss tensor: [1] [-45291.8203125]
14:20:26 [INFO] Mean epoch policy entropy for the trajectories is: 1.7070549
14:20:29 [INFO] Training epoch 68
14:20:30 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:20:30 [INFO] Size of sampled logits tensor [400, 2, 1]
14:20:30 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:20:30 [INFO] Unweighted rewards: [400, 2]
14:20:30 [INFO] Weighted rewards: [400]
14:20:30 [INFO] Loss tensor: [1] [-96196.1953125]
14:20:30 [INFO] Mean epoch policy entropy for the trajectories is: 1.7395432
14:20:33 [INFO] Training epoch 69
14:20:35 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:20:35 [INFO] Size of sampled logits tensor [400, 2, 1]
14:20:35 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:20:35 [INFO] Unweighted rewards: [400, 2]
14:20:35 [INFO] Weighted rewards: [400]
14:20:35 [INFO] Loss tensor: [1] [34238.53515625]
14:20:35 [INFO] Mean epoch policy entropy for the trajectories is: 1.7515393
14:20:38 [INFO] Training epoch 70
14:20:39 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:20:39 [INFO] Size of sampled logits tensor [400, 2, 1]
14:20:39 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:20:39 [INFO] Unweighted rewards: [400, 2]
14:20:39 [INFO] Weighted rewards: [400]
14:20:39 [INFO] Loss tensor: [1] [-66387.2109375]
14:20:39 [INFO] Mean epoch policy entropy for the trajectories is: 1.7376342
14:20:42 [INFO] Training epoch 71
14:20:44 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:20:44 [INFO] Size of sampled logits tensor [400, 2, 1]
14:20:44 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:20:44 [INFO] Unweighted rewards: [400, 2]
14:20:44 [INFO] Weighted rewards: [400]
14:20:44 [INFO] Loss tensor: [1] [22184.12890625]
14:20:44 [INFO] Mean epoch policy entropy for the trajectories is: 1.7367733
14:20:47 [INFO] Training epoch 72
14:20:48 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:20:48 [INFO] Size of sampled logits tensor [400, 2, 1]
14:20:48 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:20:48 [INFO] Unweighted rewards: [400, 2]
14:20:48 [INFO] Weighted rewards: [400]
14:20:48 [INFO] Loss tensor: [1] [-8211.8984375]
14:20:48 [INFO] Mean epoch policy entropy for the trajectories is: 1.6760904
14:20:51 [INFO] Training epoch 73
14:20:53 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:20:53 [INFO] Size of sampled logits tensor [400, 2, 1]
14:20:53 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:20:53 [INFO] Unweighted rewards: [400, 2]
14:20:53 [INFO] Weighted rewards: [400]
14:20:53 [INFO] Loss tensor: [1] [-115448.875]
14:20:53 [INFO] Mean epoch policy entropy for the trajectories is: 1.8526028
14:20:56 [INFO] Training epoch 74
14:20:57 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:20:57 [INFO] Size of sampled logits tensor [400, 2, 1]
14:20:57 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:20:57 [INFO] Unweighted rewards: [400, 2]
14:20:57 [INFO] Weighted rewards: [400]
14:20:57 [INFO] Loss tensor: [1] [-105222.5390625]
14:20:57 [INFO] Mean epoch policy entropy for the trajectories is: 1.7967354
14:21:00 [INFO] Training epoch 75
14:21:02 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:21:02 [INFO] Size of sampled logits tensor [400, 2, 1]
14:21:02 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:21:02 [INFO] Unweighted rewards: [400, 2]
14:21:02 [INFO] Weighted rewards: [400]
14:21:02 [INFO] Loss tensor: [1] [-109567.9765625]
14:21:02 [INFO] Mean epoch policy entropy for the trajectories is: 1.8601747
14:21:05 [INFO] Training epoch 76
14:21:06 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:21:06 [INFO] Size of sampled logits tensor [400, 2, 1]
14:21:06 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:21:06 [INFO] Unweighted rewards: [400, 2]
14:21:06 [INFO] Weighted rewards: [400]
14:21:06 [INFO] Loss tensor: [1] [-61501.01953125]
14:21:06 [INFO] Mean epoch policy entropy for the trajectories is: 1.7842896
14:21:09 [INFO] Training epoch 77
14:21:11 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:21:11 [INFO] Size of sampled logits tensor [400, 2, 1]
14:21:11 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:21:11 [INFO] Unweighted rewards: [400, 2]
14:21:11 [INFO] Weighted rewards: [400]
14:21:11 [INFO] Loss tensor: [1] [-27204.26953125]
14:21:11 [INFO] Mean epoch policy entropy for the trajectories is: 1.7387985
14:21:14 [INFO] Training epoch 78
14:21:15 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:21:15 [INFO] Size of sampled logits tensor [400, 2, 1]
14:21:15 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:21:15 [INFO] Unweighted rewards: [400, 2]
14:21:15 [INFO] Weighted rewards: [400]
14:21:15 [INFO] Loss tensor: [1] [-175673.703125]
14:21:15 [INFO] Mean epoch policy entropy for the trajectories is: 1.8247404
14:21:18 [INFO] Training epoch 79
14:21:20 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:21:20 [INFO] Size of sampled logits tensor [400, 2, 1]
14:21:20 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:21:20 [INFO] Unweighted rewards: [400, 2]
14:21:20 [INFO] Weighted rewards: [400]
14:21:20 [INFO] Loss tensor: [1] [-22777.419921875]
14:21:20 [INFO] Mean epoch policy entropy for the trajectories is: 1.7599216
14:21:23 [INFO] Training epoch 80
14:21:24 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:21:24 [INFO] Size of sampled logits tensor [400, 2, 1]
14:21:24 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:21:24 [INFO] Unweighted rewards: [400, 2]
14:21:24 [INFO] Weighted rewards: [400]
14:21:24 [INFO] Loss tensor: [1] [-47030.87890625]
14:21:24 [INFO] Mean epoch policy entropy for the trajectories is: 1.7702183
14:21:27 [INFO] Training epoch 81
14:21:29 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:21:29 [INFO] Size of sampled logits tensor [400, 2, 1]
14:21:29 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:21:29 [INFO] Unweighted rewards: [400, 2]
14:21:29 [INFO] Weighted rewards: [400]
14:21:29 [INFO] Loss tensor: [1] [-102073.21875]
14:21:29 [INFO] Mean epoch policy entropy for the trajectories is: 1.8525376
14:21:32 [INFO] Training epoch 82
14:21:33 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:21:33 [INFO] Size of sampled logits tensor [400, 2, 1]
14:21:33 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:21:33 [INFO] Unweighted rewards: [400, 2]
14:21:33 [INFO] Weighted rewards: [400]
14:21:33 [INFO] Loss tensor: [1] [-103302.6171875]
14:21:33 [INFO] Mean epoch policy entropy for the trajectories is: 1.8273958
14:21:36 [INFO] Training epoch 83
14:21:38 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:21:38 [INFO] Size of sampled logits tensor [400, 2, 1]
14:21:38 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:21:38 [INFO] Unweighted rewards: [400, 2]
14:21:38 [INFO] Weighted rewards: [400]
14:21:38 [INFO] Loss tensor: [1] [-74845.46875]
14:21:38 [INFO] Mean epoch policy entropy for the trajectories is: 1.6751927
14:21:41 [INFO] Training epoch 84
14:21:42 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:21:42 [INFO] Size of sampled logits tensor [400, 2, 1]
14:21:42 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:21:42 [INFO] Unweighted rewards: [400, 2]
14:21:42 [INFO] Weighted rewards: [400]
14:21:42 [INFO] Loss tensor: [1] [24447.771484375]
14:21:42 [INFO] Mean epoch policy entropy for the trajectories is: 1.7876123
14:21:45 [INFO] Training epoch 85
14:21:47 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:21:47 [INFO] Size of sampled logits tensor [400, 2, 1]
14:21:47 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:21:47 [INFO] Unweighted rewards: [400, 2]
14:21:47 [INFO] Weighted rewards: [400]
14:21:47 [INFO] Loss tensor: [1] [18465.94140625]
14:21:47 [INFO] Mean epoch policy entropy for the trajectories is: 1.7132531
14:21:50 [INFO] Training epoch 86
14:21:51 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:21:51 [INFO] Size of sampled logits tensor [400, 2, 1]
14:21:51 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:21:51 [INFO] Unweighted rewards: [400, 2]
14:21:51 [INFO] Weighted rewards: [400]
14:21:51 [INFO] Loss tensor: [1] [-20844.099609375]
14:21:51 [INFO] Mean epoch policy entropy for the trajectories is: 1.7661324
14:21:54 [INFO] Training epoch 87
14:21:56 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:21:56 [INFO] Size of sampled logits tensor [400, 2, 1]
14:21:56 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:21:56 [INFO] Unweighted rewards: [400, 2]
14:21:56 [INFO] Weighted rewards: [400]
14:21:56 [INFO] Loss tensor: [1] [-88472.90625]
14:21:56 [INFO] Mean epoch policy entropy for the trajectories is: 1.7473693
14:21:59 [INFO] Training epoch 88
14:22:00 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:22:00 [INFO] Size of sampled logits tensor [400, 2, 1]
14:22:00 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:22:00 [INFO] Unweighted rewards: [400, 2]
14:22:00 [INFO] Weighted rewards: [400]
14:22:00 [INFO] Loss tensor: [1] [-48548.94921875]
14:22:00 [INFO] Mean epoch policy entropy for the trajectories is: 1.6881813
14:22:03 [INFO] Training epoch 89
14:22:05 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:22:05 [INFO] Size of sampled logits tensor [400, 2, 1]
14:22:05 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:22:05 [INFO] Unweighted rewards: [400, 2]
14:22:05 [INFO] Weighted rewards: [400]
14:22:05 [INFO] Loss tensor: [1] [-15776.421875]
14:22:05 [INFO] Mean epoch policy entropy for the trajectories is: 1.6896611
14:22:08 [INFO] Training epoch 90
14:22:09 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:22:09 [INFO] Size of sampled logits tensor [400, 2, 1]
14:22:09 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:22:09 [INFO] Unweighted rewards: [400, 2]
14:22:09 [INFO] Weighted rewards: [400]
14:22:09 [INFO] Loss tensor: [1] [-74448.0546875]
14:22:09 [INFO] Mean epoch policy entropy for the trajectories is: 1.6962847
14:22:12 [INFO] Training epoch 91
14:22:14 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:22:14 [INFO] Size of sampled logits tensor [400, 2, 1]
14:22:14 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:22:14 [INFO] Unweighted rewards: [400, 2]
14:22:14 [INFO] Weighted rewards: [400]
14:22:14 [INFO] Loss tensor: [1] [-82576.6484375]
14:22:14 [INFO] Mean epoch policy entropy for the trajectories is: 1.8154845
14:22:17 [INFO] Training epoch 92
14:22:18 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:22:18 [INFO] Size of sampled logits tensor [400, 2, 1]
14:22:18 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:22:18 [INFO] Unweighted rewards: [400, 2]
14:22:18 [INFO] Weighted rewards: [400]
14:22:18 [INFO] Loss tensor: [1] [-40966.4609375]
14:22:18 [INFO] Mean epoch policy entropy for the trajectories is: 1.7516129
14:22:21 [INFO] Training epoch 93
14:22:23 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:22:23 [INFO] Size of sampled logits tensor [400, 2, 1]
14:22:23 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:22:23 [INFO] Unweighted rewards: [400, 2]
14:22:23 [INFO] Weighted rewards: [400]
14:22:23 [INFO] Loss tensor: [1] [-42292.4296875]
14:22:23 [INFO] Mean epoch policy entropy for the trajectories is: 1.6536027
14:22:26 [INFO] Training epoch 94
14:22:27 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:22:27 [INFO] Size of sampled logits tensor [400, 2, 1]
14:22:27 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:22:27 [INFO] Unweighted rewards: [400, 2]
14:22:27 [INFO] Weighted rewards: [400]
14:22:27 [INFO] Loss tensor: [1] [-58250.29296875]
14:22:27 [INFO] Mean epoch policy entropy for the trajectories is: 1.7154624
14:22:30 [INFO] Training epoch 95
14:22:32 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:22:32 [INFO] Size of sampled logits tensor [400, 2, 1]
14:22:32 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:22:32 [INFO] Unweighted rewards: [400, 2]
14:22:32 [INFO] Weighted rewards: [400]
14:22:32 [INFO] Loss tensor: [1] [-57108.5625]
14:22:32 [INFO] Mean epoch policy entropy for the trajectories is: 1.7417058
14:22:35 [INFO] Training epoch 96
14:22:36 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:22:36 [INFO] Size of sampled logits tensor [400, 2, 1]
14:22:36 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:22:36 [INFO] Unweighted rewards: [400, 2]
14:22:36 [INFO] Weighted rewards: [400]
14:22:36 [INFO] Loss tensor: [1] [4878.5322265625]
14:22:36 [INFO] Mean epoch policy entropy for the trajectories is: 1.632586
14:22:39 [INFO] Training epoch 97
14:22:41 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:22:41 [INFO] Size of sampled logits tensor [400, 2, 1]
14:22:41 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:22:41 [INFO] Unweighted rewards: [400, 2]
14:22:41 [INFO] Weighted rewards: [400]
14:22:41 [INFO] Loss tensor: [1] [-30574.671875]
14:22:41 [INFO] Mean epoch policy entropy for the trajectories is: 1.4732819
14:22:44 [INFO] Training epoch 98
14:22:45 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:22:45 [INFO] Size of sampled logits tensor [400, 2, 1]
14:22:45 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:22:45 [INFO] Unweighted rewards: [400, 2]
14:22:45 [INFO] Weighted rewards: [400]
14:22:45 [INFO] Loss tensor: [1] [-32143.080078125]
14:22:45 [INFO] Mean epoch policy entropy for the trajectories is: 1.4409184
14:22:48 [INFO] Training epoch 99
14:22:50 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:22:50 [INFO] Size of sampled logits tensor [400, 2, 1]
14:22:50 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:22:50 [INFO] Unweighted rewards: [400, 2]
14:22:50 [INFO] Weighted rewards: [400]
14:22:50 [INFO] Loss tensor: [1] [-100113.328125]
14:22:50 [INFO] Mean epoch policy entropy for the trajectories is: 1.4994904
14:22:53 [INFO] Training epoch 100
14:22:54 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:22:54 [INFO] Size of sampled logits tensor [400, 2, 1]
14:22:54 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:22:54 [INFO] Unweighted rewards: [400, 2]
14:22:54 [INFO] Weighted rewards: [400]
14:22:54 [INFO] Loss tensor: [1] [-41932.09375]
14:22:54 [INFO] Mean epoch policy entropy for the trajectories is: 1.4546193
14:22:57 [INFO] Training epoch 101
14:22:59 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:22:59 [INFO] Size of sampled logits tensor [400, 2, 1]
14:22:59 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:22:59 [INFO] Unweighted rewards: [400, 2]
14:22:59 [INFO] Weighted rewards: [400]
14:22:59 [INFO] Loss tensor: [1] [-50255.09765625]
14:22:59 [INFO] Mean epoch policy entropy for the trajectories is: 1.466989
14:23:02 [INFO] Training epoch 102
14:23:03 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:23:03 [INFO] Size of sampled logits tensor [400, 2, 1]
14:23:03 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:23:03 [INFO] Unweighted rewards: [400, 2]
14:23:03 [INFO] Weighted rewards: [400]
14:23:03 [INFO] Loss tensor: [1] [-197225.828125]
14:23:03 [INFO] Mean epoch policy entropy for the trajectories is: 1.4640263
14:23:06 [INFO] Training epoch 103
14:23:08 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:23:08 [INFO] Size of sampled logits tensor [400, 2, 1]
14:23:08 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:23:08 [INFO] Unweighted rewards: [400, 2]
14:23:08 [INFO] Weighted rewards: [400]
14:23:08 [INFO] Loss tensor: [1] [-82453.140625]
14:23:08 [INFO] Mean epoch policy entropy for the trajectories is: 1.4930236
14:23:11 [INFO] Training epoch 104
14:23:12 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:23:12 [INFO] Size of sampled logits tensor [400, 2, 1]
14:23:12 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:23:12 [INFO] Unweighted rewards: [400, 2]
14:23:12 [INFO] Weighted rewards: [400]
14:23:12 [INFO] Loss tensor: [1] [-58753.25]
14:23:12 [INFO] Mean epoch policy entropy for the trajectories is: 1.440999
14:23:15 [INFO] Training epoch 105
14:23:17 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:23:17 [INFO] Size of sampled logits tensor [400, 2, 1]
14:23:17 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:23:17 [INFO] Unweighted rewards: [400, 2]
14:23:17 [INFO] Weighted rewards: [400]
14:23:17 [INFO] Loss tensor: [1] [-1011.8424682617188]
14:23:17 [INFO] Mean epoch policy entropy for the trajectories is: 1.4917101
14:23:20 [INFO] Training epoch 106
14:23:21 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:23:21 [INFO] Size of sampled logits tensor [400, 2, 1]
14:23:21 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:23:21 [INFO] Unweighted rewards: [400, 2]
14:23:21 [INFO] Weighted rewards: [400]
14:23:21 [INFO] Loss tensor: [1] [-47912.98828125]
14:23:21 [INFO] Mean epoch policy entropy for the trajectories is: 1.5782464
14:23:24 [INFO] Training epoch 107
14:23:26 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:23:26 [INFO] Size of sampled logits tensor [400, 2, 1]
14:23:26 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:23:26 [INFO] Unweighted rewards: [400, 2]
14:23:26 [INFO] Weighted rewards: [400]
14:23:26 [INFO] Loss tensor: [1] [-30482.95703125]
14:23:26 [INFO] Mean epoch policy entropy for the trajectories is: 1.6078556
14:23:29 [INFO] Training epoch 108
14:23:30 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:23:30 [INFO] Size of sampled logits tensor [400, 2, 1]
14:23:30 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:23:30 [INFO] Unweighted rewards: [400, 2]
14:23:30 [INFO] Weighted rewards: [400]
14:23:30 [INFO] Loss tensor: [1] [-90582.7578125]
14:23:30 [INFO] Mean epoch policy entropy for the trajectories is: 1.5711834
14:23:33 [INFO] Training epoch 109
14:23:35 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:23:35 [INFO] Size of sampled logits tensor [400, 2, 1]
14:23:35 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:23:35 [INFO] Unweighted rewards: [400, 2]
14:23:35 [INFO] Weighted rewards: [400]
14:23:35 [INFO] Loss tensor: [1] [-113183.1171875]
14:23:35 [INFO] Mean epoch policy entropy for the trajectories is: 1.5437845
14:23:38 [INFO] Training epoch 110
14:23:39 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:23:39 [INFO] Size of sampled logits tensor [400, 2, 1]
14:23:39 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:23:39 [INFO] Unweighted rewards: [400, 2]
14:23:39 [INFO] Weighted rewards: [400]
14:23:39 [INFO] Loss tensor: [1] [12885.0986328125]
14:23:39 [INFO] Mean epoch policy entropy for the trajectories is: 1.5897956
14:23:42 [INFO] Training epoch 111
14:23:44 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:23:44 [INFO] Size of sampled logits tensor [400, 2, 1]
14:23:44 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:23:44 [INFO] Unweighted rewards: [400, 2]
14:23:44 [INFO] Weighted rewards: [400]
14:23:44 [INFO] Loss tensor: [1] [-74871.265625]
14:23:44 [INFO] Mean epoch policy entropy for the trajectories is: 1.5602942
14:23:47 [INFO] Training epoch 112
14:23:48 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:23:48 [INFO] Size of sampled logits tensor [400, 2, 1]
14:23:48 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:23:48 [INFO] Unweighted rewards: [400, 2]
14:23:48 [INFO] Weighted rewards: [400]
14:23:48 [INFO] Loss tensor: [1] [29433.259765625]
14:23:48 [INFO] Mean epoch policy entropy for the trajectories is: 1.4382924
14:23:51 [INFO] Training epoch 113
14:23:53 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:23:53 [INFO] Size of sampled logits tensor [400, 2, 1]
14:23:53 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:23:53 [INFO] Unweighted rewards: [400, 2]
14:23:53 [INFO] Weighted rewards: [400]
14:23:53 [INFO] Loss tensor: [1] [-56405.96484375]
14:23:53 [INFO] Mean epoch policy entropy for the trajectories is: 1.4649678
14:23:56 [INFO] Training epoch 114
14:23:57 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:23:57 [INFO] Size of sampled logits tensor [400, 2, 1]
14:23:57 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:23:57 [INFO] Unweighted rewards: [400, 2]
14:23:57 [INFO] Weighted rewards: [400]
14:23:57 [INFO] Loss tensor: [1] [-62515.01171875]
14:23:57 [INFO] Mean epoch policy entropy for the trajectories is: 1.5253053
14:24:00 [INFO] Training epoch 115
14:24:02 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:24:02 [INFO] Size of sampled logits tensor [400, 2, 1]
14:24:02 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:24:02 [INFO] Unweighted rewards: [400, 2]
14:24:02 [INFO] Weighted rewards: [400]
14:24:02 [INFO] Loss tensor: [1] [-9144.1533203125]
14:24:02 [INFO] Mean epoch policy entropy for the trajectories is: 1.4383497
14:24:05 [INFO] Training epoch 116
14:24:06 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:24:06 [INFO] Size of sampled logits tensor [400, 2, 1]
14:24:06 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:24:06 [INFO] Unweighted rewards: [400, 2]
14:24:06 [INFO] Weighted rewards: [400]
14:24:06 [INFO] Loss tensor: [1] [-137256.421875]
14:24:06 [INFO] Mean epoch policy entropy for the trajectories is: 1.5509536
14:24:09 [INFO] Training epoch 117
14:24:11 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:24:11 [INFO] Size of sampled logits tensor [400, 2, 1]
14:24:11 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:24:11 [INFO] Unweighted rewards: [400, 2]
14:24:11 [INFO] Weighted rewards: [400]
14:24:11 [INFO] Loss tensor: [1] [-53608.48828125]
14:24:11 [INFO] Mean epoch policy entropy for the trajectories is: 1.4752713
14:24:14 [INFO] Training epoch 118
14:24:15 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:24:15 [INFO] Size of sampled logits tensor [400, 2, 1]
14:24:15 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:24:15 [INFO] Unweighted rewards: [400, 2]
14:24:15 [INFO] Weighted rewards: [400]
14:24:15 [INFO] Loss tensor: [1] [-74364.6796875]
14:24:15 [INFO] Mean epoch policy entropy for the trajectories is: 1.4299667
14:24:18 [INFO] Training epoch 119
14:24:20 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:24:20 [INFO] Size of sampled logits tensor [400, 2, 1]
14:24:20 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:24:20 [INFO] Unweighted rewards: [400, 2]
14:24:20 [INFO] Weighted rewards: [400]
14:24:20 [INFO] Loss tensor: [1] [7323.87353515625]
14:24:20 [INFO] Mean epoch policy entropy for the trajectories is: 1.4486908
14:24:23 [INFO] Training epoch 120
14:24:24 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:24:24 [INFO] Size of sampled logits tensor [400, 2, 1]
14:24:24 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:24:24 [INFO] Unweighted rewards: [400, 2]
14:24:24 [INFO] Weighted rewards: [400]
14:24:24 [INFO] Loss tensor: [1] [-96400.0546875]
14:24:24 [INFO] Mean epoch policy entropy for the trajectories is: 1.4413227
14:24:27 [INFO] Training epoch 121
14:24:29 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:24:29 [INFO] Size of sampled logits tensor [400, 2, 1]
14:24:29 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:24:29 [INFO] Unweighted rewards: [400, 2]
14:24:29 [INFO] Weighted rewards: [400]
14:24:29 [INFO] Loss tensor: [1] [-16228.455078125]
14:24:29 [INFO] Mean epoch policy entropy for the trajectories is: 1.4996051
14:24:32 [INFO] Training epoch 122
14:24:33 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:24:33 [INFO] Size of sampled logits tensor [400, 2, 1]
14:24:33 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:24:33 [INFO] Unweighted rewards: [400, 2]
14:24:33 [INFO] Weighted rewards: [400]
14:24:33 [INFO] Loss tensor: [1] [-69543.3046875]
14:24:33 [INFO] Mean epoch policy entropy for the trajectories is: 1.497287
14:24:36 [INFO] Training epoch 123
14:24:38 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:24:38 [INFO] Size of sampled logits tensor [400, 2, 1]
14:24:38 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:24:38 [INFO] Unweighted rewards: [400, 2]
14:24:38 [INFO] Weighted rewards: [400]
14:24:38 [INFO] Loss tensor: [1] [-48852.140625]
14:24:38 [INFO] Mean epoch policy entropy for the trajectories is: 1.3844848
14:24:41 [INFO] Training epoch 124
14:24:42 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:24:42 [INFO] Size of sampled logits tensor [400, 2, 1]
14:24:42 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:24:42 [INFO] Unweighted rewards: [400, 2]
14:24:42 [INFO] Weighted rewards: [400]
14:24:42 [INFO] Loss tensor: [1] [-42977.98046875]
14:24:42 [INFO] Mean epoch policy entropy for the trajectories is: 1.4761488
14:24:45 [INFO] Training epoch 125
14:24:47 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:24:47 [INFO] Size of sampled logits tensor [400, 2, 1]
14:24:47 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:24:47 [INFO] Unweighted rewards: [400, 2]
14:24:47 [INFO] Weighted rewards: [400]
14:24:47 [INFO] Loss tensor: [1] [-89197.21875]
14:24:47 [INFO] Mean epoch policy entropy for the trajectories is: 1.5771992
14:24:50 [INFO] Training epoch 126
14:24:51 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:24:51 [INFO] Size of sampled logits tensor [400, 2, 1]
14:24:51 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:24:51 [INFO] Unweighted rewards: [400, 2]
14:24:51 [INFO] Weighted rewards: [400]
14:24:51 [INFO] Loss tensor: [1] [-109208.65625]
14:24:51 [INFO] Mean epoch policy entropy for the trajectories is: 1.5189624
14:24:54 [INFO] Training epoch 127
14:24:56 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:24:56 [INFO] Size of sampled logits tensor [400, 2, 1]
14:24:56 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:24:56 [INFO] Unweighted rewards: [400, 2]
14:24:56 [INFO] Weighted rewards: [400]
14:24:56 [INFO] Loss tensor: [1] [-109642.3203125]
14:24:56 [INFO] Mean epoch policy entropy for the trajectories is: 1.5764184
14:24:59 [INFO] Training epoch 128
14:25:00 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:25:00 [INFO] Size of sampled logits tensor [400, 2, 1]
14:25:00 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:25:00 [INFO] Unweighted rewards: [400, 2]
14:25:00 [INFO] Weighted rewards: [400]
14:25:00 [INFO] Loss tensor: [1] [-8382.9365234375]
14:25:00 [INFO] Mean epoch policy entropy for the trajectories is: 1.5138131
14:25:03 [INFO] Training epoch 129
14:25:05 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:25:05 [INFO] Size of sampled logits tensor [400, 2, 1]
14:25:05 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:25:05 [INFO] Unweighted rewards: [400, 2]
14:25:05 [INFO] Weighted rewards: [400]
14:25:05 [INFO] Loss tensor: [1] [22104.724609375]
14:25:05 [INFO] Mean epoch policy entropy for the trajectories is: 1.4657502
14:25:08 [INFO] Training epoch 130
14:25:09 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:25:09 [INFO] Size of sampled logits tensor [400, 2, 1]
14:25:09 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:25:09 [INFO] Unweighted rewards: [400, 2]
14:25:09 [INFO] Weighted rewards: [400]
14:25:09 [INFO] Loss tensor: [1] [-96430.0]
14:25:09 [INFO] Mean epoch policy entropy for the trajectories is: 1.4591727
14:25:12 [INFO] Training epoch 131
14:25:14 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:25:14 [INFO] Size of sampled logits tensor [400, 2, 1]
14:25:14 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:25:14 [INFO] Unweighted rewards: [400, 2]
14:25:14 [INFO] Weighted rewards: [400]
14:25:14 [INFO] Loss tensor: [1] [-35814.75390625]
14:25:14 [INFO] Mean epoch policy entropy for the trajectories is: 1.5730215
14:25:17 [INFO] Training epoch 132
14:25:18 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:25:18 [INFO] Size of sampled logits tensor [400, 2, 1]
14:25:18 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:25:18 [INFO] Unweighted rewards: [400, 2]
14:25:18 [INFO] Weighted rewards: [400]
14:25:18 [INFO] Loss tensor: [1] [-42367.94921875]
14:25:18 [INFO] Mean epoch policy entropy for the trajectories is: 1.4997057
14:25:21 [INFO] Training epoch 133
14:25:23 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:25:23 [INFO] Size of sampled logits tensor [400, 2, 1]
14:25:23 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:25:23 [INFO] Unweighted rewards: [400, 2]
14:25:23 [INFO] Weighted rewards: [400]
14:25:23 [INFO] Loss tensor: [1] [-104801.75]
14:25:23 [INFO] Mean epoch policy entropy for the trajectories is: 1.557427
14:25:26 [INFO] Training epoch 134
14:25:27 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:25:27 [INFO] Size of sampled logits tensor [400, 2, 1]
14:25:27 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:25:27 [INFO] Unweighted rewards: [400, 2]
14:25:27 [INFO] Weighted rewards: [400]
14:25:27 [INFO] Loss tensor: [1] [-21810.07421875]
14:25:27 [INFO] Mean epoch policy entropy for the trajectories is: 1.596278
14:25:30 [INFO] Training epoch 135
14:25:32 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:25:32 [INFO] Size of sampled logits tensor [400, 2, 1]
14:25:32 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:25:32 [INFO] Unweighted rewards: [400, 2]
14:25:32 [INFO] Weighted rewards: [400]
14:25:32 [INFO] Loss tensor: [1] [-62168.76953125]
14:25:32 [INFO] Mean epoch policy entropy for the trajectories is: 1.5527416
14:25:35 [INFO] Training epoch 136
14:25:36 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:25:36 [INFO] Size of sampled logits tensor [400, 2, 1]
14:25:36 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:25:36 [INFO] Unweighted rewards: [400, 2]
14:25:36 [INFO] Weighted rewards: [400]
14:25:36 [INFO] Loss tensor: [1] [85558.3984375]
14:25:37 [INFO] Mean epoch policy entropy for the trajectories is: 1.5463628
14:25:40 [INFO] Training epoch 137
14:25:41 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:25:41 [INFO] Size of sampled logits tensor [400, 2, 1]
14:25:41 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:25:41 [INFO] Unweighted rewards: [400, 2]
14:25:41 [INFO] Weighted rewards: [400]
14:25:41 [INFO] Loss tensor: [1] [-67514.296875]
14:25:41 [INFO] Mean epoch policy entropy for the trajectories is: 1.5408534
14:25:44 [INFO] Training epoch 138
14:25:46 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:25:46 [INFO] Size of sampled logits tensor [400, 2, 1]
14:25:46 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:25:46 [INFO] Unweighted rewards: [400, 2]
14:25:46 [INFO] Weighted rewards: [400]
14:25:46 [INFO] Loss tensor: [1] [-28947.041015625]
14:25:46 [INFO] Mean epoch policy entropy for the trajectories is: 1.6355941
14:25:49 [INFO] Training epoch 139
14:25:50 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:25:50 [INFO] Size of sampled logits tensor [400, 2, 1]
14:25:50 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:25:50 [INFO] Unweighted rewards: [400, 2]
14:25:50 [INFO] Weighted rewards: [400]
14:25:50 [INFO] Loss tensor: [1] [5783.43505859375]
14:25:50 [INFO] Mean epoch policy entropy for the trajectories is: 1.5256497
14:25:53 [INFO] Training epoch 140
14:25:55 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:25:55 [INFO] Size of sampled logits tensor [400, 2, 1]
14:25:55 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:25:55 [INFO] Unweighted rewards: [400, 2]
14:25:55 [INFO] Weighted rewards: [400]
14:25:55 [INFO] Loss tensor: [1] [-25788.298828125]
14:25:55 [INFO] Mean epoch policy entropy for the trajectories is: 1.584325
14:25:58 [INFO] Training epoch 141
14:25:59 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:25:59 [INFO] Size of sampled logits tensor [400, 2, 1]
14:25:59 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:25:59 [INFO] Unweighted rewards: [400, 2]
14:25:59 [INFO] Weighted rewards: [400]
14:25:59 [INFO] Loss tensor: [1] [-109362.3984375]
14:25:59 [INFO] Mean epoch policy entropy for the trajectories is: 1.5315049
14:26:02 [INFO] Training epoch 142
14:26:04 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:26:04 [INFO] Size of sampled logits tensor [400, 2, 1]
14:26:04 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:26:04 [INFO] Unweighted rewards: [400, 2]
14:26:04 [INFO] Weighted rewards: [400]
14:26:04 [INFO] Loss tensor: [1] [-53673.79296875]
14:26:04 [INFO] Mean epoch policy entropy for the trajectories is: 1.4984217
14:26:07 [INFO] Training epoch 143
14:26:08 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:26:08 [INFO] Size of sampled logits tensor [400, 2, 1]
14:26:08 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:26:08 [INFO] Unweighted rewards: [400, 2]
14:26:08 [INFO] Weighted rewards: [400]
14:26:08 [INFO] Loss tensor: [1] [-15736.6025390625]
14:26:09 [INFO] Mean epoch policy entropy for the trajectories is: 1.5278168
14:26:12 [INFO] Training epoch 144
14:26:13 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:26:13 [INFO] Size of sampled logits tensor [400, 2, 1]
14:26:13 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:26:13 [INFO] Unweighted rewards: [400, 2]
14:26:13 [INFO] Weighted rewards: [400]
14:26:13 [INFO] Loss tensor: [1] [-82807.0]
14:26:13 [INFO] Mean epoch policy entropy for the trajectories is: 1.5113666
14:26:16 [INFO] Training epoch 145
14:26:18 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:26:18 [INFO] Size of sampled logits tensor [400, 2, 1]
14:26:18 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:26:18 [INFO] Unweighted rewards: [400, 2]
14:26:18 [INFO] Weighted rewards: [400]
14:26:18 [INFO] Loss tensor: [1] [17406.1640625]
14:26:18 [INFO] Mean epoch policy entropy for the trajectories is: 1.4729501
14:26:21 [INFO] Training epoch 146
14:26:22 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:26:22 [INFO] Size of sampled logits tensor [400, 2, 1]
14:26:22 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:26:22 [INFO] Unweighted rewards: [400, 2]
14:26:22 [INFO] Weighted rewards: [400]
14:26:22 [INFO] Loss tensor: [1] [38156.28125]
14:26:22 [INFO] Mean epoch policy entropy for the trajectories is: 1.5323944
14:26:25 [INFO] Training epoch 147
14:26:27 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:26:27 [INFO] Size of sampled logits tensor [400, 2, 1]
14:26:27 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:26:27 [INFO] Unweighted rewards: [400, 2]
14:26:27 [INFO] Weighted rewards: [400]
14:26:27 [INFO] Loss tensor: [1] [-49225.92578125]
14:26:27 [INFO] Mean epoch policy entropy for the trajectories is: 1.4797024
14:26:30 [INFO] Training epoch 148
14:26:31 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:26:31 [INFO] Size of sampled logits tensor [400, 2, 1]
14:26:31 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:26:31 [INFO] Unweighted rewards: [400, 2]
14:26:31 [INFO] Weighted rewards: [400]
14:26:31 [INFO] Loss tensor: [1] [-56608.8125]
14:26:31 [INFO] Mean epoch policy entropy for the trajectories is: 1.5810453
14:26:34 [INFO] Training epoch 149
14:26:36 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:26:36 [INFO] Size of sampled logits tensor [400, 2, 1]
14:26:36 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:26:36 [INFO] Unweighted rewards: [400, 2]
14:26:36 [INFO] Weighted rewards: [400]
14:26:36 [INFO] Loss tensor: [1] [-8777.689453125]
14:26:36 [INFO] Mean epoch policy entropy for the trajectories is: 1.5030191
14:26:39 [INFO] The random scrambling applied these moves: [[LMinus, UPlus], [BPlus, BPlus], [BPlus, DPlus], [DMinus, DPlus], [RPlus, DPlus], [FPlus, BPlus], [LPlus, FMinus], [BMinus, DPlus], [LPlus, LPlus], [FMinus, DPlus]] for testing
14:26:39 [INFO] Running the test loop now: 
14:26:39 [INFO] Number of tests: 10
14:26:39 [INFO] Scrambled cube used for test 0 used this moves: [LMinus, UPlus]
14:26:39 [INFO] Got the cubemove from policy: RPlus for turn 0
14:26:39 [INFO] The above is complementary or not: false
14:26:39 [INFO] The cross entropy for turn 0 2.5646887
14:26:39 [INFO] Got the cubemove from policy: BPlus for turn 1
14:26:39 [INFO] The above is complementary or not: false
14:26:39 [INFO] The cross entropy for turn 1 2.4724076
14:26:39 [INFO] Scrambled cube used for test 1 used this moves: [BPlus, BPlus]
14:26:39 [INFO] Got the cubemove from policy: BPlus for turn 0
14:26:39 [INFO] The above is complementary or not: false
14:26:39 [INFO] The cross entropy for turn 0 0.0003388361
14:26:39 [INFO] Got the cubemove from policy: BPlus for turn 1
14:26:39 [INFO] The above is complementary or not: false
14:26:39 [INFO] The cross entropy for turn 1 0.00012028024
14:26:39 [INFO] Scrambled cube used for test 2 used this moves: [BPlus, DPlus]
14:26:39 [INFO] Got the cubemove from policy: LMinus for turn 0
14:26:39 [INFO] The above is complementary or not: false
14:26:39 [INFO] The cross entropy for turn 0 2.5649495
14:26:39 [INFO] Got the cubemove from policy: LPlus for turn 1
14:26:39 [INFO] The above is complementary or not: false
14:26:39 [INFO] The cross entropy for turn 1 2.4447703
14:26:39 [INFO] Scrambled cube used for test 3 used this moves: [DMinus, DPlus]
14:26:39 [INFO] Got the cubemove from policy: LMinus for turn 0
14:26:39 [INFO] The above is complementary or not: false
14:26:39 [INFO] The cross entropy for turn 0 2.5649495
14:26:39 [INFO] Got the cubemove from policy: LPlus for turn 1
14:26:39 [INFO] The above is complementary or not: false
14:26:39 [INFO] The cross entropy for turn 1 0.00014074695
14:26:39 [INFO] Scrambled cube used for test 4 used this moves: [RPlus, DPlus]
14:26:39 [INFO] Got the cubemove from policy: BMinus for turn 0
14:26:39 [INFO] The above is complementary or not: false
14:26:39 [INFO] The cross entropy for turn 0 2.5649495
14:26:39 [INFO] Got the cubemove from policy: FPlus for turn 1
14:26:39 [INFO] The above is complementary or not: false
14:26:39 [INFO] The cross entropy for turn 1 2.5569386
14:26:39 [INFO] Scrambled cube used for test 5 used this moves: [FPlus, BPlus]
14:26:39 [INFO] Got the cubemove from policy: RPlus for turn 0
14:26:39 [INFO] The above is complementary or not: false
14:26:39 [INFO] The cross entropy for turn 0 2.5649495
14:26:39 [INFO] Got the cubemove from policy: RPlus for turn 1
14:26:39 [INFO] The above is complementary or not: false
14:26:39 [INFO] The cross entropy for turn 1 2.5649495
14:26:39 [INFO] Scrambled cube used for test 6 used this moves: [LPlus, FMinus]
14:26:39 [INFO] Got the cubemove from policy: FPlus for turn 0
14:26:39 [INFO] The above is complementary or not: true
14:26:39 [INFO] The cross entropy for turn 0 0.034501694
14:26:39 [INFO] Got the cubemove from policy: LPlus for turn 1
14:26:39 [INFO] The above is complementary or not: false
14:26:39 [INFO] The cross entropy for turn 1 2.5649495
14:26:39 [INFO] Scrambled cube used for test 7 used this moves: [BMinus, DPlus]
14:26:39 [INFO] Got the cubemove from policy: BPlus for turn 0
14:26:39 [INFO] The above is complementary or not: false
14:26:39 [INFO] The cross entropy for turn 0 2.5649495
14:26:39 [INFO] Got the cubemove from policy: DMinus for turn 1
14:26:39 [INFO] The above is complementary or not: false
14:26:39 [INFO] The cross entropy for turn 1 2.5649495
14:26:39 [INFO] Scrambled cube used for test 8 used this moves: [LPlus, LPlus]
14:26:39 [INFO] Got the cubemove from policy: LPlus for turn 0
14:26:39 [INFO] The above is complementary or not: false
14:26:39 [INFO] The cross entropy for turn 0 0.0008877062
14:26:39 [INFO] Got the cubemove from policy: LPlus for turn 1
14:26:39 [INFO] The above is complementary or not: false
14:26:39 [INFO] The cross entropy for turn 1 0.00014074695
14:26:39 [INFO] Scrambled cube used for test 9 used this moves: [FMinus, DPlus]
14:26:39 [INFO] Got the cubemove from policy: RPlus for turn 0
14:26:39 [INFO] The above is complementary or not: false
14:26:39 [INFO] The cross entropy for turn 0 2.5649495
14:26:39 [INFO] Got the cubemove from policy: FMinus for turn 1
14:26:39 [INFO] The above is complementary or not: false
14:26:39 [INFO] The cross entropy for turn 1 2.5649495
14:26:39 [INFO] Mean loss for tests 0.0
14:26:39 [INFO] Mean entropy for tests 1.7862213
14:26:39 [INFO] Mean correct moves for 0.1
