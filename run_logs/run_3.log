14:05:51 [INFO] Is MPS available: true
14:05:51 [INFO] Using the following hyperparameters:
14:05:51 [INFO] Hidden layer dimensions: 2500
14:05:51 [INFO] Number of layers: 4
14:05:51 [INFO] Number of trajectories: 400
14:05:51 [INFO] Trajectory depth: 2
14:05:51 [INFO] Number of epochs: 100
14:05:51 [INFO] Learning rate: 0.0001
14:05:51 [INFO] Starting policy training...
14:05:51 [INFO] Training epoch 0
14:05:53 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:05:53 [INFO] Size of sampled logits tensor [400, 2, 1]
14:05:53 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:05:53 [INFO] Unweighted rewards: [400, 2]
14:05:53 [INFO] Weighted rewards: [400]
14:05:53 [INFO] Loss tensor: [1] [-50400.71875]
14:05:53 [INFO] Mean epoch policy entropy for the trajectories is: 2.5572865
14:05:56 [INFO] Training epoch 1
14:05:57 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:05:57 [INFO] Size of sampled logits tensor [400, 2, 1]
14:05:57 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:05:57 [INFO] Unweighted rewards: [400, 2]
14:05:57 [INFO] Weighted rewards: [400]
14:05:57 [INFO] Loss tensor: [1] [-150083.609375]
14:05:57 [INFO] Mean epoch policy entropy for the trajectories is: 2.5469964
14:06:00 [INFO] Training epoch 2
14:06:02 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:06:02 [INFO] Size of sampled logits tensor [400, 2, 1]
14:06:02 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:06:02 [INFO] Unweighted rewards: [400, 2]
14:06:02 [INFO] Weighted rewards: [400]
14:06:02 [INFO] Loss tensor: [1] [-252045.234375]
14:06:02 [INFO] Mean epoch policy entropy for the trajectories is: 2.500057
14:06:05 [INFO] Training epoch 3
14:06:06 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:06:06 [INFO] Size of sampled logits tensor [400, 2, 1]
14:06:06 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:06:06 [INFO] Unweighted rewards: [400, 2]
14:06:06 [INFO] Weighted rewards: [400]
14:06:06 [INFO] Loss tensor: [1] [-89073.78125]
14:06:06 [INFO] Mean epoch policy entropy for the trajectories is: 2.3801324
14:06:09 [INFO] Training epoch 4
14:06:11 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:06:11 [INFO] Size of sampled logits tensor [400, 2, 1]
14:06:11 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:06:11 [INFO] Unweighted rewards: [400, 2]
14:06:11 [INFO] Weighted rewards: [400]
14:06:11 [INFO] Loss tensor: [1] [-57311.265625]
14:06:11 [INFO] Mean epoch policy entropy for the trajectories is: 2.2516258
14:06:14 [INFO] Training epoch 5
14:06:15 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:06:15 [INFO] Size of sampled logits tensor [400, 2, 1]
14:06:15 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:06:15 [INFO] Unweighted rewards: [400, 2]
14:06:15 [INFO] Weighted rewards: [400]
14:06:15 [INFO] Loss tensor: [1] [-71038.859375]
14:06:15 [INFO] Mean epoch policy entropy for the trajectories is: 2.2097473
14:06:18 [INFO] Training epoch 6
14:06:20 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:06:20 [INFO] Size of sampled logits tensor [400, 2, 1]
14:06:20 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:06:20 [INFO] Unweighted rewards: [400, 2]
14:06:20 [INFO] Weighted rewards: [400]
14:06:20 [INFO] Loss tensor: [1] [-29711.4921875]
14:06:20 [INFO] Mean epoch policy entropy for the trajectories is: 2.1705537
14:06:23 [INFO] Training epoch 7
14:06:24 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:06:24 [INFO] Size of sampled logits tensor [400, 2, 1]
14:06:24 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:06:24 [INFO] Unweighted rewards: [400, 2]
14:06:24 [INFO] Weighted rewards: [400]
14:06:24 [INFO] Loss tensor: [1] [2206.50244140625]
14:06:24 [INFO] Mean epoch policy entropy for the trajectories is: 2.1574497
14:06:27 [INFO] Training epoch 8
14:06:29 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:06:29 [INFO] Size of sampled logits tensor [400, 2, 1]
14:06:29 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:06:29 [INFO] Unweighted rewards: [400, 2]
14:06:29 [INFO] Weighted rewards: [400]
14:06:29 [INFO] Loss tensor: [1] [-14772.41796875]
14:06:29 [INFO] Mean epoch policy entropy for the trajectories is: 2.120858
14:06:32 [INFO] Training epoch 9
14:06:33 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:06:33 [INFO] Size of sampled logits tensor [400, 2, 1]
14:06:33 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:06:33 [INFO] Unweighted rewards: [400, 2]
14:06:33 [INFO] Weighted rewards: [400]
14:06:33 [INFO] Loss tensor: [1] [-16466.888671875]
14:06:33 [INFO] Mean epoch policy entropy for the trajectories is: 2.125186
14:06:36 [INFO] Training epoch 10
14:06:38 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:06:38 [INFO] Size of sampled logits tensor [400, 2, 1]
14:06:38 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:06:38 [INFO] Unweighted rewards: [400, 2]
14:06:38 [INFO] Weighted rewards: [400]
14:06:38 [INFO] Loss tensor: [1] [31038.19921875]
14:06:38 [INFO] Mean epoch policy entropy for the trajectories is: 2.097743
14:06:41 [INFO] Training epoch 11
14:06:42 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:06:42 [INFO] Size of sampled logits tensor [400, 2, 1]
14:06:42 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:06:42 [INFO] Unweighted rewards: [400, 2]
14:06:42 [INFO] Weighted rewards: [400]
14:06:42 [INFO] Loss tensor: [1] [17350.767578125]
14:06:42 [INFO] Mean epoch policy entropy for the trajectories is: 2.083916
14:06:45 [INFO] Training epoch 12
14:06:47 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:06:47 [INFO] Size of sampled logits tensor [400, 2, 1]
14:06:47 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:06:47 [INFO] Unweighted rewards: [400, 2]
14:06:47 [INFO] Weighted rewards: [400]
14:06:47 [INFO] Loss tensor: [1] [53667.19921875]
14:06:47 [INFO] Mean epoch policy entropy for the trajectories is: 1.9948845
14:06:50 [INFO] Training epoch 13
14:06:51 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:06:51 [INFO] Size of sampled logits tensor [400, 2, 1]
14:06:51 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:06:51 [INFO] Unweighted rewards: [400, 2]
14:06:51 [INFO] Weighted rewards: [400]
14:06:51 [INFO] Loss tensor: [1] [87106.15625]
14:06:51 [INFO] Mean epoch policy entropy for the trajectories is: 1.9421195
14:06:54 [INFO] Training epoch 14
14:06:56 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:06:56 [INFO] Size of sampled logits tensor [400, 2, 1]
14:06:56 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:06:56 [INFO] Unweighted rewards: [400, 2]
14:06:56 [INFO] Weighted rewards: [400]
14:06:56 [INFO] Loss tensor: [1] [24997.38671875]
14:06:56 [INFO] Mean epoch policy entropy for the trajectories is: 1.9052823
14:06:59 [INFO] Training epoch 15
14:07:00 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:07:00 [INFO] Size of sampled logits tensor [400, 2, 1]
14:07:00 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:07:00 [INFO] Unweighted rewards: [400, 2]
14:07:00 [INFO] Weighted rewards: [400]
14:07:00 [INFO] Loss tensor: [1] [-24802.615234375]
14:07:00 [INFO] Mean epoch policy entropy for the trajectories is: 1.7602109
14:07:03 [INFO] Training epoch 16
14:07:05 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:07:05 [INFO] Size of sampled logits tensor [400, 2, 1]
14:07:05 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:07:05 [INFO] Unweighted rewards: [400, 2]
14:07:05 [INFO] Weighted rewards: [400]
14:07:05 [INFO] Loss tensor: [1] [54837.16796875]
14:07:05 [INFO] Mean epoch policy entropy for the trajectories is: 1.6161445
14:07:08 [INFO] Training epoch 17
14:07:09 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:07:09 [INFO] Size of sampled logits tensor [400, 2, 1]
14:07:09 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:07:09 [INFO] Unweighted rewards: [400, 2]
14:07:09 [INFO] Weighted rewards: [400]
14:07:09 [INFO] Loss tensor: [1] [102000.9296875]
14:07:09 [INFO] Mean epoch policy entropy for the trajectories is: 1.4943296
14:07:12 [INFO] Training epoch 18
14:07:14 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:07:14 [INFO] Size of sampled logits tensor [400, 2, 1]
14:07:14 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:07:14 [INFO] Unweighted rewards: [400, 2]
14:07:14 [INFO] Weighted rewards: [400]
14:07:14 [INFO] Loss tensor: [1] [123153.578125]
14:07:14 [INFO] Mean epoch policy entropy for the trajectories is: 1.3854653
14:07:17 [INFO] Training epoch 19
14:07:18 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:07:18 [INFO] Size of sampled logits tensor [400, 2, 1]
14:07:18 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:07:18 [INFO] Unweighted rewards: [400, 2]
14:07:18 [INFO] Weighted rewards: [400]
14:07:18 [INFO] Loss tensor: [1] [51632.54296875]
14:07:18 [INFO] Mean epoch policy entropy for the trajectories is: 1.3224573
14:07:21 [INFO] Training epoch 20
14:07:23 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:07:23 [INFO] Size of sampled logits tensor [400, 2, 1]
14:07:23 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:07:23 [INFO] Unweighted rewards: [400, 2]
14:07:23 [INFO] Weighted rewards: [400]
14:07:23 [INFO] Loss tensor: [1] [92659.234375]
14:07:23 [INFO] Mean epoch policy entropy for the trajectories is: 1.2699995
14:07:26 [INFO] Training epoch 21
14:07:27 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:07:27 [INFO] Size of sampled logits tensor [400, 2, 1]
14:07:27 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:07:27 [INFO] Unweighted rewards: [400, 2]
14:07:27 [INFO] Weighted rewards: [400]
14:07:27 [INFO] Loss tensor: [1] [604.07763671875]
14:07:27 [INFO] Mean epoch policy entropy for the trajectories is: 1.17622
14:07:30 [INFO] Training epoch 22
14:07:32 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:07:32 [INFO] Size of sampled logits tensor [400, 2, 1]
14:07:32 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:07:32 [INFO] Unweighted rewards: [400, 2]
14:07:32 [INFO] Weighted rewards: [400]
14:07:32 [INFO] Loss tensor: [1] [37389.98828125]
14:07:32 [INFO] Mean epoch policy entropy for the trajectories is: 1.1505687
14:07:35 [INFO] Training epoch 23
14:07:36 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:07:36 [INFO] Size of sampled logits tensor [400, 2, 1]
14:07:36 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:07:36 [INFO] Unweighted rewards: [400, 2]
14:07:36 [INFO] Weighted rewards: [400]
14:07:36 [INFO] Loss tensor: [1] [40560.3984375]
14:07:36 [INFO] Mean epoch policy entropy for the trajectories is: 1.0481114
14:07:39 [INFO] Training epoch 24
14:07:41 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:07:41 [INFO] Size of sampled logits tensor [400, 2, 1]
14:07:41 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:07:41 [INFO] Unweighted rewards: [400, 2]
14:07:41 [INFO] Weighted rewards: [400]
14:07:41 [INFO] Loss tensor: [1] [59004.328125]
14:07:41 [INFO] Mean epoch policy entropy for the trajectories is: 0.9401157
14:07:44 [INFO] Training epoch 25
14:07:45 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:07:45 [INFO] Size of sampled logits tensor [400, 2, 1]
14:07:45 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:07:45 [INFO] Unweighted rewards: [400, 2]
14:07:45 [INFO] Weighted rewards: [400]
14:07:45 [INFO] Loss tensor: [1] [60951.88671875]
14:07:45 [INFO] Mean epoch policy entropy for the trajectories is: 0.90997803
14:07:48 [INFO] Training epoch 26
14:07:50 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:07:50 [INFO] Size of sampled logits tensor [400, 2, 1]
14:07:50 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:07:50 [INFO] Unweighted rewards: [400, 2]
14:07:50 [INFO] Weighted rewards: [400]
14:07:50 [INFO] Loss tensor: [1] [82411.765625]
14:07:50 [INFO] Mean epoch policy entropy for the trajectories is: 0.8075433
14:07:53 [INFO] Training epoch 27
14:07:54 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:07:54 [INFO] Size of sampled logits tensor [400, 2, 1]
14:07:54 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:07:54 [INFO] Unweighted rewards: [400, 2]
14:07:54 [INFO] Weighted rewards: [400]
14:07:54 [INFO] Loss tensor: [1] [65822.609375]
14:07:54 [INFO] Mean epoch policy entropy for the trajectories is: 0.76439536
14:07:57 [INFO] Training epoch 28
14:07:59 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:07:59 [INFO] Size of sampled logits tensor [400, 2, 1]
14:07:59 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:07:59 [INFO] Unweighted rewards: [400, 2]
14:07:59 [INFO] Weighted rewards: [400]
14:07:59 [INFO] Loss tensor: [1] [12470.517578125]
14:07:59 [INFO] Mean epoch policy entropy for the trajectories is: 0.69984317
14:08:02 [INFO] Training epoch 29
14:08:03 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:08:03 [INFO] Size of sampled logits tensor [400, 2, 1]
14:08:03 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:08:03 [INFO] Unweighted rewards: [400, 2]
14:08:03 [INFO] Weighted rewards: [400]
14:08:03 [INFO] Loss tensor: [1] [23568.109375]
14:08:03 [INFO] Mean epoch policy entropy for the trajectories is: 0.69716746
14:08:06 [INFO] Training epoch 30
14:08:08 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:08:08 [INFO] Size of sampled logits tensor [400, 2, 1]
14:08:08 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:08:08 [INFO] Unweighted rewards: [400, 2]
14:08:08 [INFO] Weighted rewards: [400]
14:08:08 [INFO] Loss tensor: [1] [29612.955078125]
14:08:08 [INFO] Mean epoch policy entropy for the trajectories is: 0.63180876
14:08:11 [INFO] Training epoch 31
14:08:12 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:08:12 [INFO] Size of sampled logits tensor [400, 2, 1]
14:08:12 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:08:12 [INFO] Unweighted rewards: [400, 2]
14:08:12 [INFO] Weighted rewards: [400]
14:08:12 [INFO] Loss tensor: [1] [25753.375]
14:08:13 [INFO] Mean epoch policy entropy for the trajectories is: 0.50762725
14:08:16 [INFO] Training epoch 32
14:08:17 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:08:17 [INFO] Size of sampled logits tensor [400, 2, 1]
14:08:17 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:08:17 [INFO] Unweighted rewards: [400, 2]
14:08:17 [INFO] Weighted rewards: [400]
14:08:17 [INFO] Loss tensor: [1] [43995.203125]
14:08:17 [INFO] Mean epoch policy entropy for the trajectories is: 0.52771354
14:08:20 [INFO] Training epoch 33
14:08:21 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:08:21 [INFO] Size of sampled logits tensor [400, 2, 1]
14:08:21 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:08:21 [INFO] Unweighted rewards: [400, 2]
14:08:21 [INFO] Weighted rewards: [400]
14:08:21 [INFO] Loss tensor: [1] [36135.8359375]
14:08:22 [INFO] Mean epoch policy entropy for the trajectories is: 0.5399427
14:08:25 [INFO] Training epoch 34
14:08:26 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:08:26 [INFO] Size of sampled logits tensor [400, 2, 1]
14:08:26 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:08:26 [INFO] Unweighted rewards: [400, 2]
14:08:26 [INFO] Weighted rewards: [400]
14:08:26 [INFO] Loss tensor: [1] [19847.33984375]
14:08:26 [INFO] Mean epoch policy entropy for the trajectories is: 0.4985421
14:08:29 [INFO] Training epoch 35
14:08:30 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:08:30 [INFO] Size of sampled logits tensor [400, 2, 1]
14:08:30 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:08:30 [INFO] Unweighted rewards: [400, 2]
14:08:30 [INFO] Weighted rewards: [400]
14:08:30 [INFO] Loss tensor: [1] [4270.85595703125]
14:08:31 [INFO] Mean epoch policy entropy for the trajectories is: 0.5449423
14:08:34 [INFO] Training epoch 36
14:08:35 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:08:35 [INFO] Size of sampled logits tensor [400, 2, 1]
14:08:35 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:08:35 [INFO] Unweighted rewards: [400, 2]
14:08:35 [INFO] Weighted rewards: [400]
14:08:35 [INFO] Loss tensor: [1] [44887.58984375]
14:08:35 [INFO] Mean epoch policy entropy for the trajectories is: 0.5785933
14:08:38 [INFO] Training epoch 37
14:08:39 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:08:39 [INFO] Size of sampled logits tensor [400, 2, 1]
14:08:39 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:08:39 [INFO] Unweighted rewards: [400, 2]
14:08:39 [INFO] Weighted rewards: [400]
14:08:39 [INFO] Loss tensor: [1] [11698.2822265625]
14:08:40 [INFO] Mean epoch policy entropy for the trajectories is: 0.63170564
14:08:43 [INFO] Training epoch 38
14:08:44 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:08:44 [INFO] Size of sampled logits tensor [400, 2, 1]
14:08:44 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:08:44 [INFO] Unweighted rewards: [400, 2]
14:08:44 [INFO] Weighted rewards: [400]
14:08:44 [INFO] Loss tensor: [1] [16225.5634765625]
14:08:44 [INFO] Mean epoch policy entropy for the trajectories is: 0.65979123
14:08:47 [INFO] Training epoch 39
14:08:48 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:08:48 [INFO] Size of sampled logits tensor [400, 2, 1]
14:08:48 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:08:48 [INFO] Unweighted rewards: [400, 2]
14:08:48 [INFO] Weighted rewards: [400]
14:08:48 [INFO] Loss tensor: [1] [45982.09765625]
14:08:49 [INFO] Mean epoch policy entropy for the trajectories is: 0.69826996
14:08:52 [INFO] Training epoch 40
14:08:53 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:08:53 [INFO] Size of sampled logits tensor [400, 2, 1]
14:08:53 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:08:53 [INFO] Unweighted rewards: [400, 2]
14:08:53 [INFO] Weighted rewards: [400]
14:08:53 [INFO] Loss tensor: [1] [66287.7265625]
14:08:53 [INFO] Mean epoch policy entropy for the trajectories is: 0.68205017
14:08:56 [INFO] Training epoch 41
14:08:57 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:08:57 [INFO] Size of sampled logits tensor [400, 2, 1]
14:08:57 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:08:57 [INFO] Unweighted rewards: [400, 2]
14:08:57 [INFO] Weighted rewards: [400]
14:08:57 [INFO] Loss tensor: [1] [77176.0546875]
14:08:58 [INFO] Mean epoch policy entropy for the trajectories is: 0.7891036
14:09:01 [INFO] Training epoch 42
14:09:02 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:09:02 [INFO] Size of sampled logits tensor [400, 2, 1]
14:09:02 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:09:02 [INFO] Unweighted rewards: [400, 2]
14:09:02 [INFO] Weighted rewards: [400]
14:09:02 [INFO] Loss tensor: [1] [58310.33203125]
14:09:02 [INFO] Mean epoch policy entropy for the trajectories is: 0.78240824
14:09:05 [INFO] Training epoch 43
14:09:06 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:09:06 [INFO] Size of sampled logits tensor [400, 2, 1]
14:09:06 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:09:06 [INFO] Unweighted rewards: [400, 2]
14:09:06 [INFO] Weighted rewards: [400]
14:09:06 [INFO] Loss tensor: [1] [55903.60546875]
14:09:07 [INFO] Mean epoch policy entropy for the trajectories is: 0.8411762
14:09:10 [INFO] Training epoch 44
14:09:11 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:09:11 [INFO] Size of sampled logits tensor [400, 2, 1]
14:09:11 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:09:11 [INFO] Unweighted rewards: [400, 2]
14:09:11 [INFO] Weighted rewards: [400]
14:09:11 [INFO] Loss tensor: [1] [56064.05859375]
14:09:11 [INFO] Mean epoch policy entropy for the trajectories is: 0.79494894
14:09:14 [INFO] Training epoch 45
14:09:15 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:09:15 [INFO] Size of sampled logits tensor [400, 2, 1]
14:09:15 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:09:15 [INFO] Unweighted rewards: [400, 2]
14:09:15 [INFO] Weighted rewards: [400]
14:09:15 [INFO] Loss tensor: [1] [69354.234375]
14:09:16 [INFO] Mean epoch policy entropy for the trajectories is: 0.780995
14:09:19 [INFO] Training epoch 46
14:09:20 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:09:20 [INFO] Size of sampled logits tensor [400, 2, 1]
14:09:20 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:09:20 [INFO] Unweighted rewards: [400, 2]
14:09:20 [INFO] Weighted rewards: [400]
14:09:20 [INFO] Loss tensor: [1] [42391.640625]
14:09:20 [INFO] Mean epoch policy entropy for the trajectories is: 0.72676665
14:09:23 [INFO] Training epoch 47
14:09:24 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:09:24 [INFO] Size of sampled logits tensor [400, 2, 1]
14:09:24 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:09:24 [INFO] Unweighted rewards: [400, 2]
14:09:24 [INFO] Weighted rewards: [400]
14:09:24 [INFO] Loss tensor: [1] [74039.4921875]
14:09:25 [INFO] Mean epoch policy entropy for the trajectories is: 0.78147453
14:09:28 [INFO] Training epoch 48
14:09:29 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:09:29 [INFO] Size of sampled logits tensor [400, 2, 1]
14:09:29 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:09:29 [INFO] Unweighted rewards: [400, 2]
14:09:29 [INFO] Weighted rewards: [400]
14:09:29 [INFO] Loss tensor: [1] [110703.359375]
14:09:29 [INFO] Mean epoch policy entropy for the trajectories is: 0.8775341
14:09:32 [INFO] Training epoch 49
14:09:33 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:09:33 [INFO] Size of sampled logits tensor [400, 2, 1]
14:09:33 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:09:33 [INFO] Unweighted rewards: [400, 2]
14:09:33 [INFO] Weighted rewards: [400]
14:09:33 [INFO] Loss tensor: [1] [97459.3515625]
14:09:34 [INFO] Mean epoch policy entropy for the trajectories is: 0.93068296
14:09:37 [INFO] Training epoch 50
14:09:38 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:09:38 [INFO] Size of sampled logits tensor [400, 2, 1]
14:09:38 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:09:38 [INFO] Unweighted rewards: [400, 2]
14:09:38 [INFO] Weighted rewards: [400]
14:09:38 [INFO] Loss tensor: [1] [73743.25]
14:09:38 [INFO] Mean epoch policy entropy for the trajectories is: 1.0359967
14:09:41 [INFO] Training epoch 51
14:09:42 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:09:42 [INFO] Size of sampled logits tensor [400, 2, 1]
14:09:42 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:09:42 [INFO] Unweighted rewards: [400, 2]
14:09:42 [INFO] Weighted rewards: [400]
14:09:42 [INFO] Loss tensor: [1] [102345.2265625]
14:09:43 [INFO] Mean epoch policy entropy for the trajectories is: 1.0673469
14:09:46 [INFO] Training epoch 52
14:09:47 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:09:47 [INFO] Size of sampled logits tensor [400, 2, 1]
14:09:47 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:09:47 [INFO] Unweighted rewards: [400, 2]
14:09:47 [INFO] Weighted rewards: [400]
14:09:47 [INFO] Loss tensor: [1] [46535.30859375]
14:09:47 [INFO] Mean epoch policy entropy for the trajectories is: 0.9601843
14:09:50 [INFO] Training epoch 53
14:09:51 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:09:51 [INFO] Size of sampled logits tensor [400, 2, 1]
14:09:51 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:09:51 [INFO] Unweighted rewards: [400, 2]
14:09:51 [INFO] Weighted rewards: [400]
14:09:52 [INFO] Loss tensor: [1] [78215.5]
14:09:52 [INFO] Mean epoch policy entropy for the trajectories is: 1.028297
14:09:55 [INFO] Training epoch 54
14:09:56 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:09:56 [INFO] Size of sampled logits tensor [400, 2, 1]
14:09:56 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:09:56 [INFO] Unweighted rewards: [400, 2]
14:09:56 [INFO] Weighted rewards: [400]
14:09:56 [INFO] Loss tensor: [1] [58430.19921875]
14:09:56 [INFO] Mean epoch policy entropy for the trajectories is: 0.992681
14:09:59 [INFO] Training epoch 55
14:10:00 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:10:00 [INFO] Size of sampled logits tensor [400, 2, 1]
14:10:00 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:10:00 [INFO] Unweighted rewards: [400, 2]
14:10:00 [INFO] Weighted rewards: [400]
14:10:00 [INFO] Loss tensor: [1] [105835.859375]
14:10:01 [INFO] Mean epoch policy entropy for the trajectories is: 0.9987845
14:10:04 [INFO] Training epoch 56
14:10:05 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:10:05 [INFO] Size of sampled logits tensor [400, 2, 1]
14:10:05 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:10:05 [INFO] Unweighted rewards: [400, 2]
14:10:05 [INFO] Weighted rewards: [400]
14:10:05 [INFO] Loss tensor: [1] [90374.484375]
14:10:05 [INFO] Mean epoch policy entropy for the trajectories is: 1.0490046
14:10:08 [INFO] Training epoch 57
14:10:10 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:10:10 [INFO] Size of sampled logits tensor [400, 2, 1]
14:10:10 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:10:10 [INFO] Unweighted rewards: [400, 2]
14:10:10 [INFO] Weighted rewards: [400]
14:10:10 [INFO] Loss tensor: [1] [100243.4296875]
14:10:10 [INFO] Mean epoch policy entropy for the trajectories is: 0.95531744
14:10:13 [INFO] Training epoch 58
14:10:14 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:10:14 [INFO] Size of sampled logits tensor [400, 2, 1]
14:10:14 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:10:14 [INFO] Unweighted rewards: [400, 2]
14:10:14 [INFO] Weighted rewards: [400]
14:10:14 [INFO] Loss tensor: [1] [66392.6328125]
14:10:14 [INFO] Mean epoch policy entropy for the trajectories is: 0.8099596
14:10:17 [INFO] Training epoch 59
14:10:19 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:10:19 [INFO] Size of sampled logits tensor [400, 2, 1]
14:10:19 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:10:19 [INFO] Unweighted rewards: [400, 2]
14:10:19 [INFO] Weighted rewards: [400]
14:10:19 [INFO] Loss tensor: [1] [59248.26953125]
14:10:19 [INFO] Mean epoch policy entropy for the trajectories is: 0.6988962
14:10:22 [INFO] Training epoch 60
14:10:23 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:10:23 [INFO] Size of sampled logits tensor [400, 2, 1]
14:10:23 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:10:23 [INFO] Unweighted rewards: [400, 2]
14:10:23 [INFO] Weighted rewards: [400]
14:10:23 [INFO] Loss tensor: [1] [75556.9609375]
14:10:23 [INFO] Mean epoch policy entropy for the trajectories is: 0.620731
14:10:26 [INFO] Training epoch 61
14:10:28 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:10:28 [INFO] Size of sampled logits tensor [400, 2, 1]
14:10:28 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:10:28 [INFO] Unweighted rewards: [400, 2]
14:10:28 [INFO] Weighted rewards: [400]
14:10:28 [INFO] Loss tensor: [1] [52816.26953125]
14:10:28 [INFO] Mean epoch policy entropy for the trajectories is: 0.6036
14:10:31 [INFO] Training epoch 62
14:10:32 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:10:32 [INFO] Size of sampled logits tensor [400, 2, 1]
14:10:32 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:10:32 [INFO] Unweighted rewards: [400, 2]
14:10:32 [INFO] Weighted rewards: [400]
14:10:32 [INFO] Loss tensor: [1] [56523.2890625]
14:10:32 [INFO] Mean epoch policy entropy for the trajectories is: 0.5977886
14:10:35 [INFO] Training epoch 63
14:10:37 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:10:37 [INFO] Size of sampled logits tensor [400, 2, 1]
14:10:37 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:10:37 [INFO] Unweighted rewards: [400, 2]
14:10:37 [INFO] Weighted rewards: [400]
14:10:37 [INFO] Loss tensor: [1] [54644.37890625]
14:10:37 [INFO] Mean epoch policy entropy for the trajectories is: 0.61849505
14:10:40 [INFO] Training epoch 64
14:10:41 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:10:41 [INFO] Size of sampled logits tensor [400, 2, 1]
14:10:41 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:10:41 [INFO] Unweighted rewards: [400, 2]
14:10:41 [INFO] Weighted rewards: [400]
14:10:41 [INFO] Loss tensor: [1] [70342.109375]
14:10:41 [INFO] Mean epoch policy entropy for the trajectories is: 0.63902104
14:10:44 [INFO] Training epoch 65
14:10:46 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:10:46 [INFO] Size of sampled logits tensor [400, 2, 1]
14:10:46 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:10:46 [INFO] Unweighted rewards: [400, 2]
14:10:46 [INFO] Weighted rewards: [400]
14:10:46 [INFO] Loss tensor: [1] [56625.9296875]
14:10:46 [INFO] Mean epoch policy entropy for the trajectories is: 0.67056
14:10:49 [INFO] Training epoch 66
14:10:50 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:10:50 [INFO] Size of sampled logits tensor [400, 2, 1]
14:10:50 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:10:50 [INFO] Unweighted rewards: [400, 2]
14:10:50 [INFO] Weighted rewards: [400]
14:10:50 [INFO] Loss tensor: [1] [49335.8203125]
14:10:50 [INFO] Mean epoch policy entropy for the trajectories is: 0.659702
14:10:53 [INFO] Training epoch 67
14:10:55 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:10:55 [INFO] Size of sampled logits tensor [400, 2, 1]
14:10:55 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:10:55 [INFO] Unweighted rewards: [400, 2]
14:10:55 [INFO] Weighted rewards: [400]
14:10:55 [INFO] Loss tensor: [1] [99316.1171875]
14:10:55 [INFO] Mean epoch policy entropy for the trajectories is: 0.7106117
14:10:58 [INFO] Training epoch 68
14:10:59 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:10:59 [INFO] Size of sampled logits tensor [400, 2, 1]
14:10:59 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:10:59 [INFO] Unweighted rewards: [400, 2]
14:10:59 [INFO] Weighted rewards: [400]
14:10:59 [INFO] Loss tensor: [1] [76522.234375]
14:10:59 [INFO] Mean epoch policy entropy for the trajectories is: 0.63145775
14:11:02 [INFO] Training epoch 69
14:11:04 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:11:04 [INFO] Size of sampled logits tensor [400, 2, 1]
14:11:04 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:11:04 [INFO] Unweighted rewards: [400, 2]
14:11:04 [INFO] Weighted rewards: [400]
14:11:04 [INFO] Loss tensor: [1] [23603.13671875]
14:11:04 [INFO] Mean epoch policy entropy for the trajectories is: 0.6655072
14:11:07 [INFO] Training epoch 70
14:11:08 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:11:08 [INFO] Size of sampled logits tensor [400, 2, 1]
14:11:08 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:11:08 [INFO] Unweighted rewards: [400, 2]
14:11:08 [INFO] Weighted rewards: [400]
14:11:08 [INFO] Loss tensor: [1] [47481.6875]
14:11:08 [INFO] Mean epoch policy entropy for the trajectories is: 0.7000927
14:11:11 [INFO] Training epoch 71
14:11:13 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:11:13 [INFO] Size of sampled logits tensor [400, 2, 1]
14:11:13 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:11:13 [INFO] Unweighted rewards: [400, 2]
14:11:13 [INFO] Weighted rewards: [400]
14:11:13 [INFO] Loss tensor: [1] [51203.6171875]
14:11:13 [INFO] Mean epoch policy entropy for the trajectories is: 0.6763297
14:11:16 [INFO] Training epoch 72
14:11:17 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:11:17 [INFO] Size of sampled logits tensor [400, 2, 1]
14:11:17 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:11:17 [INFO] Unweighted rewards: [400, 2]
14:11:17 [INFO] Weighted rewards: [400]
14:11:17 [INFO] Loss tensor: [1] [68526.7109375]
14:11:18 [INFO] Mean epoch policy entropy for the trajectories is: 0.7891455
14:11:21 [INFO] Training epoch 73
14:11:22 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:11:22 [INFO] Size of sampled logits tensor [400, 2, 1]
14:11:22 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:11:22 [INFO] Unweighted rewards: [400, 2]
14:11:22 [INFO] Weighted rewards: [400]
14:11:22 [INFO] Loss tensor: [1] [67251.2109375]
14:11:22 [INFO] Mean epoch policy entropy for the trajectories is: 0.7903849
14:11:25 [INFO] Training epoch 74
14:11:27 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:11:27 [INFO] Size of sampled logits tensor [400, 2, 1]
14:11:27 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:11:27 [INFO] Unweighted rewards: [400, 2]
14:11:27 [INFO] Weighted rewards: [400]
14:11:27 [INFO] Loss tensor: [1] [125618.1171875]
14:11:27 [INFO] Mean epoch policy entropy for the trajectories is: 0.7920592
14:11:30 [INFO] Training epoch 75
14:11:31 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:11:31 [INFO] Size of sampled logits tensor [400, 2, 1]
14:11:31 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:11:31 [INFO] Unweighted rewards: [400, 2]
14:11:31 [INFO] Weighted rewards: [400]
14:11:31 [INFO] Loss tensor: [1] [60330.1484375]
14:11:31 [INFO] Mean epoch policy entropy for the trajectories is: 0.72667104
14:11:34 [INFO] Training epoch 76
14:11:36 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:11:36 [INFO] Size of sampled logits tensor [400, 2, 1]
14:11:36 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:11:36 [INFO] Unweighted rewards: [400, 2]
14:11:36 [INFO] Weighted rewards: [400]
14:11:36 [INFO] Loss tensor: [1] [69113.1796875]
14:11:36 [INFO] Mean epoch policy entropy for the trajectories is: 0.7273849
14:11:39 [INFO] Training epoch 77
14:11:40 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:11:40 [INFO] Size of sampled logits tensor [400, 2, 1]
14:11:40 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:11:40 [INFO] Unweighted rewards: [400, 2]
14:11:40 [INFO] Weighted rewards: [400]
14:11:40 [INFO] Loss tensor: [1] [51802.12890625]
14:11:41 [INFO] Mean epoch policy entropy for the trajectories is: 0.7860639
14:11:44 [INFO] Training epoch 78
14:11:45 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:11:45 [INFO] Size of sampled logits tensor [400, 2, 1]
14:11:45 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:11:45 [INFO] Unweighted rewards: [400, 2]
14:11:45 [INFO] Weighted rewards: [400]
14:11:45 [INFO] Loss tensor: [1] [26981.03125]
14:11:45 [INFO] Mean epoch policy entropy for the trajectories is: 0.7624559
14:11:48 [INFO] Training epoch 79
14:11:50 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:11:50 [INFO] Size of sampled logits tensor [400, 2, 1]
14:11:50 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:11:50 [INFO] Unweighted rewards: [400, 2]
14:11:50 [INFO] Weighted rewards: [400]
14:11:50 [INFO] Loss tensor: [1] [96027.1171875]
14:11:50 [INFO] Mean epoch policy entropy for the trajectories is: 0.8186373
14:11:53 [INFO] Training epoch 80
14:11:54 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:11:54 [INFO] Size of sampled logits tensor [400, 2, 1]
14:11:54 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:11:54 [INFO] Unweighted rewards: [400, 2]
14:11:54 [INFO] Weighted rewards: [400]
14:11:54 [INFO] Loss tensor: [1] [66866.9296875]
14:11:54 [INFO] Mean epoch policy entropy for the trajectories is: 0.787868
14:11:57 [INFO] Training epoch 81
14:11:59 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:11:59 [INFO] Size of sampled logits tensor [400, 2, 1]
14:11:59 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:11:59 [INFO] Unweighted rewards: [400, 2]
14:11:59 [INFO] Weighted rewards: [400]
14:11:59 [INFO] Loss tensor: [1] [33427.62109375]
14:11:59 [INFO] Mean epoch policy entropy for the trajectories is: 0.76637024
14:12:02 [INFO] Training epoch 82
14:12:03 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:12:03 [INFO] Size of sampled logits tensor [400, 2, 1]
14:12:03 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:12:03 [INFO] Unweighted rewards: [400, 2]
14:12:03 [INFO] Weighted rewards: [400]
14:12:03 [INFO] Loss tensor: [1] [77855.96875]
14:12:03 [INFO] Mean epoch policy entropy for the trajectories is: 0.84840417
14:12:06 [INFO] Training epoch 83
14:12:08 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:12:08 [INFO] Size of sampled logits tensor [400, 2, 1]
14:12:08 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:12:08 [INFO] Unweighted rewards: [400, 2]
14:12:08 [INFO] Weighted rewards: [400]
14:12:08 [INFO] Loss tensor: [1] [46622.00390625]
14:12:08 [INFO] Mean epoch policy entropy for the trajectories is: 0.7299551
14:12:11 [INFO] Training epoch 84
14:12:12 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:12:12 [INFO] Size of sampled logits tensor [400, 2, 1]
14:12:12 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:12:12 [INFO] Unweighted rewards: [400, 2]
14:12:12 [INFO] Weighted rewards: [400]
14:12:12 [INFO] Loss tensor: [1] [90835.3203125]
14:12:12 [INFO] Mean epoch policy entropy for the trajectories is: 0.7929293
14:12:15 [INFO] Training epoch 85
14:12:16 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:12:16 [INFO] Size of sampled logits tensor [400, 2, 1]
14:12:16 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:12:16 [INFO] Unweighted rewards: [400, 2]
14:12:16 [INFO] Weighted rewards: [400]
14:12:16 [INFO] Loss tensor: [1] [87609.3984375]
14:12:17 [INFO] Mean epoch policy entropy for the trajectories is: 0.7994148
14:12:20 [INFO] Training epoch 86
14:12:21 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:12:21 [INFO] Size of sampled logits tensor [400, 2, 1]
14:12:21 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:12:21 [INFO] Unweighted rewards: [400, 2]
14:12:21 [INFO] Weighted rewards: [400]
14:12:21 [INFO] Loss tensor: [1] [53707.2578125]
14:12:21 [INFO] Mean epoch policy entropy for the trajectories is: 0.85012835
14:12:24 [INFO] Training epoch 87
14:12:25 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:12:25 [INFO] Size of sampled logits tensor [400, 2, 1]
14:12:25 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:12:25 [INFO] Unweighted rewards: [400, 2]
14:12:25 [INFO] Weighted rewards: [400]
14:12:25 [INFO] Loss tensor: [1] [98650.1796875]
14:12:26 [INFO] Mean epoch policy entropy for the trajectories is: 0.757045
14:12:29 [INFO] Training epoch 88
14:12:30 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:12:30 [INFO] Size of sampled logits tensor [400, 2, 1]
14:12:30 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:12:30 [INFO] Unweighted rewards: [400, 2]
14:12:30 [INFO] Weighted rewards: [400]
14:12:30 [INFO] Loss tensor: [1] [66469.484375]
14:12:30 [INFO] Mean epoch policy entropy for the trajectories is: 0.84848315
14:12:33 [INFO] Training epoch 89
14:12:34 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:12:34 [INFO] Size of sampled logits tensor [400, 2, 1]
14:12:34 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:12:34 [INFO] Unweighted rewards: [400, 2]
14:12:34 [INFO] Weighted rewards: [400]
14:12:34 [INFO] Loss tensor: [1] [60523.34765625]
14:12:35 [INFO] Mean epoch policy entropy for the trajectories is: 0.7817261
14:12:38 [INFO] Training epoch 90
14:12:39 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:12:39 [INFO] Size of sampled logits tensor [400, 2, 1]
14:12:39 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:12:39 [INFO] Unweighted rewards: [400, 2]
14:12:39 [INFO] Weighted rewards: [400]
14:12:39 [INFO] Loss tensor: [1] [72293.671875]
14:12:39 [INFO] Mean epoch policy entropy for the trajectories is: 0.78847903
14:12:42 [INFO] Training epoch 91
14:12:43 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:12:43 [INFO] Size of sampled logits tensor [400, 2, 1]
14:12:43 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:12:43 [INFO] Unweighted rewards: [400, 2]
14:12:43 [INFO] Weighted rewards: [400]
14:12:43 [INFO] Loss tensor: [1] [46125.6796875]
14:12:43 [INFO] Mean epoch policy entropy for the trajectories is: 0.8595772
14:12:46 [INFO] Training epoch 92
14:12:48 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:12:48 [INFO] Size of sampled logits tensor [400, 2, 1]
14:12:48 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:12:48 [INFO] Unweighted rewards: [400, 2]
14:12:48 [INFO] Weighted rewards: [400]
14:12:48 [INFO] Loss tensor: [1] [100271.2265625]
14:12:48 [INFO] Mean epoch policy entropy for the trajectories is: 0.82947356
14:12:51 [INFO] Training epoch 93
14:12:52 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:12:52 [INFO] Size of sampled logits tensor [400, 2, 1]
14:12:52 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:12:52 [INFO] Unweighted rewards: [400, 2]
14:12:52 [INFO] Weighted rewards: [400]
14:12:52 [INFO] Loss tensor: [1] [67089.171875]
14:12:52 [INFO] Mean epoch policy entropy for the trajectories is: 0.7712661
14:12:55 [INFO] Training epoch 94
14:12:57 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:12:57 [INFO] Size of sampled logits tensor [400, 2, 1]
14:12:57 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:12:57 [INFO] Unweighted rewards: [400, 2]
14:12:57 [INFO] Weighted rewards: [400]
14:12:57 [INFO] Loss tensor: [1] [68985.1015625]
14:12:57 [INFO] Mean epoch policy entropy for the trajectories is: 0.7953714
14:13:00 [INFO] Training epoch 95
14:13:01 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:13:01 [INFO] Size of sampled logits tensor [400, 2, 1]
14:13:01 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:13:01 [INFO] Unweighted rewards: [400, 2]
14:13:01 [INFO] Weighted rewards: [400]
14:13:01 [INFO] Loss tensor: [1] [33115.0859375]
14:13:01 [INFO] Mean epoch policy entropy for the trajectories is: 0.85092115
14:13:04 [INFO] Training epoch 96
14:13:06 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:13:06 [INFO] Size of sampled logits tensor [400, 2, 1]
14:13:06 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:13:06 [INFO] Unweighted rewards: [400, 2]
14:13:06 [INFO] Weighted rewards: [400]
14:13:06 [INFO] Loss tensor: [1] [66996.3671875]
14:13:06 [INFO] Mean epoch policy entropy for the trajectories is: 0.80894226
14:13:09 [INFO] Training epoch 97
14:13:10 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:13:10 [INFO] Size of sampled logits tensor [400, 2, 1]
14:13:10 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:13:10 [INFO] Unweighted rewards: [400, 2]
14:13:10 [INFO] Weighted rewards: [400]
14:13:10 [INFO] Loss tensor: [1] [48002.234375]
14:13:10 [INFO] Mean epoch policy entropy for the trajectories is: 0.80625236
14:13:13 [INFO] Training epoch 98
14:13:15 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:13:15 [INFO] Size of sampled logits tensor [400, 2, 1]
14:13:15 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:13:15 [INFO] Unweighted rewards: [400, 2]
14:13:15 [INFO] Weighted rewards: [400]
14:13:15 [INFO] Loss tensor: [1] [90680.8359375]
14:13:15 [INFO] Mean epoch policy entropy for the trajectories is: 0.79194045
14:13:18 [INFO] Training epoch 99
14:13:19 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:13:19 [INFO] Size of sampled logits tensor [400, 2, 1]
14:13:19 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:13:19 [INFO] Unweighted rewards: [400, 2]
14:13:19 [INFO] Weighted rewards: [400]
14:13:19 [INFO] Loss tensor: [1] [86009.15625]
14:13:19 [INFO] Mean epoch policy entropy for the trajectories is: 0.7918758
14:13:22 [INFO] The random scrambling applied these moves: [[RPlus, LMinus], [LPlus, DPlus], [LPlus, RPlus], [LPlus, UMinus], [RPlus, DMinus], [DMinus, UMinus], [RMinus, LPlus], [LMinus, UMinus], [FMinus, DMinus], [DMinus, BPlus]] for testing
14:13:22 [INFO] Running the test loop now: 
14:13:22 [INFO] Number of tests: 10
14:13:22 [INFO] Scrambled cube used for test 0 used this moves: [RPlus, LMinus]
14:13:22 [INFO] Got the cubemove from policy: RMinus for turn 0
14:13:22 [INFO] The above is complementary or not: false
14:13:23 [INFO] The cross entropy for turn 0 0.02486148
14:13:23 [INFO] Got the cubemove from policy: DPlus for turn 1
14:13:23 [INFO] The above is complementary or not: false
14:13:23 [INFO] The cross entropy for turn 1 2.5649495
14:13:23 [INFO] Scrambled cube used for test 1 used this moves: [LPlus, DPlus]
14:13:23 [INFO] Got the cubemove from policy: LPlus for turn 0
14:13:23 [INFO] The above is complementary or not: false
14:13:23 [INFO] The cross entropy for turn 0 2.5649495
14:13:23 [INFO] Got the cubemove from policy: FMinus for turn 1
14:13:23 [INFO] The above is complementary or not: false
14:13:23 [INFO] The cross entropy for turn 1 2.5649495
14:13:23 [INFO] Scrambled cube used for test 2 used this moves: [LPlus, RPlus]
14:13:23 [INFO] Got the cubemove from policy: RMinus for turn 0
14:13:23 [INFO] The above is complementary or not: true
14:13:23 [INFO] The cross entropy for turn 0 0.0087020565
14:13:23 [INFO] Got the cubemove from policy: FMinus for turn 1
14:13:23 [INFO] The above is complementary or not: false
14:13:23 [INFO] The cross entropy for turn 1 2.5649495
14:13:23 [INFO] Scrambled cube used for test 3 used this moves: [LPlus, UMinus]
14:13:23 [INFO] Got the cubemove from policy: UPlus for turn 0
14:13:23 [INFO] The above is complementary or not: true
14:13:23 [INFO] The cross entropy for turn 0 0.18521956
14:13:23 [INFO] Got the cubemove from policy: LMinus for turn 1
14:13:23 [INFO] The above is complementary or not: true
14:13:23 [INFO] The cross entropy for turn 1 2.5649495
14:13:23 [INFO] Scrambled cube used for test 4 used this moves: [RPlus, DMinus]
14:13:23 [INFO] Got the cubemove from policy: DPlus for turn 0
14:13:23 [INFO] The above is complementary or not: true
14:13:23 [INFO] The cross entropy for turn 0 0.0044327984
14:13:23 [INFO] Got the cubemove from policy: RMinus for turn 1
14:13:23 [INFO] The above is complementary or not: true
14:13:23 [INFO] The cross entropy for turn 1 0.00012127536
14:13:23 [INFO] Scrambled cube used for test 5 used this moves: [DMinus, UMinus]
14:13:23 [INFO] Got the cubemove from policy: UPlus for turn 0
14:13:23 [INFO] The above is complementary or not: true
14:13:23 [INFO] The cross entropy for turn 0 0.0014986359
14:13:23 [INFO] Got the cubemove from policy: DPlus for turn 1
14:13:23 [INFO] The above is complementary or not: true
14:13:23 [INFO] The cross entropy for turn 1 0.00087936653
14:13:23 [INFO] Scrambled cube used for test 6 used this moves: [RMinus, LPlus]
14:13:23 [INFO] Got the cubemove from policy: RPlus for turn 0
14:13:23 [INFO] The above is complementary or not: false
14:13:23 [INFO] The cross entropy for turn 0 0.100099474
14:13:23 [INFO] Got the cubemove from policy: FMinus for turn 1
14:13:23 [INFO] The above is complementary or not: false
14:13:23 [INFO] The cross entropy for turn 1 2.5649495
14:13:23 [INFO] Scrambled cube used for test 7 used this moves: [LMinus, UMinus]
14:13:23 [INFO] Got the cubemove from policy: UPlus for turn 0
14:13:23 [INFO] The above is complementary or not: true
14:13:23 [INFO] The cross entropy for turn 0 0.11202797
14:13:23 [INFO] Got the cubemove from policy: NoOp for turn 1
14:13:23 [INFO] The above is complementary or not: false
14:13:23 [INFO] The cross entropy for turn 1 2.5649495
14:13:23 [INFO] Scrambled cube used for test 8 used this moves: [FMinus, DMinus]
14:13:23 [INFO] Got the cubemove from policy: DPlus for turn 0
14:13:23 [INFO] The above is complementary or not: true
14:13:23 [INFO] The cross entropy for turn 0 0.007984062
14:13:23 [INFO] Got the cubemove from policy: FPlus for turn 1
14:13:23 [INFO] The above is complementary or not: true
14:13:23 [INFO] The cross entropy for turn 1 0.00029505865
14:13:23 [INFO] Scrambled cube used for test 9 used this moves: [DMinus, BPlus]
14:13:23 [INFO] Got the cubemove from policy: BMinus for turn 0
14:13:23 [INFO] The above is complementary or not: true
14:13:23 [INFO] The cross entropy for turn 0 0.040490676
14:13:23 [INFO] Got the cubemove from policy: DPlus for turn 1
14:13:23 [INFO] The above is complementary or not: true
14:13:23 [INFO] The cross entropy for turn 1 0.00087936653
14:13:23 [INFO] Mean loss for tests 0.0
14:13:23 [INFO] Mean entropy for tests 0.9221069
14:13:23 [INFO] Mean correct moves for 1.2
