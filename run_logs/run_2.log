14:00:39 [INFO] Is MPS available: true
14:00:39 [INFO] Using the following hyperparameters:
14:00:39 [INFO] Hidden layer dimensions: 2500
14:00:39 [INFO] Number of layers: 4
14:00:39 [INFO] Number of trajectories: 400
14:00:39 [INFO] Trajectory depth: 2
14:00:39 [INFO] Number of epochs: 50
14:00:39 [INFO] Learning rate: 0.0001
14:00:39 [INFO] Starting policy training...
14:00:39 [INFO] Training epoch 0
14:00:40 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:00:40 [INFO] Size of sampled logits tensor [400, 2, 1]
14:00:40 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:00:40 [INFO] Unweighted rewards: [400, 2]
14:00:40 [INFO] Weighted rewards: [400]
14:00:40 [INFO] Loss tensor: [1] [-156942.828125]
14:00:40 [INFO] Mean epoch policy entropy for the trajectories is: 2.5604694
14:00:44 [INFO] Training epoch 1
14:00:45 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:00:45 [INFO] Size of sampled logits tensor [400, 2, 1]
14:00:45 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:00:45 [INFO] Unweighted rewards: [400, 2]
14:00:45 [INFO] Weighted rewards: [400]
14:00:45 [INFO] Loss tensor: [1] [-196720.140625]
14:00:45 [INFO] Mean epoch policy entropy for the trajectories is: 2.5535364
14:00:49 [INFO] Training epoch 2
14:00:50 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:00:50 [INFO] Size of sampled logits tensor [400, 2, 1]
14:00:50 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:00:50 [INFO] Unweighted rewards: [400, 2]
14:00:50 [INFO] Weighted rewards: [400]
14:00:50 [INFO] Loss tensor: [1] [-35645.25]
14:00:50 [INFO] Mean epoch policy entropy for the trajectories is: 2.5410469
14:00:53 [INFO] Training epoch 3
14:00:54 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:00:54 [INFO] Size of sampled logits tensor [400, 2, 1]
14:00:54 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:00:54 [INFO] Unweighted rewards: [400, 2]
14:00:54 [INFO] Weighted rewards: [400]
14:00:54 [INFO] Loss tensor: [1] [-147547.09375]
14:00:55 [INFO] Mean epoch policy entropy for the trajectories is: 2.518315
14:00:58 [INFO] Training epoch 4
14:00:59 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:00:59 [INFO] Size of sampled logits tensor [400, 2, 1]
14:00:59 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:00:59 [INFO] Unweighted rewards: [400, 2]
14:00:59 [INFO] Weighted rewards: [400]
14:00:59 [INFO] Loss tensor: [1] [-24717.06640625]
14:00:59 [INFO] Mean epoch policy entropy for the trajectories is: 2.488856
14:01:02 [INFO] Training epoch 5
14:01:03 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:01:03 [INFO] Size of sampled logits tensor [400, 2, 1]
14:01:03 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:01:03 [INFO] Unweighted rewards: [400, 2]
14:01:03 [INFO] Weighted rewards: [400]
14:01:03 [INFO] Loss tensor: [1] [-81541.1015625]
14:01:04 [INFO] Mean epoch policy entropy for the trajectories is: 2.4450057
14:01:07 [INFO] Training epoch 6
14:01:08 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:01:08 [INFO] Size of sampled logits tensor [400, 2, 1]
14:01:08 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:01:08 [INFO] Unweighted rewards: [400, 2]
14:01:08 [INFO] Weighted rewards: [400]
14:01:08 [INFO] Loss tensor: [1] [1624.20751953125]
14:01:08 [INFO] Mean epoch policy entropy for the trajectories is: 2.3781438
14:01:11 [INFO] Training epoch 7
14:01:12 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:01:12 [INFO] Size of sampled logits tensor [400, 2, 1]
14:01:12 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:01:12 [INFO] Unweighted rewards: [400, 2]
14:01:12 [INFO] Weighted rewards: [400]
14:01:12 [INFO] Loss tensor: [1] [29371.302734375]
14:01:13 [INFO] Mean epoch policy entropy for the trajectories is: 2.2915795
14:01:16 [INFO] Training epoch 8
14:01:17 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:01:17 [INFO] Size of sampled logits tensor [400, 2, 1]
14:01:17 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:01:17 [INFO] Unweighted rewards: [400, 2]
14:01:17 [INFO] Weighted rewards: [400]
14:01:17 [INFO] Loss tensor: [1] [32707.732421875]
14:01:17 [INFO] Mean epoch policy entropy for the trajectories is: 2.201774
14:01:20 [INFO] Training epoch 9
14:01:21 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:01:21 [INFO] Size of sampled logits tensor [400, 2, 1]
14:01:21 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:01:21 [INFO] Unweighted rewards: [400, 2]
14:01:21 [INFO] Weighted rewards: [400]
14:01:21 [INFO] Loss tensor: [1] [48613.66015625]
14:01:22 [INFO] Mean epoch policy entropy for the trajectories is: 2.1411252
14:01:25 [INFO] Training epoch 10
14:01:26 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:01:26 [INFO] Size of sampled logits tensor [400, 2, 1]
14:01:26 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:01:26 [INFO] Unweighted rewards: [400, 2]
14:01:26 [INFO] Weighted rewards: [400]
14:01:26 [INFO] Loss tensor: [1] [73386.5078125]
14:01:26 [INFO] Mean epoch policy entropy for the trajectories is: 2.0409856
14:01:29 [INFO] Training epoch 11
14:01:30 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:01:30 [INFO] Size of sampled logits tensor [400, 2, 1]
14:01:30 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:01:30 [INFO] Unweighted rewards: [400, 2]
14:01:30 [INFO] Weighted rewards: [400]
14:01:30 [INFO] Loss tensor: [1] [69139.828125]
14:01:31 [INFO] Mean epoch policy entropy for the trajectories is: 1.927523
14:01:33 [INFO] Training epoch 12
14:01:35 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:01:35 [INFO] Size of sampled logits tensor [400, 2, 1]
14:01:35 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:01:35 [INFO] Unweighted rewards: [400, 2]
14:01:35 [INFO] Weighted rewards: [400]
14:01:35 [INFO] Loss tensor: [1] [83733.5234375]
14:01:35 [INFO] Mean epoch policy entropy for the trajectories is: 1.8058562
14:01:38 [INFO] Training epoch 13
14:01:39 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:01:39 [INFO] Size of sampled logits tensor [400, 2, 1]
14:01:39 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:01:39 [INFO] Unweighted rewards: [400, 2]
14:01:39 [INFO] Weighted rewards: [400]
14:01:39 [INFO] Loss tensor: [1] [129946.0]
14:01:39 [INFO] Mean epoch policy entropy for the trajectories is: 1.6065452
14:01:42 [INFO] Training epoch 14
14:01:44 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:01:44 [INFO] Size of sampled logits tensor [400, 2, 1]
14:01:44 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:01:44 [INFO] Unweighted rewards: [400, 2]
14:01:44 [INFO] Weighted rewards: [400]
14:01:44 [INFO] Loss tensor: [1] [131255.3125]
14:01:44 [INFO] Mean epoch policy entropy for the trajectories is: 1.5035689
14:01:47 [INFO] Training epoch 15
14:01:48 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:01:48 [INFO] Size of sampled logits tensor [400, 2, 1]
14:01:48 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:01:48 [INFO] Unweighted rewards: [400, 2]
14:01:48 [INFO] Weighted rewards: [400]
14:01:48 [INFO] Loss tensor: [1] [144212.03125]
14:01:48 [INFO] Mean epoch policy entropy for the trajectories is: 1.410051
14:01:51 [INFO] Training epoch 16
14:01:53 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:01:53 [INFO] Size of sampled logits tensor [400, 2, 1]
14:01:53 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:01:53 [INFO] Unweighted rewards: [400, 2]
14:01:53 [INFO] Weighted rewards: [400]
14:01:53 [INFO] Loss tensor: [1] [160474.671875]
14:01:53 [INFO] Mean epoch policy entropy for the trajectories is: 1.3359823
14:01:56 [INFO] Training epoch 17
14:01:57 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:01:57 [INFO] Size of sampled logits tensor [400, 2, 1]
14:01:57 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:01:57 [INFO] Unweighted rewards: [400, 2]
14:01:57 [INFO] Weighted rewards: [400]
14:01:57 [INFO] Loss tensor: [1] [107407.5]
14:01:57 [INFO] Mean epoch policy entropy for the trajectories is: 1.2541511
14:02:00 [INFO] Training epoch 18
14:02:02 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:02:02 [INFO] Size of sampled logits tensor [400, 2, 1]
14:02:02 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:02:02 [INFO] Unweighted rewards: [400, 2]
14:02:02 [INFO] Weighted rewards: [400]
14:02:02 [INFO] Loss tensor: [1] [102196.1875]
14:02:02 [INFO] Mean epoch policy entropy for the trajectories is: 1.1624362
14:02:05 [INFO] Training epoch 19
14:02:06 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:02:06 [INFO] Size of sampled logits tensor [400, 2, 1]
14:02:06 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:02:06 [INFO] Unweighted rewards: [400, 2]
14:02:06 [INFO] Weighted rewards: [400]
14:02:06 [INFO] Loss tensor: [1] [111227.09375]
14:02:06 [INFO] Mean epoch policy entropy for the trajectories is: 1.1136273
14:02:09 [INFO] Training epoch 20
14:02:11 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:02:11 [INFO] Size of sampled logits tensor [400, 2, 1]
14:02:11 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:02:11 [INFO] Unweighted rewards: [400, 2]
14:02:11 [INFO] Weighted rewards: [400]
14:02:11 [INFO] Loss tensor: [1] [88107.0859375]
14:02:11 [INFO] Mean epoch policy entropy for the trajectories is: 1.0489837
14:02:14 [INFO] Training epoch 21
14:02:15 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:02:15 [INFO] Size of sampled logits tensor [400, 2, 1]
14:02:15 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:02:15 [INFO] Unweighted rewards: [400, 2]
14:02:15 [INFO] Weighted rewards: [400]
14:02:15 [INFO] Loss tensor: [1] [100567.375]
14:02:15 [INFO] Mean epoch policy entropy for the trajectories is: 1.0054815
14:02:19 [INFO] Training epoch 22
14:02:20 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:02:20 [INFO] Size of sampled logits tensor [400, 2, 1]
14:02:20 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:02:20 [INFO] Unweighted rewards: [400, 2]
14:02:20 [INFO] Weighted rewards: [400]
14:02:20 [INFO] Loss tensor: [1] [132406.421875]
14:02:20 [INFO] Mean epoch policy entropy for the trajectories is: 0.92394674
14:02:23 [INFO] Training epoch 23
14:02:24 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:02:24 [INFO] Size of sampled logits tensor [400, 2, 1]
14:02:24 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:02:24 [INFO] Unweighted rewards: [400, 2]
14:02:24 [INFO] Weighted rewards: [400]
14:02:24 [INFO] Loss tensor: [1] [109128.6953125]
14:02:24 [INFO] Mean epoch policy entropy for the trajectories is: 0.93025076
14:02:27 [INFO] Training epoch 24
14:02:29 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:02:29 [INFO] Size of sampled logits tensor [400, 2, 1]
14:02:29 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:02:29 [INFO] Unweighted rewards: [400, 2]
14:02:29 [INFO] Weighted rewards: [400]
14:02:29 [INFO] Loss tensor: [1] [135810.359375]
14:02:29 [INFO] Mean epoch policy entropy for the trajectories is: 0.8787311
14:02:32 [INFO] Training epoch 25
14:02:33 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:02:33 [INFO] Size of sampled logits tensor [400, 2, 1]
14:02:33 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:02:33 [INFO] Unweighted rewards: [400, 2]
14:02:33 [INFO] Weighted rewards: [400]
14:02:33 [INFO] Loss tensor: [1] [148443.46875]
14:02:33 [INFO] Mean epoch policy entropy for the trajectories is: 0.8011415
14:02:36 [INFO] Training epoch 26
14:02:38 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:02:38 [INFO] Size of sampled logits tensor [400, 2, 1]
14:02:38 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:02:38 [INFO] Unweighted rewards: [400, 2]
14:02:38 [INFO] Weighted rewards: [400]
14:02:38 [INFO] Loss tensor: [1] [130622.515625]
14:02:38 [INFO] Mean epoch policy entropy for the trajectories is: 0.71720093
14:02:41 [INFO] Training epoch 27
14:02:42 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:02:42 [INFO] Size of sampled logits tensor [400, 2, 1]
14:02:42 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:02:42 [INFO] Unweighted rewards: [400, 2]
14:02:42 [INFO] Weighted rewards: [400]
14:02:42 [INFO] Loss tensor: [1] [88170.21875]
14:02:43 [INFO] Mean epoch policy entropy for the trajectories is: 0.6327063
14:02:46 [INFO] Training epoch 28
14:02:47 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:02:47 [INFO] Size of sampled logits tensor [400, 2, 1]
14:02:47 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:02:47 [INFO] Unweighted rewards: [400, 2]
14:02:47 [INFO] Weighted rewards: [400]
14:02:47 [INFO] Loss tensor: [1] [81904.515625]
14:02:47 [INFO] Mean epoch policy entropy for the trajectories is: 0.5669891
14:02:50 [INFO] Training epoch 29
14:02:51 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:02:51 [INFO] Size of sampled logits tensor [400, 2, 1]
14:02:51 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:02:51 [INFO] Unweighted rewards: [400, 2]
14:02:51 [INFO] Weighted rewards: [400]
14:02:51 [INFO] Loss tensor: [1] [18253.310546875]
14:02:52 [INFO] Mean epoch policy entropy for the trajectories is: 0.53942424
14:02:55 [INFO] Training epoch 30
14:02:56 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:02:56 [INFO] Size of sampled logits tensor [400, 2, 1]
14:02:56 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:02:56 [INFO] Unweighted rewards: [400, 2]
14:02:56 [INFO] Weighted rewards: [400]
14:02:56 [INFO] Loss tensor: [1] [61167.546875]
14:02:56 [INFO] Mean epoch policy entropy for the trajectories is: 0.45232448
14:02:59 [INFO] Training epoch 31
14:03:00 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:03:00 [INFO] Size of sampled logits tensor [400, 2, 1]
14:03:00 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:03:00 [INFO] Unweighted rewards: [400, 2]
14:03:00 [INFO] Weighted rewards: [400]
14:03:00 [INFO] Loss tensor: [1] [50166.10546875]
14:03:01 [INFO] Mean epoch policy entropy for the trajectories is: 0.45678252
14:03:04 [INFO] Training epoch 32
14:03:05 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:03:05 [INFO] Size of sampled logits tensor [400, 2, 1]
14:03:05 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:03:05 [INFO] Unweighted rewards: [400, 2]
14:03:05 [INFO] Weighted rewards: [400]
14:03:05 [INFO] Loss tensor: [1] [72942.609375]
14:03:05 [INFO] Mean epoch policy entropy for the trajectories is: 0.42194435
14:03:08 [INFO] Training epoch 33
14:03:09 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:03:09 [INFO] Size of sampled logits tensor [400, 2, 1]
14:03:09 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:03:09 [INFO] Unweighted rewards: [400, 2]
14:03:09 [INFO] Weighted rewards: [400]
14:03:09 [INFO] Loss tensor: [1] [56925.15234375]
14:03:10 [INFO] Mean epoch policy entropy for the trajectories is: 0.37420714
14:03:13 [INFO] Training epoch 34
14:03:14 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:03:14 [INFO] Size of sampled logits tensor [400, 2, 1]
14:03:14 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:03:14 [INFO] Unweighted rewards: [400, 2]
14:03:14 [INFO] Weighted rewards: [400]
14:03:14 [INFO] Loss tensor: [1] [79779.171875]
14:03:14 [INFO] Mean epoch policy entropy for the trajectories is: 0.36799687
14:03:17 [INFO] Training epoch 35
14:03:18 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:03:18 [INFO] Size of sampled logits tensor [400, 2, 1]
14:03:18 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:03:18 [INFO] Unweighted rewards: [400, 2]
14:03:18 [INFO] Weighted rewards: [400]
14:03:18 [INFO] Loss tensor: [1] [35902.140625]
14:03:19 [INFO] Mean epoch policy entropy for the trajectories is: 0.3669547
14:03:22 [INFO] Training epoch 36
14:03:23 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:03:23 [INFO] Size of sampled logits tensor [400, 2, 1]
14:03:23 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:03:23 [INFO] Unweighted rewards: [400, 2]
14:03:23 [INFO] Weighted rewards: [400]
14:03:23 [INFO] Loss tensor: [1] [43056.41796875]
14:03:23 [INFO] Mean epoch policy entropy for the trajectories is: 0.32644922
14:03:26 [INFO] Training epoch 37
14:03:27 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:03:27 [INFO] Size of sampled logits tensor [400, 2, 1]
14:03:27 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:03:27 [INFO] Unweighted rewards: [400, 2]
14:03:27 [INFO] Weighted rewards: [400]
14:03:27 [INFO] Loss tensor: [1] [27699.625]
14:03:28 [INFO] Mean epoch policy entropy for the trajectories is: 0.30409148
14:03:31 [INFO] Training epoch 38
14:03:32 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:03:32 [INFO] Size of sampled logits tensor [400, 2, 1]
14:03:32 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:03:32 [INFO] Unweighted rewards: [400, 2]
14:03:32 [INFO] Weighted rewards: [400]
14:03:32 [INFO] Loss tensor: [1] [34205.3984375]
14:03:32 [INFO] Mean epoch policy entropy for the trajectories is: 0.28037733
14:03:35 [INFO] Training epoch 39
14:03:37 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:03:37 [INFO] Size of sampled logits tensor [400, 2, 1]
14:03:37 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:03:37 [INFO] Unweighted rewards: [400, 2]
14:03:37 [INFO] Weighted rewards: [400]
14:03:37 [INFO] Loss tensor: [1] [27128.654296875]
14:03:37 [INFO] Mean epoch policy entropy for the trajectories is: 0.24029392
14:03:40 [INFO] Training epoch 40
14:03:41 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:03:41 [INFO] Size of sampled logits tensor [400, 2, 1]
14:03:41 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:03:41 [INFO] Unweighted rewards: [400, 2]
14:03:41 [INFO] Weighted rewards: [400]
14:03:41 [INFO] Loss tensor: [1] [36567.16015625]
14:03:41 [INFO] Mean epoch policy entropy for the trajectories is: 0.2518988
14:03:44 [INFO] Training epoch 41
14:03:46 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:03:46 [INFO] Size of sampled logits tensor [400, 2, 1]
14:03:46 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:03:46 [INFO] Unweighted rewards: [400, 2]
14:03:46 [INFO] Weighted rewards: [400]
14:03:46 [INFO] Loss tensor: [1] [22045.884765625]
14:03:46 [INFO] Mean epoch policy entropy for the trajectories is: 0.21635264
14:03:49 [INFO] Training epoch 42
14:03:50 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:03:50 [INFO] Size of sampled logits tensor [400, 2, 1]
14:03:50 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:03:50 [INFO] Unweighted rewards: [400, 2]
14:03:50 [INFO] Weighted rewards: [400]
14:03:50 [INFO] Loss tensor: [1] [45504.86328125]
14:03:50 [INFO] Mean epoch policy entropy for the trajectories is: 0.23130813
14:03:53 [INFO] Training epoch 43
14:03:55 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:03:55 [INFO] Size of sampled logits tensor [400, 2, 1]
14:03:55 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:03:55 [INFO] Unweighted rewards: [400, 2]
14:03:55 [INFO] Weighted rewards: [400]
14:03:55 [INFO] Loss tensor: [1] [19670.556640625]
14:03:55 [INFO] Mean epoch policy entropy for the trajectories is: 0.23864876
14:03:58 [INFO] Training epoch 44
14:03:59 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:03:59 [INFO] Size of sampled logits tensor [400, 2, 1]
14:03:59 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:03:59 [INFO] Unweighted rewards: [400, 2]
14:03:59 [INFO] Weighted rewards: [400]
14:03:59 [INFO] Loss tensor: [1] [21371.96875]
14:03:59 [INFO] Mean epoch policy entropy for the trajectories is: 0.21554527
14:04:02 [INFO] Training epoch 45
14:04:04 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:04:04 [INFO] Size of sampled logits tensor [400, 2, 1]
14:04:04 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:04:04 [INFO] Unweighted rewards: [400, 2]
14:04:04 [INFO] Weighted rewards: [400]
14:04:04 [INFO] Loss tensor: [1] [24880.3828125]
14:04:04 [INFO] Mean epoch policy entropy for the trajectories is: 0.23635975
14:04:07 [INFO] Training epoch 46
14:04:08 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:04:08 [INFO] Size of sampled logits tensor [400, 2, 1]
14:04:08 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:04:08 [INFO] Unweighted rewards: [400, 2]
14:04:08 [INFO] Weighted rewards: [400]
14:04:08 [INFO] Loss tensor: [1] [15133.873046875]
14:04:08 [INFO] Mean epoch policy entropy for the trajectories is: 0.21912311
14:04:11 [INFO] Training epoch 47
14:04:12 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:04:12 [INFO] Size of sampled logits tensor [400, 2, 1]
14:04:12 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:04:12 [INFO] Unweighted rewards: [400, 2]
14:04:12 [INFO] Weighted rewards: [400]
14:04:12 [INFO] Loss tensor: [1] [45196.55078125]
14:04:13 [INFO] Mean epoch policy entropy for the trajectories is: 0.22629541
14:04:16 [INFO] Training epoch 48
14:04:17 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:04:17 [INFO] Size of sampled logits tensor [400, 2, 1]
14:04:17 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:04:17 [INFO] Unweighted rewards: [400, 2]
14:04:17 [INFO] Weighted rewards: [400]
14:04:17 [INFO] Loss tensor: [1] [34454.86328125]
14:04:17 [INFO] Mean epoch policy entropy for the trajectories is: 0.21373314
14:04:20 [INFO] Training epoch 49
14:04:22 [INFO] Size of logits & moves [400, 2, 13] [400, 2] Tensor[[400, 2, 1], Int64]
14:04:22 [INFO] Size of sampled logits tensor [400, 2, 1]
14:04:22 [INFO] Tensors for calculating policy loss, log_probs: [400, 2, 1] & rewards: [400, 2]
14:04:22 [INFO] Unweighted rewards: [400, 2]
14:04:22 [INFO] Weighted rewards: [400]
14:04:22 [INFO] Loss tensor: [1] [23801.716796875]
14:04:22 [INFO] Mean epoch policy entropy for the trajectories is: 0.2272305
14:04:25 [INFO] The random scrambling applied these moves: [[FMinus, RMinus], [UMinus, BMinus], [BPlus, FPlus], [LPlus, RMinus], [FMinus, FPlus], [RMinus, RPlus], [FPlus, BMinus], [FMinus, FMinus], [RMinus, BPlus], [RPlus, DMinus]] for testing
14:04:25 [INFO] Running the test loop now: 
14:04:25 [INFO] Number of tests: 10
14:04:25 [INFO] Scrambled cube used for test 0 used this moves: [FMinus, RMinus]
14:04:25 [INFO] Got the cubemove from policy: RPlus for turn 0
14:04:25 [INFO] The above is complementary or not: true
14:04:25 [INFO] The cross entropy for turn 0 0.014615906
14:04:25 [INFO] Got the cubemove from policy: FMinus for turn 1
14:04:25 [INFO] The above is complementary or not: false
14:04:25 [INFO] The cross entropy for turn 1 0.5873771
14:04:25 [INFO] Scrambled cube used for test 1 used this moves: [UMinus, BMinus]
14:04:25 [INFO] Got the cubemove from policy: UPlus for turn 0
14:04:25 [INFO] The above is complementary or not: false
14:04:25 [INFO] The cross entropy for turn 0 0.28924868
14:04:25 [INFO] Got the cubemove from policy: UPlus for turn 1
14:04:25 [INFO] The above is complementary or not: true
14:04:25 [INFO] The cross entropy for turn 1 1.418771
14:04:25 [INFO] Scrambled cube used for test 2 used this moves: [BPlus, FPlus]
14:04:25 [INFO] Got the cubemove from policy: FMinus for turn 0
14:04:25 [INFO] The above is complementary or not: true
14:04:25 [INFO] The cross entropy for turn 0 0.010883039
14:04:25 [INFO] Got the cubemove from policy: BMinus for turn 1
14:04:25 [INFO] The above is complementary or not: true
14:04:25 [INFO] The cross entropy for turn 1 0.00023881946
14:04:25 [INFO] Scrambled cube used for test 3 used this moves: [LPlus, RMinus]
14:04:25 [INFO] Got the cubemove from policy: LMinus for turn 0
14:04:25 [INFO] The above is complementary or not: false
14:04:25 [INFO] The cross entropy for turn 0 0.6894642
14:04:25 [INFO] Got the cubemove from policy: RPlus for turn 1
14:04:25 [INFO] The above is complementary or not: false
14:04:25 [INFO] The cross entropy for turn 1 0.00031763158
14:04:25 [INFO] Scrambled cube used for test 4 used this moves: [FMinus, FPlus]
14:04:25 [INFO] Got the cubemove from policy: LMinus for turn 0
14:04:25 [INFO] The above is complementary or not: false
14:04:25 [INFO] The cross entropy for turn 0 0.009780601
14:04:25 [INFO] Got the cubemove from policy: LPlus for turn 1
14:04:25 [INFO] The above is complementary or not: false
14:04:25 [INFO] The cross entropy for turn 1 0.00011197467
14:04:25 [INFO] Scrambled cube used for test 5 used this moves: [RMinus, RPlus]
14:04:25 [INFO] Got the cubemove from policy: LMinus for turn 0
14:04:25 [INFO] The above is complementary or not: false
14:04:25 [INFO] The cross entropy for turn 0 0.009780601
14:04:25 [INFO] Got the cubemove from policy: LPlus for turn 1
14:04:25 [INFO] The above is complementary or not: false
14:04:25 [INFO] The cross entropy for turn 1 0.00011197467
14:04:25 [INFO] Scrambled cube used for test 6 used this moves: [FPlus, BMinus]
14:04:25 [INFO] Got the cubemove from policy: FMinus for turn 0
14:04:25 [INFO] The above is complementary or not: false
14:04:25 [INFO] The cross entropy for turn 0 0.002829741
14:04:25 [INFO] Got the cubemove from policy: BMinus for turn 1
14:04:25 [INFO] The above is complementary or not: false
14:04:25 [INFO] The cross entropy for turn 1 0.5548464
14:04:25 [INFO] Scrambled cube used for test 7 used this moves: [FMinus, FMinus]
14:04:25 [INFO] Got the cubemove from policy: FMinus for turn 0
14:04:25 [INFO] The above is complementary or not: false
14:04:25 [INFO] The cross entropy for turn 0 0.0004905702
14:04:25 [INFO] Got the cubemove from policy: FMinus for turn 1
14:04:25 [INFO] The above is complementary or not: false
14:04:25 [INFO] The cross entropy for turn 1 1.6301054e-5
14:04:25 [INFO] Scrambled cube used for test 8 used this moves: [RMinus, BPlus]
14:04:25 [INFO] Got the cubemove from policy: BMinus for turn 0
14:04:25 [INFO] The above is complementary or not: true
14:04:25 [INFO] The cross entropy for turn 0 0.03956432
14:04:25 [INFO] Got the cubemove from policy: RPlus for turn 1
14:04:25 [INFO] The above is complementary or not: true
14:04:25 [INFO] The cross entropy for turn 1 0.00031763158
14:04:25 [INFO] Scrambled cube used for test 9 used this moves: [RPlus, DMinus]
14:04:25 [INFO] Got the cubemove from policy: DPlus for turn 0
14:04:25 [INFO] The above is complementary or not: true
14:04:25 [INFO] The cross entropy for turn 0 0.024966158
14:04:25 [INFO] Got the cubemove from policy: BMinus for turn 1
14:04:25 [INFO] The above is complementary or not: false
14:04:25 [INFO] The cross entropy for turn 1 1.0133075
14:04:25 [INFO] Mean loss for tests 0.0
14:04:25 [INFO] Mean entropy for tests 0.23335204
14:04:25 [INFO] Mean correct moves for 0.7
